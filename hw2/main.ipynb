{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7dbea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8caae505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    x,y,z = x\n",
    "    return (x - z)**2 + (2*y + z)**2 + (4*x - 2*y + z)**2 + x + y\n",
    "\n",
    "def grad_f1(x):\n",
    "    x,y,z = x[0], x[1], x[2]\n",
    "    dx = 2*(x - z) + 8*(4*x - 2*y + z) + 1\n",
    "    dy = 4*(2*y + z) - 4*(4*x - 2*y + z) + 1\n",
    "    dz = -2*(x - z) + 2*(2*y + z) + 2*(4*x - 2*y + z)\n",
    "    \n",
    "    return np.array([dx, dy, dz])\n",
    "\n",
    "def hess_f1(x):\n",
    "    dxx = 34 # 2 + 32\n",
    "    dxy = -16\n",
    "    dxz = 6 #-2 + 8\n",
    "    dyy = 16\n",
    "    dyz = 0\n",
    "    dzz = 6\n",
    "    \n",
    "    return np.array([[dxx, dxy, dxz], [dxy, dyy, dyz], [dxz, dyz, dzz]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08eaaa03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(0),\n",
       " array([1, 1, 0]),\n",
       " array([[ 34, -16,   6],\n",
       "        [-16,  16,   0],\n",
       "        [  6,   0,   6]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_x1 = np.array([0,0,0]) \n",
    "f1(f1_x1), grad_f1(f1_x1), hess_f1(f1_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c45d29e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x):\n",
    "    x,y,z = x\n",
    "    return (x - 1)**2 + (y - 1)**2 + 100*(y-x**2)**2 + 100*(z-y**2)**2\n",
    "\n",
    "def grad_f2(x):\n",
    "    x,y,z = x[0], x[1], x[2]\n",
    "    dx = 2*x - 2 - 400 * x * y + 400 * x**3\n",
    "    dy = 2*y - 2 + 200 * (y - x**2) - 400*z*y + 400 * y**3\n",
    "    dz = 200 * (z - y**2)\n",
    "        \n",
    "    return np.array([dx, dy, dz])\n",
    "\n",
    "def hess_f2(x):\n",
    "    x,y,z = x[0], x[1], x[2]\n",
    "    \n",
    "    dxx = 2 - 400 * y + 1200 * x**2\n",
    "    dxy = -400 * x\n",
    "    dxz = 0\n",
    "    dyy = 2 + 200 - 400 * z + 1200 * y**2\n",
    "    dyz = -400 * y\n",
    "    dzz = 200\n",
    "    return np.array([[dxx, dxy, dxz], [dxy, dyy, dyz], [dxz, dyz, dzz]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fff7db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(11.6),\n",
       " array([115.6,  67.6, -48. ]),\n",
       " array([[1250., -480.,    0.],\n",
       "        [-480., 1450., -480.],\n",
       "        [   0., -480.,  200.]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2_x1 = np.array([1.2, 1.2, 1.2])\n",
    "f2(f2_x1), grad_f2(f2_x1), hess_f2(f2_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ac8e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(x):\n",
    "    x,y = x\n",
    "    return (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2\n",
    "\n",
    "def grad_f3(x):\n",
    "    x,y = x[0], x[1]\n",
    "    \n",
    "    dx = -12.75 * 6*x + 3*y - 4*x*y - 2*x*y**2 + 4.5*y**2 + 2*x*y**4 - 4*x*y**3 + 5.25 * y**3 + 2*x*y**6\n",
    "    dy = 3*x + 9*y*x - 4*x**2*y - 2*x**2 + 15.75*y**2 *x + 2*x**2 * y - 6 * x**2 * y**2 + 4*x**3*y**3 + 6*x**2*y**5\n",
    "    \n",
    "    return np.array([dx, dy])\n",
    "\n",
    "def hess_f3(x):\n",
    "    x,y = x[0], x[1]\n",
    "    \n",
    "    dxx = 6 - 4*y - 2*y**2 + 2*y**4 - 4*y**3 + 2*y**6\n",
    "    dxy = 3 - 4*x - 4*x*y + 9*y + 8*x*y**3 - 12*x*y**2 + 15.75*y**2 + 12*x*y**5\n",
    "    dyy = 9*x - 4*x**2 + 31.5*y*x + 2*x**2 - 12*x**2*y + 12*x**2*y**2 + 30*x**2*y**4\n",
    "    return np.array([[dxx, dxy], [dxy, dyy]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6582d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(14.203125),\n",
       " array([-69.75,  27.75]),\n",
       " array([[ 0.  , 27.75],\n",
       "        [27.75, 68.5 ]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f3_x1 = np.array([1,1])\n",
    "f3(f3_x1), grad_f3(f3_x1), hess_f3(f3_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdd8fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate(i):\n",
    "    return 0.01 * 0.99**i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f3a8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(x, grad_f, lr_fn, n_steps, t=None):\n",
    "    \n",
    "    if t is not None:\n",
    "        i = 0\n",
    "        start = time.time()\n",
    "        while time.time() - start < t:\n",
    "            x = x - lr_fn(i) * grad_f(x)\n",
    "            i += 1\n",
    "    else:\n",
    "        for i in range(n_steps):\n",
    "            x = x - lr_fn(i) * grad_f(x)\n",
    "        \n",
    "    return x \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06be5673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyak(x: np.ndarray, grad_f, lr_fn, mu, n_steps, t=None):\n",
    "    x_prev = x\n",
    "    \n",
    "    if t is not None:\n",
    "        start = time.time()\n",
    "        i = 0\n",
    "        while time.time() - start < t:\n",
    "            dx = grad_f(x)\n",
    "            x = x - lr_fn(i) * dx  + mu * (x - x_prev)\n",
    "        \n",
    "            x_prev = x\n",
    "            i+=1\n",
    "    else:\n",
    "        for i in range(n_steps):\n",
    "            dx = grad_f(x)\n",
    "            x = x - lr_fn(i) * dx  + mu * (x - x_prev)\n",
    "            \n",
    "            x_prev = x\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42bb25bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov(x, f, grad_f, lr_fn, mu, n_steps, t=None):\n",
    "    x_prev = x\n",
    "    \n",
    "    if t is not None:\n",
    "        start = time.time()\n",
    "        i = 0\n",
    "        while time.time() - start < t:\n",
    "            dx = grad_f(x + mu * (x - x_prev))\n",
    "            x = x - lr_fn(i) * dx + mu * (x_prev - x)\n",
    "        \n",
    "            x_prev = x\n",
    "            i+=1\n",
    "    else:\n",
    "        for i in range(n_steps):\n",
    "            dx = grad_f(x + mu * (x - x_prev))\n",
    "            x = x - lr_fn(i) * dx + mu * (x_prev - x)\n",
    "            \n",
    "            x_prev = x\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "044fa1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adagrad(x, grad_f, lr_fn, n_steps, eps=1e-8, t=None):\n",
    "    x = np.array(x, dtype=float)\n",
    "    n = len(x)\n",
    "    \n",
    "    grad_sq_sum = np.zeros(n)\n",
    "    \n",
    "     \n",
    "    if t is not None:\n",
    "        start = time.time()\n",
    "        i = 0\n",
    "        while time.time() - start < t:\n",
    "            grad = np.array(grad_f(x))\n",
    "        \n",
    "            # Accumulate squared gradients\n",
    "            grad_sq_sum += grad ** 2\n",
    "            \n",
    "            # Compute D_k^{-1/2}\n",
    "            D_inv_sqrt = np.diag(1.0 / (np.sqrt(grad_sq_sum) + eps))\n",
    "            \n",
    "            # Adaptive step\n",
    "            step = lr_fn(i) * D_inv_sqrt @ grad\n",
    "            x_new = x - step\n",
    "            \n",
    "            x = x_new\n",
    "            i+= 1\n",
    "    else:\n",
    "        for i in range(n_steps):\n",
    "            grad = np.array(grad_f(x))\n",
    "            \n",
    "            # Accumulate squared gradients\n",
    "            grad_sq_sum += grad ** 2\n",
    "            \n",
    "            # Compute D_k^{-1/2}\n",
    "            D_inv_sqrt = np.diag(1.0 / (np.sqrt(grad_sq_sum) + eps))\n",
    "            \n",
    "            # Adaptive step\n",
    "            step = lr_fn(i) * D_inv_sqrt @ grad\n",
    "            x_new = x - step\n",
    "            \n",
    "            x = x_new\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93b767ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-0.15969334544827538)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(adagrad(f1_x1, grad_f1, learning_rate, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94603437",
   "metadata": {},
   "source": [
    "Newton and BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e15c33bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def is_positive_definite(H):\n",
    "    try:\n",
    "        np.linalg.cholesky(H)\n",
    "        return True\n",
    "    except np.linalg.LinAlgError:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb4858f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(x0, grad_f, hess_f, n_steps=100, t=None, lr=None):\n",
    "    x = np.array(x0, dtype=float)\n",
    "    \n",
    "    if t is not None:\n",
    "        start = time.time()\n",
    "        i = 0\n",
    "        while time.time() - start < t:\n",
    "            grad = np.array(grad_f(x))\n",
    "            hess = np.array(hess_f(x))\n",
    "            \n",
    "            hess_inv = np.linalg.inv(hess)\n",
    "            \n",
    "            lr = lr if lr is not None else learning_rate(i)\n",
    "            # Compute Newton step\n",
    "            x = x - learning_rate(i) * hess_inv @ grad\n",
    "            i += 1\n",
    "\n",
    "    else:\n",
    "        for i in range(n_steps):\n",
    "            grad = np.array(grad_f(x))\n",
    "            grad = np.clip(grad, -10, 10)\n",
    "\n",
    "            hess = np.array(hess_f(x)) + 1e-4 * np.eye(len(x))\n",
    "            \n",
    "            hess_inv = np.linalg.inv(hess)\n",
    "            \n",
    "            # Compute Newton step\n",
    "            x = x - hess_inv @ grad\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e96eed59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton:  10.777012954523533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1., 0.]), None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum3(f2), print(\"Newton: \", f2(newton(np.array([0,0,0]), grad_f1, hess_f1, 100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9266eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(x, grad_f, lr_fn, n_steps=100, t=None):\n",
    "    x = np.array(x, dtype=float)\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        grad = grad_f(x) \n",
    "        x = x - lr_fn(i) * grad\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fb9f53",
   "metadata": {},
   "source": [
    "### BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d97b3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfgs(x, grad_f, lr_fn, n_steps, t=None):\n",
    "    B = np.eye(len(x)) * 0.0001\n",
    "    grad = grad_f(x)\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        x_new = x - B @ grad\n",
    "        grad_new = grad_f(x_new)\n",
    "\n",
    "        delta = (x_new - x).reshape((-1, 1))\n",
    "        gamma = (grad_new - grad).reshape((-1, 1))\n",
    "        rho = 1.0 / (delta.T @ gamma)\n",
    "\n",
    "        if np.isfinite(rho) and rho > 0:\n",
    "            I = np.eye(len(x))\n",
    "            B = (I - rho * delta @ gamma.T) @ B @ (I - rho * gamma @ delta.T) + rho * delta @ delta.T\n",
    "\n",
    "        x, grad = x_new, grad_new\n",
    "        print(f\"x: {x.flatten()}, grad norm: {np.linalg.norm(grad)}\")\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fcabe647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [ 9.965e-01  1.500e-03 -6.000e-04], grad norm: 38.38061094771682\n",
      "x: [ 0.1970345   0.34373421 -0.13766342], grad norm: 3.6355145084048957\n",
      "x: [ 0.19683472  0.34342622 -0.13770975], grad norm: 3.6329667270874513\n",
      "x: [-0.03220376 -0.08872086 -0.19311509], grad norm: 1.3654158770052056\n",
      "x: [-0.03092996 -0.09073154 -0.19280275], grad norm: 1.3649438631310555\n",
      "x: [-0.02975393 -0.09263503 -0.1922545 ], grad norm: 1.3692643543515144\n",
      "x: [-0.02783246 -0.09642315 -0.19055679], grad norm: 1.3898917204806442\n",
      "x: [-0.02557648 -0.10282014 -0.18626188], grad norm: 1.4505395613916403\n",
      "x: [-0.02373176 -0.11468055 -0.17492248], grad norm: 1.6078827525291324\n",
      "x: [-0.02555418 -0.13606992 -0.14703653], grad norm: 1.9225510592743007\n",
      "x: [-0.03917976 -0.17236634 -0.08450469], grad norm: 2.347624819305254\n",
      "x: [-0.07626491 -0.2203837   0.02574462], grad norm: 2.4809943421590726\n",
      "x: [-0.13089408 -0.25316616  0.14334218], grad norm: 1.7471987250292211\n",
      "x: [-0.16508769 -0.24835257  0.18703895], grad norm: 0.6007621934307672\n",
      "x: [-0.16913368 -0.23386221  0.17577332], grad norm: 0.07045808099507042\n",
      "x: [-0.16712547 -0.22951138  0.16773624], grad norm: 0.005495805102659031\n",
      "x: [-0.16668951 -0.2291659   0.16669875], grad norm: 0.0007080007285383034\n",
      "x: [-0.16666687 -0.22916608  0.16666626], grad norm: 2.2888890027348024e-05\n",
      "x: [-0.16666666 -0.22916665  0.16666665], grad norm: 2.0009012042868245e-07\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 8.380554479109728e-10\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.381800953807708e-12\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 1.5034506401509132e-14\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 1.6653345369377348e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 4.742874840267547e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 1.6653345369377348e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n",
      "x: [-0.16666667 -0.22916667  0.16666667], grad norm: 7.130536482749599e-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_8464\\3924494986.py:11: RuntimeWarning: divide by zero encountered in divide\n",
      "  rho = 1.0 / (delta.T @ gamma)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-0.16666667, -0.22916667,  0.16666667]),\n",
       " array([-0.15151515, -0.21212121,  0.15151515, -0.19651056]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfgs(np.array([1,0,0]), grad_f1, hess_f1, 100), minimum3(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ee979b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [ 1.00765e+00 -1.00000e-04], grad norm: 77.0914985644133\n",
      "x: [ 1.01535851e+00 -1.99152924e-04], grad norm: 77.68093152203751\n",
      "x: [ 1.02312598e+00 -2.97429008e-04], grad norm: 78.27486885960934\n",
      "x: [ 1.03095286e+00 -3.94797927e-04], grad norm: 78.87334516541986\n",
      "x: [ 1.03883961e+00 -4.91228785e-04], grad norm: 79.47639530015786\n",
      "x: [ 1.04678668e+00 -5.86690108e-04], grad norm: 80.08405439924803\n",
      "x: [ 1.05479452e+00 -6.81149832e-04], grad norm: 80.69635787521426\n",
      "x: [ 1.06286362e+00 -7.74575295e-04], grad norm: 81.31334142006803\n",
      "x: [ 1.07099443e+00 -8.66933227e-04], grad norm: 81.9350410077221\n",
      "x: [ 1.07918742e+00 -9.58189739e-04], grad norm: 82.56149289643076\n",
      "x: [ 1.08744308e+00 -1.04831032e-03], grad norm: 83.19273363125593\n",
      "x: [ 1.09576188 -0.00113726], grad norm: 83.82880004656025\n",
      "x: [ 1.1041443 -0.001225 ], grad norm: 84.4697292685275\n",
      "x: [ 1.11259083 -0.0013115 ], grad norm: 85.11555871771033\n",
      "x: [ 1.12110196 -0.00139672], grad norm: 85.76632611160622\n",
      "x: [ 1.12967818 -0.00148062], grad norm: 86.42206946726175\n",
      "x: [ 1.13832    -0.00156316], grad norm: 87.08282710390614\n",
      "x: [ 1.1470279  -0.00164431], grad norm: 87.74863764561374\n",
      "x: [ 1.1558024  -0.00172402], grad norm: 88.41954002399649\n",
      "x: [ 1.16464401 -0.00180226], grad norm: 89.09557348092699\n",
      "x: [ 1.17355324 -0.00187898], grad norm: 89.77677757129189\n",
      "x: [ 1.1825306  -0.00195413], grad norm: 90.46319216577697\n",
      "x: [ 1.19157662 -0.00202769], grad norm: 91.15485745368393\n",
      "x: [ 1.20069182 -0.0020996 ], grad norm: 91.85181394577928\n",
      "x: [ 1.20987673 -0.00216981], grad norm: 92.5541024771766\n",
      "x: [ 1.21913189 -0.00223829], grad norm: 93.26176421025156\n",
      "x: [ 1.22845783 -0.00230499], grad norm: 93.97484063759161\n",
      "x: [ 1.23785509 -0.00236986], grad norm: 94.69337358497953\n",
      "x: [ 1.24732422 -0.00243285], grad norm: 95.4174052144126\n",
      "x: [ 1.25686576 -0.00249391], grad norm: 96.1469780271573\n",
      "x: [ 1.26648028 -0.00255301], grad norm: 96.8821348668403\n",
      "x: [ 1.27616832 -0.00261007], grad norm: 97.62291892257636\n",
      "x: [ 1.28593046 -0.00266506], grad norm: 98.36937373213411\n",
      "x: [ 1.29576726 -0.00271792], grad norm: 99.12154318513963\n",
      "x: [ 1.30567928 -0.0027686 ], grad norm: 99.87947152631934\n",
      "x: [ 1.31566711 -0.00281704], grad norm: 100.64320335878233\n",
      "x: [ 1.32573133 -0.00286319], grad norm: 101.41278364734302\n",
      "x: [ 1.33587251 -0.002907  ], grad norm: 102.18825772188461\n",
      "x: [ 1.34609125 -0.0029484 ], grad norm: 102.96967128076487\n",
      "x: [ 1.35638815 -0.00298734], grad norm: 103.75707039426376\n",
      "x: [ 1.36676379 -0.00302376], grad norm: 104.550501508075\n",
      "x: [ 1.37721878 -0.0030576 ], grad norm: 105.35001144684115\n",
      "x: [ 1.38775374 -0.0030888 ], grad norm: 106.15564741773403\n",
      "x: [ 1.39836926 -0.00311729], grad norm: 106.9674570140808\n",
      "x: [ 1.40906598 -0.00314302], grad norm: 107.78548821903632\n",
      "x: [ 1.4198445  -0.00316592], grad norm: 108.60978940930352\n",
      "x: [ 1.43070546 -0.00318593], grad norm: 109.44040935890182\n",
      "x: [ 1.44164949 -0.00320297], grad norm: 110.27739724298479\n",
      "x: [ 1.45267722 -0.00321698], grad norm: 111.12080264170844\n",
      "x: [ 1.4637893  -0.00322789], grad norm: 111.97067554415048\n",
      "x: [ 1.47498636 -0.00323563], grad norm: 112.82706635228199\n",
      "x: [ 1.48626907 -0.00324013], grad norm: 113.69002588499201\n",
      "x: [ 1.49763807 -0.00324132], grad norm: 114.5596053821671\n",
      "x: [ 1.50909403 -0.00323913], grad norm: 115.43585650882578\n",
      "x: [ 1.52063762 -0.00323347], grad norm: 116.31883135930987\n",
      "x: [ 1.5322695  -0.00322427], grad norm: 117.20858246153318\n",
      "x: [ 1.54399035 -0.00321146], grad norm: 118.10516278128958\n",
      "x: [ 1.55580085 -0.00319495], grad norm: 119.00862572662061\n",
      "x: [ 1.5677017  -0.00317467], grad norm: 119.9190251522451\n",
      "x: [ 1.57969357 -0.00315054], grad norm: 120.83641536405058\n",
      "x: [ 1.59177718 -0.00312246], grad norm: 121.7608511236497\n",
      "x: [ 1.60395323 -0.00309036], grad norm: 122.69238765300103\n",
      "x: [ 1.61622241 -0.00305416], grad norm: 123.63108063909718\n",
      "x: [ 1.62858545 -0.00301375], grad norm: 124.576986238721\n",
      "x: [ 1.64104307 -0.00296906], grad norm: 125.53016108327131\n",
      "x: [ 1.65359599 -0.00291999], grad norm: 126.49066228366001\n",
      "x: [ 1.66624495 -0.00286645], grad norm: 127.45854743528206\n",
      "x: [ 1.67899067 -0.00280835], grad norm: 128.4338746230596\n",
      "x: [ 1.6918339  -0.00274559], grad norm: 129.41670242656252\n",
      "x: [ 1.7047754  -0.00267808], grad norm: 130.4070899252068\n",
      "x: [ 1.7178159  -0.00260572], grad norm: 131.40509670353248\n",
      "x: [ 1.73095619 -0.0025284 ], grad norm: 132.41078285656295\n",
      "x: [ 1.74419701 -0.00244602], grad norm: 133.4242089952484\n",
      "x: [ 1.75753914 -0.00235849], grad norm: 134.44543625199344\n",
      "x: [ 1.77098337 -0.0022657 ], grad norm: 135.47452628627354\n",
      "x: [ 1.78453046 -0.00216753], grad norm: 136.51154129033995\n",
      "x: [ 1.79818122 -0.00206388], grad norm: 137.5565439950168\n",
      "x: [ 1.81193644 -0.00195465], grad norm: 138.6095976755922\n",
      "x: [ 1.82579693 -0.0018397 ], grad norm: 139.6707661578057\n",
      "x: [ 1.83976348e+00 -1.71894168e-03], grad norm: 140.7401138239341\n",
      "x: [ 1.85383692e+00 -1.59224477e-03], grad norm: 141.8177056189788\n",
      "x: [ 1.86801807e+00 -1.45949357e-03], grad norm: 142.90360705695704\n",
      "x: [ 1.88230776e+00 -1.32056734e-03], grad norm: 143.997884227299\n",
      "x: [ 1.89670681e+00 -1.17534326e-03], grad norm: 145.10060380135502\n",
      "x: [ 1.91121608e+00 -1.02369640e-03], grad norm: 146.21183303901395\n",
      "x: [ 1.92583641e+00 -8.65499705e-04], grad norm: 147.33163979543735\n",
      "x: [ 1.94056865e+00 -7.00623928e-04], grad norm: 148.46009252791146\n",
      "x: [ 1.95541367e+00 -5.28937611e-04], grad norm: 149.59726030282002\n",
      "x: [ 1.97037233e+00 -3.50307039e-04], grad norm: 150.74321280274296\n",
      "x: [ 1.98544550e+00 -1.64596204e-04], grad norm: 151.89802033368161\n",
      "x: [2.00063408e+00 2.83332440e-05], grad norm: 153.06175383241688\n",
      "x: [2.01593895e+00 2.28622029e-04], grad norm: 154.23448487400174\n",
      "x: [2.03136099e+00 4.36413298e-04], grad norm: 155.41628567939318\n",
      "x: [2.04690113e+00 6.51852663e-04], grad norm: 156.6072291232273\n",
      "x: [2.06256026e+00 8.75088245e-04], grad norm: 157.80738874174185\n",
      "x: [2.07833931e+00 1.10627072e-03], grad norm: 159.01683874084964\n",
      "x: [2.09423919e+00 1.34555336e-03], grad norm: 160.2356540043684\n",
      "x: [2.11026084e+00 1.59309210e-03], grad norm: 161.4639101024112\n",
      "x: [2.12640521e+00 1.84904556e-03], grad norm: 162.7016832999422\n",
      "x: [2.14267322e+00 2.11357511e-03], grad norm: 163.94905056550283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.6513886623156608),\n",
       " array([3.00000000e+00, 5.01501502e-01, 5.21242614e-05]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f3(bfgs(np.array([1,0]), grad_f3, hess_f3, 100)), minimum2(f3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f75232aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [-3.    2.02  0.  ], grad norm: 8635.905496565501\n",
      "x: [1.01562454 1.18043131 0.4160447 ], grad norm: 532.5174462711475\n",
      "x: [ 0.13795619 -3.74108952  2.18232326], grad norm: 18591.43208760717\n",
      "x: [0.75003492 0.66991456 3.59163957], grad norm: 1034.800476144824\n",
      "x: [ 1.68675114  2.44631029 -5.87470283], grad norm: 11772.162849827168\n",
      "x: [0.89308392 0.77879831 1.57457098], grad norm: 361.9669986194688\n",
      "x: [0.92733522 0.8165857  0.99347583], grad norm: 133.85832698772276\n",
      "x: [0.93458376 0.83830943 0.700393  ], grad norm: 14.571580147215284\n",
      "x: [0.92803404 0.83909013 0.70335939], grad norm: 9.257428470476059\n",
      "x: [0.91730949 0.84058629 0.70642679], grad norm: 0.4668775839660613\n",
      "x: [0.91705525 0.84067743 0.70660494], grad norm: 0.3412088590240737\n",
      "x: [0.917002   0.84077993 0.70678639], grad norm: 0.3250059654775852\n",
      "x: [0.91698629 0.84145895 0.70796062], grad norm: 0.4193962387182706\n",
      "x: [0.91735859 0.84303598 0.71065514], grad norm: 0.7118207425665913\n",
      "x: [0.91908169 0.84772985 0.71863388], grad norm: 1.3072801468830944\n",
      "x: [0.92430244 0.85949076 0.73862496], grad norm: 2.2020882562662547\n",
      "x: [0.938616   0.88867663 0.78862084], grad norm: 3.466061430855584\n",
      "x: [0.96880966 0.94659616 0.89043785], grad norm: 4.934599232353294\n",
      "x: [0.98169796 0.96832963 0.93407907], grad norm: 2.9908255782328057\n",
      "x: [0.9917662  0.98347626 0.96800097], grad norm: 0.3959763167218491\n",
      "x: [1.00250721 1.00350785 1.00469997], grad norm: 0.9995768202094063\n",
      "x: [0.9960055  0.99162951 0.98313899], grad norm: 0.15647728643746733\n",
      "x: [0.99701097 0.99366383 0.98715317], grad norm: 0.14681532271612824\n",
      "x: [0.99988366 0.99967148 0.99921261], grad norm: 0.05637658677456435\n",
      "x: [1.00001376 1.00000751 0.99999037], grad norm: 0.011102616526464243\n",
      "x: [1.00000316 1.00000579 1.00001172], grad norm: 0.00026558856167307354\n",
      "x: [1.00000033 1.00000063 1.00000129], grad norm: 2.5153634213125916e-05\n",
      "x: [1. 1. 1.], grad norm: 8.85576770425873e-08\n",
      "x: [1. 1. 1.], grad norm: 1.2180841268132846e-09\n",
      "x: [1. 1. 1.], grad norm: 6.3337063657957024e-12\n",
      "x: [1. 1. 1.], grad norm: 6.102632492251517e-14\n",
      "x: [1. 1. 1.], grad norm: 2.2336936793888852e-13\n",
      "x: [1. 1. 1.], grad norm: 5.684341886080802e-14\n",
      "x: [1. 1. 1.], grad norm: 1.851591070968565e-13\n",
      "x: [1. 1. 1.], grad norm: 7.213408716298969e-14\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_8464\\2962846308.py:11: RuntimeWarning: divide by zero encountered in divide\n",
      "  rho = 1.0 / (delta.T @ gamma)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfgs(np.array([1,0,0]), grad_f2, hess_f2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def l_bfgs(x, grad_f, m=10, n_steps=100):\n",
    "    \"\"\"L-BFGS optimization (inverse-Hessian approx using history size m).\"\"\"\n",
    "    x = x.reshape(-1)  # ensure 1D input\n",
    "    grad = grad_f(x).reshape(-1, 1)\n",
    "\n",
    "    B = []      # list of s (deltas)\n",
    "    gamma = []  # list of y (grad diffs)\n",
    "    rho = []    # list of 1 / (y^T s)\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        q = grad.copy()\n",
    "        alpha = []\n",
    "\n",
    "        for i in reversed(range(len(B))):\n",
    "            s_i, y_i, rho_i = B[i], gamma[i], rho[i]\n",
    "            alpha_i = rho_i * float(s_i.T @ q)\n",
    "            alpha.append(alpha_i)\n",
    "            q = q - alpha_i * y_i\n",
    "\n",
    "        if B:\n",
    "            s_last, y_last = B[-1], gamma[-1]\n",
    "            H0 = float(s_last.T @ y_last) / float(y_last.T @ y_last)\n",
    "            r = H0 * q\n",
    "        else:\n",
    "            r = q\n",
    "\n",
    "        for i in range(len(B)):\n",
    "            s_i, y_i, rho_i = B[i], gamma[i], rho[i]\n",
    "            beta_i = rho_i * float(y_i.T @ r)\n",
    "            r = r + s_i * (alpha[-(i + 1)] - beta_i)\n",
    "\n",
    "        p = -r\n",
    "        x_new = x - r.flatten()\n",
    "        grad_new = grad_f(x_new).reshape(-1, 1)\n",
    "\n",
    "        s_k = (x_new - x).reshape(-1, 1)\n",
    "        y_k = (grad_new - grad).reshape(-1, 1)\n",
    "        ys = float(y_k.T @ s_k)\n",
    "\n",
    "        if ys > 1e-10:  # safeguard\n",
    "            rho_k = 1.0 / ys\n",
    "            if len(B) == m:\n",
    "                B.pop(0)\n",
    "                gamma.pop(0)\n",
    "                rho.pop(0)\n",
    "            B.append(s_k)\n",
    "            gamma.append(y_k)\n",
    "            rho.append(rho_k)\n",
    "\n",
    "        x, grad = x_new, grad_new\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c1928654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [0.0002 0.0002 0.    ], grad norm: 2.7997370654840013\n",
      "x: [2.00081510e-02 1.96156849e-02 7.84775235e-08], grad norm: 2.8334391386940534\n",
      "x: [2.18912158e-02 2.10411076e-02 8.33772590e-06], grad norm: 3.0378685367602314\n",
      "x: [2.86949672e-02 2.50764869e-02 6.16365572e-05], grad norm: 3.659666172224478\n",
      "x: [0.04220937 0.03113953 0.0002084 ], grad norm: 4.624636953766189\n",
      "x: [0.08926944 0.04818816 0.0008051 ], grad norm: 6.983266100573047\n",
      "x: [0.13290502 0.06264361 0.00139355], grad norm: 8.300283242777935\n",
      "x: [0.20317909 0.08740298 0.00232227], grad norm: 9.338042705557081\n",
      "x: [0.33529419 0.13713429 0.0040349 ], grad norm: 6.8196537739359675\n",
      "x: [0.51252153 0.20714142 0.00643905], grad norm: 15.972325788710963\n",
      "x: [0.39293865 0.16008136 0.00525042], grad norm: 4.650273742059499\n",
      "x: [0.42050863 0.17174816 0.005998  ], grad norm: 4.827116230408463\n",
      "x: [0.44706775 0.18430493 0.00723602], grad norm: 6.252303711275049\n",
      "x: [0.44660937 0.18537198 0.00780534], grad norm: 6.028145956812536\n",
      "x: [0.47694885 0.23216025 0.02457695], grad norm: 6.531274670207339\n",
      "x: [0.52622911 0.28254755 0.04077331], grad norm: 9.079201974275017\n",
      "x: [0.55255694 0.30698145 0.05031647], grad norm: 9.878678101976652\n",
      "x: [0.57373728 0.3273731  0.06284062], grad norm: 9.778341417985773\n",
      "x: [0.64620249 0.40157653 0.1249505 ], grad norm: 8.158539578378283\n",
      "x: [0.67727029 0.44577638 0.18335776], grad norm: 4.300239401599624\n",
      "x: [0.7322631  0.52867641 0.2698649 ], grad norm: 2.5834394123096263\n",
      "x: [0.80469072 0.63903921 0.38766146], grad norm: 5.558687833865612\n",
      "x: [0.83396692 0.71115523 0.50300249], grad norm: 6.50029566114176\n",
      "x: [0.91321657 0.82792596 0.65817295], grad norm: 9.483963594287331\n",
      "x: [0.882767   0.76955362 0.60009098], grad norm: 6.00444279471137\n",
      "x: [0.89301627 0.79788375 0.63476878], grad norm: 0.5805830747966545\n",
      "x: [0.92538692 0.8539191  0.717807  ], grad norm: 3.9224947413582107\n",
      "x: [0.92774641 0.85942219 0.73235779], grad norm: 2.064419107581975\n",
      "x: [0.9572403  0.91514159 0.83408827], grad norm: 1.1390085295351282\n",
      "x: [0.97609858 0.95197768 0.9032275 ], grad norm: 1.1172873467786297\n",
      "x: [0.98747205 0.97538226 0.95226511], grad norm: 0.4092775287200927\n",
      "x: [1.0051683  1.00949715 1.01617585], grad norm: 1.227990226744247\n",
      "x: [0.99614745 0.99231551 0.98476076], grad norm: 0.045684078740088646\n",
      "x: [0.9984341  0.99685338 0.9936421 ], grad norm: 0.02521190423462816\n",
      "x: [0.99994292 0.99986712 0.99967488], grad norm: 0.02418326010022994\n",
      "x: [0.99999488 0.99998737 0.99996259], grad norm: 0.005078493388271712\n",
      "x: [1.00000016 0.99999987 0.99999989], grad norm: 0.00023419281517701222\n",
      "x: [0.99999996 1.00000003 1.00000005], grad norm: 5.899133600898711e-05\n",
      "x: [1. 1. 1.], grad norm: 2.6253151571270825e-08\n",
      "x: [1. 1. 1.], grad norm: 1.2838096087213445e-10\n",
      "x: [1. 1. 1.], grad norm: 4.010005073815382e-13\n",
      "x: [1. 1. 1.], grad norm: 5.684341886080802e-14\n",
      "x: [1. 1. 1.], grad norm: 9.183953831886928e-14\n",
      "x: [1. 1. 1.], grad norm: 1.2710574864626038e-13\n",
      "x: [1. 1. 1.], grad norm: 8.038873388460929e-14\n",
      "x: [1. 1. 1.], grad norm: 4.440892098500626e-14\n",
      "x: [1. 1. 1.], grad norm: 1.7621782166716258e-13\n",
      "x: [1. 1. 1.], grad norm: 5.669062723660282e-13\n",
      "x: [1. 1. 1.], grad norm: 2.112763458646778e-13\n",
      "x: [1. 1. 1.], grad norm: 1.1583494782190817e-13\n",
      "x: [1. 1. 1.], grad norm: 2.8766562004814574e-13\n",
      "x: [1. 1. 1.], grad norm: 8.038873388460929e-14\n",
      "x: [1. 1. 1.], grad norm: 2.8766562004814574e-13\n",
      "x: [1. 1. 1.], grad norm: 1.6230351877900013e-13\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n",
      "x: [1. 1. 1.], grad norm: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_8464\\3924494986.py:11: RuntimeWarning: divide by zero encountered in divide\n",
      "  rho = 1.0 / (delta.T @ gamma)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.0)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2(bfgs(np.array([0,0,0]), grad_f2, hess_f2, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4aa5a7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: ||grad|| = 3325.29818211841\n",
      "Step 2: ||grad|| = 2.6103351942989033\n",
      "Step 3: ||grad|| = 2.4500391460468203\n",
      "Step 4: ||grad|| = 3.298395704992419\n",
      "Step 5: ||grad|| = 20.580177130646405\n",
      "Step 6: ||grad|| = 4.322279161613207\n",
      "Step 7: ||grad|| = 3.3522446554492884\n",
      "Step 8: ||grad|| = 2.512004732537183\n",
      "Step 9: ||grad|| = 34.20552259322404\n",
      "Step 10: ||grad|| = 3.110035744942991\n",
      "Step 11: ||grad|| = 3.678248944392198\n",
      "Step 12: ||grad|| = 23.106226005007358\n",
      "Step 13: ||grad|| = 4.222014542073193\n",
      "Step 14: ||grad|| = 4.698248483836691\n",
      "Step 15: ||grad|| = 23.23686157198339\n",
      "Step 16: ||grad|| = 4.562371851037727\n",
      "Step 17: ||grad|| = 4.167793642777\n",
      "Step 18: ||grad|| = 34.41035995209404\n",
      "Step 19: ||grad|| = 3.02048200573506\n",
      "Step 20: ||grad|| = 1.7497931273660638\n",
      "Step 21: ||grad|| = 75.06689147870233\n",
      "Step 22: ||grad|| = 2.1033525257907506\n",
      "Step 23: ||grad|| = 2.0846918146265137\n",
      "Step 24: ||grad|| = 293.76819655237927\n",
      "Step 25: ||grad|| = 2.0352589101184924\n",
      "Step 26: ||grad|| = 2.0236730453493403\n",
      "Step 27: ||grad|| = 779.2849922513575\n",
      "Step 28: ||grad|| = 1.9048532530116127\n",
      "Step 29: ||grad|| = 1.854626359841816\n",
      "Step 30: ||grad|| = 180.90537461524818\n",
      "Step 31: ||grad|| = 1.8762575652168052\n",
      "Step 32: ||grad|| = 1.8556417094520372\n",
      "Step 33: ||grad|| = 665.1929692594653\n",
      "Step 34: ||grad|| = 1.8170537822303978\n",
      "Step 35: ||grad|| = 1.79093100413153\n",
      "Step 36: ||grad|| = 389.37862805085416\n",
      "Step 37: ||grad|| = 1.8007356677141861\n",
      "Step 38: ||grad|| = 1.7936364456600977\n",
      "Step 39: ||grad|| = 528.3174870273556\n",
      "Step 40: ||grad|| = 1.7985489100871686\n",
      "Step 41: ||grad|| = 1.788292739842584\n",
      "Step 42: ||grad|| = 446.43413214375414\n",
      "Step 43: ||grad|| = 1.785035067308517\n",
      "Step 44: ||grad|| = 1.7744540524035637\n",
      "Step 45: ||grad|| = 465.90875689783144\n",
      "Step 46: ||grad|| = 1.7790723165995812\n",
      "Step 47: ||grad|| = 1.7724822512004628\n",
      "Step 48: ||grad|| = 462.6039295658268\n",
      "Step 49: ||grad|| = 1.7803278439282677\n",
      "Step 50: ||grad|| = 1.7723409280992226\n",
      "Step 51: ||grad|| = 450.0848578468105\n",
      "Step 52: ||grad|| = 1.7696900988069477\n",
      "Step 53: ||grad|| = 1.7573930490970051\n",
      "Step 54: ||grad|| = 418.04657759022524\n",
      "Step 55: ||grad|| = 1.7562437289630233\n",
      "Step 56: ||grad|| = 1.745662035806855\n",
      "Step 57: ||grad|| = 412.1408175920904\n",
      "Step 58: ||grad|| = 1.7506436422630283\n",
      "Step 59: ||grad|| = 1.7420632840747758\n",
      "Step 60: ||grad|| = 404.849754153808\n",
      "Step 61: ||grad|| = 1.7474494348454432\n",
      "Step 62: ||grad|| = 1.7373596992426146\n",
      "Step 63: ||grad|| = 390.76411817283633\n",
      "Step 64: ||grad|| = 1.737085211255981\n",
      "Step 65: ||grad|| = 1.7245916062711921\n",
      "Step 66: ||grad|| = 371.2754253009242\n",
      "Step 67: ||grad|| = 1.7248360805208733\n",
      "Step 68: ||grad|| = 1.712672553344921\n",
      "Step 69: ||grad|| = 358.0751174292696\n",
      "Step 70: ||grad|| = 1.7164966730181066\n",
      "Step 71: ||grad|| = 1.7050507011916698\n",
      "Step 72: ||grad|| = 346.30669025726314\n",
      "Step 73: ||grad|| = 1.7091552954128943\n",
      "Step 74: ||grad|| = 1.6969481452101658\n",
      "Step 75: ||grad|| = 333.1440385362094\n",
      "Step 76: ||grad|| = 1.699183996710909\n",
      "Step 77: ||grad|| = 1.685762557120406\n",
      "Step 78: ||grad|| = 318.4145967365011\n",
      "Step 79: ||grad|| = 1.6879243374407162\n",
      "Step 80: ||grad|| = 1.6738984579729974\n",
      "Step 81: ||grad|| = 304.0054667069918\n",
      "Step 82: ||grad|| = 1.6771446483272299\n",
      "Step 83: ||grad|| = 1.6626854210988093\n",
      "Step 84: ||grad|| = 290.0938708463911\n",
      "Step 85: ||grad|| = 1.6662886670471961\n",
      "Step 86: ||grad|| = 1.6510981106558358\n",
      "Step 87: ||grad|| = 276.17442439331563\n",
      "Step 88: ||grad|| = 1.6545937299086075\n",
      "Step 89: ||grad|| = 1.6384811670148063\n",
      "Step 90: ||grad|| = 262.01365281736275\n",
      "Step 91: ||grad|| = 1.642086096185076\n",
      "Step 92: ||grad|| = 1.6249798729661005\n",
      "Step 93: ||grad|| = 247.67767411722417\n",
      "Step 94: ||grad|| = 1.6288436384898242\n",
      "Step 95: ||grad|| = 1.6106251183202684\n",
      "Step 96: ||grad|| = 233.21138777157432\n",
      "Step 97: ||grad|| = 1.6147118222331398\n",
      "Step 98: ||grad|| = 1.5952019459332885\n",
      "Step 99: ||grad|| = 218.58940361239118\n",
      "Step 100: ||grad|| = 1.599482672553982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_8464\\1785926926.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  ys = float(y_k.T @ s_k)\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_8464\\1785926926.py:19: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  alpha_i = rho_i * float(s_i.T @ q)\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_8464\\1785926926.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  H0 = float(s_last.T @ y_last) / float(y_last.T @ y_last)\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_8464\\1785926926.py:34: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  beta_i = rho_i * float(y_i.T @ r)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.7949332 , 0.63239939, 0.4028514 ])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_bfgs(np.array([0,0,0]), grad_f2, hess_f2, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "18e99798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2(np.array([1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6e1becf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BFGS:  1042527641122.1041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1., 0.]), None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum3(f2), print(\"BFGS: \", f2(bfgs(np.array([0,0,0]), grad_f2, hess_f2, 100)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7063c60a",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2eb0fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum2(f):\n",
    "    \"\"\"Finds approximate minimum of f(x)\"\"\"\n",
    "    xs = np.linspace(-3, 3, 1000)\n",
    "    ys = np.linspace(-3, 3, 1000)\n",
    "    X, Y = np.meshgrid(xs, ys)  \n",
    "    Z = f(np.array([X, Y]))  \n",
    "    \n",
    "    min_idx = np.unravel_index(np.argmin(Z), Z.shape)  \n",
    "    bx, by, bz = X[min_idx], Y[min_idx], Z[min_idx]\n",
    "    return np.array([bx, by, bz])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf59e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum3(f):\n",
    "    \"\"\"Finds approximate minimum of f(x)\"\"\"\n",
    "    xs = np.linspace(-3, 3, 100)\n",
    "    ys = np.linspace(-3, 3, 100)\n",
    "    zs = np.linspace(-3, 3, 100)\n",
    "    \n",
    "    X, Y, Z = np.meshgrid(xs, ys, zs)  \n",
    "    W = f(np.array([X, Y, Z]))  \n",
    "    \n",
    "    min_idx = np.unravel_index(np.argmin(W), W.shape)  \n",
    "    bx, by, bz, bw = X[min_idx], Y[min_idx], Z[min_idx], W[min_idx]\n",
    "    return np.array([bx, by, bz, bw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8be12379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 minimum: [-0.15151515 -0.21212121  0.15151515 -0.19651056]\n",
      "f2 minimum: [1. 1. 1. 0.]\n",
      "f3 minimum: [3.00000000e+00 5.01501502e-01 5.21242614e-05]\n"
     ]
    }
   ],
   "source": [
    "f1_xopt = minimum3(f1)\n",
    "f2_xopt = minimum3(f2)\n",
    "f3_xopt = minimum2(f3)\n",
    "print(\"f1 minimum:\", f1_xopt)\n",
    "print(\"f2 minimum:\", f2_xopt)\n",
    "print(\"f3 minimum:\", f3_xopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0293c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum(f):\n",
    "    if f == f1 or f == f2:\n",
    "        return minimum3(f)\n",
    "    return minimum2(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cac05bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_steps(f, grad_f, hess_f, x0, steps):\n",
    "    print(\"True minimum\", minimum(f)[-1], \"at \", minimum(f)[:-1])\n",
    "    for step in steps:\n",
    "        print(\"Steps, \", step)\n",
    "        print(\"GD: \", f(gd(x0, grad_f, learning_rate, step)))\n",
    "        print(\"Polyak: \", f(polyak(x0, grad_f, learning_rate, 0.05, step)))\n",
    "        print(\"Nesterov: \", f(nesterov(x0, f, grad_f, learning_rate, 0.05, step)))\n",
    "        print(\"Adagrad: \", f(adagrad(x0, grad_f, learning_rate, step)))\n",
    "        print(\"Newton: \", f(newton(x0, grad_f, hess_f, step)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9009436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True minimum -0.19651056014692375 at  [-0.15151515 -0.21212121  0.15151515]\n",
      "Steps,  2\n",
      "GD:  -0.035101752536\n",
      "Polyak:  -0.035101752536\n",
      "Nesterov:  -0.035101752536\n",
      "Adagrad:  -0.03156182988530955\n",
      "Newton:  -0.19791666666666666\n",
      "\n",
      "Steps,  5\n",
      "GD:  -0.07159791032768308\n",
      "Polyak:  -0.07159791032768308\n",
      "Nesterov:  -0.07159791032768308\n",
      "Adagrad:  -0.055560190582874064\n",
      "Newton:  -0.19791666666666666\n",
      "\n",
      "Steps,  10\n",
      "GD:  -0.10989895086006889\n",
      "Polyak:  -0.10989895086006889\n",
      "Nesterov:  -0.10989895086006889\n",
      "Adagrad:  -0.07927804062158532\n",
      "Newton:  -0.19791666666666669\n",
      "\n",
      "Steps,  100\n",
      "GD:  -0.1945330755830976\n",
      "Polyak:  -0.1945330755830976\n",
      "Nesterov:  -0.1945330755830976\n",
      "Adagrad:  -0.15969334544827538\n",
      "Newton:  -0.19791666666666669\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1x0 = np.array([0, 0, 0])\n",
    "f1x1 = np.array([1, 1, 0])\n",
    "f2x0 = np.array([1.2, 1.2, 1.2])\n",
    "f2x1 = np.array([-1, 1.2, 1.2])\n",
    "f3x0 = np.array([1,1])\n",
    "f3x1 = np.array([4.5, 4.5])\n",
    "\n",
    "steps = [2, 5, 10, 100]\n",
    "f = f1\n",
    "grad_f = grad_f1\n",
    "hess_f = hess_f1\n",
    "\n",
    "test_steps(f, grad_f, hess_f, f1x0, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e10ac6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True minimum -0.19651056014692375 at  [-0.15151515 -0.21212121  0.15151515]\n",
      "Steps,  2\n",
      "GD:  6.055305723432\n",
      "Polyak:  6.055305723432\n",
      "Nesterov:  6.055305723432\n",
      "Adagrad:  10.564434729571323\n",
      "Newton:  -0.1979166639129524\n",
      "\n",
      "Steps,  5\n",
      "GD:  3.7295389463962967\n",
      "Polyak:  3.7295389463962967\n",
      "Nesterov:  3.7295389463962967\n",
      "Adagrad:  10.19757537130621\n",
      "Newton:  -0.19791666666666669\n",
      "\n",
      "Steps,  10\n",
      "GD:  1.9655816298846445\n",
      "Polyak:  1.9655816298846445\n",
      "Nesterov:  1.9655816298846445\n",
      "Adagrad:  9.796470605386531\n",
      "Newton:  -0.19791666666666666\n",
      "\n",
      "Steps,  100\n",
      "GD:  -0.14664799141363444\n",
      "Polyak:  -0.14664799141363444\n",
      "Nesterov:  -0.14664799141363444\n",
      "Adagrad:  7.890825377305587\n",
      "Newton:  -0.19791666666666666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_steps(f, grad_f, hess_f, f1x1, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f765bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate(i):\n",
    "    return 0.001 * 0.99**i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8c15a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2 1.2 1.2]\n",
      "True minimum 0.0 at  [1. 1. 1.]\n",
      "Steps,  2\n",
      "GD:  0.04360204395724923\n",
      "Polyak:  0.04360204395724923\n",
      "Nesterov:  0.04360204395724923\n",
      "Adagrad:  11.212136336931973\n",
      "Newton:  6.486502125611452\n",
      "\n",
      "Steps,  5\n",
      "GD:  0.018222098858116126\n",
      "Polyak:  0.018222098858116126\n",
      "Nesterov:  0.018222098858116126\n",
      "Adagrad:  10.884354328434366\n",
      "Newton:  7.82723804484026\n",
      "\n",
      "Steps,  10\n",
      "GD:  0.018114796902295175\n",
      "Polyak:  0.018114796902295175\n",
      "Nesterov:  0.018114796902295175\n",
      "Adagrad:  10.524769246910026\n",
      "Newton:  32.57752757205745\n",
      "\n",
      "Steps,  100\n",
      "GD:  0.017367128919067866\n",
      "Polyak:  0.017367128919067866\n",
      "Nesterov:  0.017367128919067866\n",
      "Adagrad:  8.795304386701261\n",
      "Newton:  3523.7010595623265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = f2\n",
    "grad_f = grad_f2\n",
    "hess_f = hess_f2\n",
    "\n",
    "print(f2x0)\n",
    "test_steps(f, grad_f, hess_f, f2x0, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f03af44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True minimum 0.0 at  [1. 1. 1.]\n",
      "Steps,  2\n",
      "GD:  5.229583521614642\n",
      "Polyak:  5.229583521614642\n",
      "Nesterov:  5.229583521614642\n",
      "Adagrad:  13.332066264049159\n",
      "Newton:  13.335799674114092\n",
      "\n",
      "Steps,  5\n",
      "GD:  4.227341057837024\n",
      "Polyak:  4.227341057837024\n",
      "Nesterov:  4.227341057837024\n",
      "Adagrad:  12.938862640765567\n",
      "Newton:  20.325429321856642\n",
      "\n",
      "Steps,  10\n",
      "GD:  4.20332860424492\n",
      "Polyak:  4.20332860424492\n",
      "Nesterov:  4.20332860424492\n",
      "Adagrad:  12.50992146936056\n",
      "Newton:  47.99574664249103\n",
      "\n",
      "Steps,  100\n",
      "GD:  4.162038855665383\n",
      "Polyak:  4.162038855665383\n",
      "Nesterov:  4.162038855665383\n",
      "Adagrad:  10.487659531267003\n",
      "Newton:  3589.711514754642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_steps(f, grad_f, hess_f, f2x1, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81bf0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate(i):\n",
    "    return 0.00002 * 0.99**i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e04b971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True minimum 5.2124261432689534e-05 at  [3.        0.5015015]\n",
      "Steps,  2\n",
      "GD:  14.172429833366285\n",
      "Polyak:  14.172429833366285\n",
      "Nesterov:  14.172429833366285\n",
      "Adagrad:  14.20218149283947\n",
      "Newton:  14.196702713495865\n",
      "\n",
      "Steps,  5\n",
      "GD:  14.127347739705236\n",
      "Polyak:  14.127347739705236\n",
      "Nesterov:  14.127347739705236\n",
      "Adagrad:  14.201359799053826\n",
      "Newton:  14.24856149388566\n",
      "\n",
      "Steps,  10\n",
      "GD:  14.054718428955553\n",
      "Polyak:  14.054718428955553\n",
      "Nesterov:  14.054718428955553\n",
      "Adagrad:  14.200432977586338\n",
      "Newton:  14.203125211194463\n",
      "\n",
      "Steps,  100\n",
      "GD:  13.175395207257178\n",
      "Polyak:  13.175395207257178\n",
      "Nesterov:  13.175395207257178\n",
      "Adagrad:  14.19557041905663\n",
      "Newton:  14.203125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = f3\n",
    "grad_f = grad_f3\n",
    "hess_f = hess_f3\n",
    "\n",
    "test_steps(f, grad_f, hess_f, f3x0, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa674423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True minimum 5.2124261432689534e-05 at  [3.        0.5015015]\n",
      "Steps,  2\n",
      "GD:  12.43281596089588\n",
      "Polyak:  12.43281596089588\n",
      "Nesterov:  12.43281596089588\n",
      "Adagrad:  174802.90334024787\n",
      "Newton:  174787.39590967732\n",
      "\n",
      "Steps,  5\n",
      "GD:  12.513887697772656\n",
      "Polyak:  12.513887697772656\n",
      "Nesterov:  12.513887697772656\n",
      "Adagrad:  174793.794192175\n",
      "Newton:  174748.44359514368\n",
      "\n",
      "Steps,  10\n",
      "GD:  12.646195813283086\n",
      "Polyak:  12.646195813283086\n",
      "Nesterov:  12.646195813283086\n",
      "Adagrad:  174783.51999161392\n",
      "Newton:  174683.519713985\n",
      "\n",
      "Steps,  100\n",
      "GD:  14.418963181952417\n",
      "Polyak:  14.418963181952417\n",
      "Nesterov:  14.418963181952417\n",
      "Adagrad:  174729.62288745432\n",
      "Newton:  173514.16284713175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_steps(f, grad_f, hess_f, f3x1, steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b4c5f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time(f, grad_f, hess_f, x0, step, times):\n",
    "    print(\"True minimum\", f1_xopt[-1])\n",
    "    for _time in times:\n",
    "        print(\"Time, \", _time)\n",
    "        print(\"GD: \", f(gd(x0, grad_f, learning_rate, step, t=_time)))\n",
    "        print(\"Polyak: \", f(polyak(x0, grad_f, learning_rate, 0.9, step, t=_time)))\n",
    "        print(\"Nesterov: \", f(nesterov(x0, f, grad_f, learning_rate, 0.9, step, t=_time)))\n",
    "        print(\"Adagrad: \", f(adagrad(x0, grad_f, learning_rate, 0.9, step, t=_time)))\n",
    "        print(\"Newton: \", f(newton(x0, grad_f, hess_f, step, t=_time)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7546db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "\n",
    "times = [.1, 1, 2]\n",
    "f = f1\n",
    "grad_f = grad_f1\n",
    "hess_f = hess_f1\n",
    "step = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a2447",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a412aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(N):\n",
    "    return np.array([np.array([i, i + np.random.random(1).item()]) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4be77daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.mu = 0.02\n",
    "        self.steps = 1000\n",
    "\n",
    "    def gd(self, theta, loss_grad, lr_fn):\n",
    "        return gd(theta, loss_grad, lr_fn, self.steps)\n",
    "    \n",
    "    def newton(self, theta, loss_grad, hess_grad, lr):\n",
    "        return newton(theta, loss_grad, hess_grad, self.steps, lr=lr)\n",
    "\n",
    "    def sgd(self, theta, grad_f, lr_fn):\n",
    "        return sgd(theta, grad_f, lr_fn, n_steps=self.steps)\n",
    "\n",
    "    def bfgs(self, theta, grad_f, lr_fn):\n",
    "        return bfgs(theta, grad_f, lr_fn, n_steps=self.steps)\n",
    "\n",
    "    def l_bfgs(self, theta, grad_f, lr_fn):\n",
    "        return l_bfgs(theta, grad_f, lr_fn, n_steps=self.steps)\n",
    "\n",
    "\n",
    "    def fit(self, X, y, lr_fn, t=None):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Add bias column\n",
    "        X_aug = np.hstack([X, np.ones((n_samples, 1))])\n",
    "        theta = np.array([0,0])#np.random.rand(n_features + 1)\n",
    "\n",
    "        def loss_grad(theta):\n",
    "            print(theta)\n",
    "            preds = X_aug @ theta\n",
    "            grad = (2 / n_samples) * X_aug.T @ (preds - y)\n",
    "\n",
    "            # Print least squares\n",
    "            loss = np.mean((preds - y) ** 2)\n",
    "            print(\"Loss:\", loss)\n",
    "\n",
    "            return grad\n",
    "        \n",
    "        def sgd_loss_grad(theta):\n",
    "            i = np.random.randint(n_samples)\n",
    "            Xi = X_aug[i:i+1]  # shape (1, d+1)\n",
    "            yi = y[i]\n",
    "            pred = Xi @ theta\n",
    "            \n",
    "            print(theta)\n",
    "            loss = np.mean((X_aug @ theta - y) ** 2)\n",
    "            print(\"Loss:\", loss)\n",
    "            \n",
    "            return 2 * Xi.T @ (pred - yi)  # shape (d+1,)\n",
    "\n",
    "        def loss_hessian(theta):\n",
    "            return (2 / n_samples) * X_aug.T @ X_aug\n",
    "\n",
    "        # theta_opt = self.gd(theta, loss_grad, lr_fn)\n",
    "        # theta_opt = self.newton(theta, loss_grad, loss_hessian, .5)\n",
    "        # theta_opt = self.sgd(theta, sgd_loss_grad, lr_fn)\n",
    "        # theta_opt = self.bfgs(theta, loss_grad, lr_fn)\n",
    "        theta_opt = self.l_bfgs(theta, loss_grad, lr_fn)\n",
    "        \n",
    "        self.coef_ = theta_opt[:-1]\n",
    "        self.intercept_ = theta_opt[-1]\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "        return X @ self.coef_ + self.intercept_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0ffa08a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "Loss: 333318.10265745065\n",
      "[666151.27284686    999.98953104]\n",
      "Loss: 1.4769758969322883e+17\n",
      "Step 1: ||grad|| = 443435751544.7872\n",
      "[1.00072525 0.00150263]\n",
      "Loss: 0.15326204195053703\n",
      "Step 2: ||grad|| = 0.2620056552255507\n",
      "[1.00072525 0.00150302]\n",
      "Loss: 0.15326193882573197\n",
      "Step 3: ||grad|| = 0.2620054478326597\n",
      "[1.00072524 0.00150341]\n",
      "Loss: 0.15326183570108373\n",
      "Step 4: ||grad|| = 0.26200525073901537\n",
      "[1.00072524 0.00150381]\n",
      "Loss: 0.15326173257659292\n",
      "Step 5: ||grad|| = 0.26200505364551846\n",
      "[1.00072524 0.0015042 ]\n",
      "Loss: 0.1532616294522587\n",
      "Step 6: ||grad|| = 0.2620048565521736\n",
      "[1.00072524 0.0015046 ]\n",
      "Loss: 0.15326152632807472\n",
      "Step 7: ||grad|| = 0.2620046594589699\n",
      "[1.00072524 0.00150499]\n",
      "Loss: 0.1532614232040479\n",
      "Step 8: ||grad|| = 0.2620044623659177\n",
      "[1.00072524 0.00150538]\n",
      "Loss: 0.1532613200801772\n",
      "Step 9: ||grad|| = 0.2620042652730143\n",
      "[1.00072524 0.00150578]\n",
      "Loss: 0.1532612169564617\n",
      "Step 10: ||grad|| = 0.26200406818025884\n",
      "[1.00072524 0.00150617]\n",
      "Loss: 0.1532611138329007\n",
      "Step 11: ||grad|| = 0.26200387108765133\n",
      "[1.00072524 0.00150656]\n",
      "Loss: 0.15326101070949605\n",
      "Step 12: ||grad|| = 0.2620036739951932\n",
      "[1.00072524 0.00150696]\n",
      "Loss: 0.15326090758624492\n",
      "Step 13: ||grad|| = 0.2620034769028827\n",
      "[1.00072524 0.00150735]\n",
      "Loss: 0.15326080446315002\n",
      "Step 14: ||grad|| = 0.26200327981072086\n",
      "[1.00072524 0.00150774]\n",
      "Loss: 0.15326070134020864\n",
      "Step 15: ||grad|| = 0.26200308271870637\n",
      "[1.00072524 0.00150814]\n",
      "Loss: 0.15326059821742496\n",
      "Step 16: ||grad|| = 0.2620028856268402\n",
      "[1.00072524 0.00150853]\n",
      "Loss: 0.15326049509479456\n",
      "Step 17: ||grad|| = 0.26200268853512215\n",
      "[1.00072524 0.00150892]\n",
      "Loss: 0.15326039197231933\n",
      "Step 18: ||grad|| = 0.2620024914435539\n",
      "[1.00072524 0.00150932]\n",
      "Loss: 0.1532602888499999\n",
      "Step 19: ||grad|| = 0.262002294352133\n",
      "[1.00072524 0.00150971]\n",
      "Loss: 0.1532601857278355\n",
      "Step 20: ||grad|| = 0.2620020972608612\n",
      "[1.00072523 0.00151011]\n",
      "Loss: 0.15326008260582658\n",
      "Step 21: ||grad|| = 0.26200190016973585\n",
      "[1.00072523 0.0015105 ]\n",
      "Loss: 0.15325997948397294\n",
      "Step 22: ||grad|| = 0.2620017030787607\n",
      "[1.00072523 0.00151089]\n",
      "Loss: 0.15325987636227323\n",
      "Step 23: ||grad|| = 0.2620015059879316\n",
      "[1.00072523 0.00151129]\n",
      "Loss: 0.153259773240729\n",
      "Step 24: ||grad|| = 0.2620013088972519\n",
      "[1.00072523 0.00151168]\n",
      "Loss: 0.15325967011934144\n",
      "Step 25: ||grad|| = 0.26200111180672103\n",
      "[1.00072523 0.00151207]\n",
      "Loss: 0.15325956699810694\n",
      "Step 26: ||grad|| = 0.26200091471633796\n",
      "[1.00072523 0.00151247]\n",
      "Loss: 0.15325946387702846\n",
      "Step 27: ||grad|| = 0.2620007176261026\n",
      "[1.00072523 0.00151286]\n",
      "Loss: 0.15325936075610602\n",
      "Step 28: ||grad|| = 0.2620005205360162\n",
      "[1.00072523 0.00151325]\n",
      "Loss: 0.1532592576353381\n",
      "Step 29: ||grad|| = 0.26200032344607765\n",
      "[1.00072523 0.00151365]\n",
      "Loss: 0.15325915451472474\n",
      "Step 30: ||grad|| = 0.2620001263562875\n",
      "[1.00072523 0.00151404]\n",
      "Loss: 0.15325905139426663\n",
      "Step 31: ||grad|| = 0.26199992926664567\n",
      "[1.00072523 0.00151443]\n",
      "Loss: 0.1532589482739648\n",
      "Step 32: ||grad|| = 0.2619997321771526\n",
      "[1.00072523 0.00151483]\n",
      "Loss: 0.15325884515381674\n",
      "Step 33: ||grad|| = 0.26199953508780716\n",
      "[1.00072523 0.00151522]\n",
      "Loss: 0.15325874203382417\n",
      "Step 34: ||grad|| = 0.26199933799860886\n",
      "[1.00072523 0.00151562]\n",
      "Loss: 0.1532586389139869\n",
      "Step 35: ||grad|| = 0.2619991409095609\n",
      "[1.00072523 0.00151601]\n",
      "Loss: 0.15325853579430485\n",
      "Step 36: ||grad|| = 0.26199894382066047\n",
      "[1.00072523 0.0015164 ]\n",
      "Loss: 0.15325843267477784\n",
      "Step 37: ||grad|| = 0.26199874673190854\n",
      "[1.00072522 0.0015168 ]\n",
      "Loss: 0.15325832955540603\n",
      "Step 38: ||grad|| = 0.26199854964330427\n",
      "[1.00072522 0.00151719]\n",
      "Loss: 0.15325822643618944\n",
      "Step 39: ||grad|| = 0.26199835255484843\n",
      "[1.00072522 0.00151758]\n",
      "Loss: 0.1532581233171286\n",
      "Step 40: ||grad|| = 0.2619981554665415\n",
      "[1.00072522 0.00151798]\n",
      "Loss: 0.1532580201982212\n",
      "Step 41: ||grad|| = 0.26199795837838197\n",
      "[1.00072522 0.00151837]\n",
      "Loss: 0.15325791707947134\n",
      "Step 42: ||grad|| = 0.2619977612903713\n",
      "[1.00072522 0.00151876]\n",
      "Loss: 0.15325781396087446\n",
      "Step 43: ||grad|| = 0.26199756420250914\n",
      "[1.00072522 0.00151916]\n",
      "Loss: 0.1532577108424333\n",
      "Step 44: ||grad|| = 0.26199736711479366\n",
      "[1.00072522 0.00151955]\n",
      "Loss: 0.15325760772414862\n",
      "Step 45: ||grad|| = 0.261997170027228\n",
      "[1.00072522 0.00151995]\n",
      "Loss: 0.1532575046060163\n",
      "Step 46: ||grad|| = 0.26199697293981017\n",
      "[1.00072522 0.00152034]\n",
      "Loss: 0.15325740148804112\n",
      "Step 47: ||grad|| = 0.2619967758525404\n",
      "[1.00072522 0.00152073]\n",
      "Loss: 0.15325729837022112\n",
      "Step 48: ||grad|| = 0.2619965787654187\n",
      "[1.00072522 0.00152113]\n",
      "Loss: 0.15325719525255568\n",
      "Step 49: ||grad|| = 0.2619963816784465\n",
      "[1.00072522 0.00152152]\n",
      "Loss: 0.1532570921350457\n",
      "Step 50: ||grad|| = 0.2619961845916215\n",
      "[1.00072522 0.00152191]\n",
      "Loss: 0.15325698901769105\n",
      "Step 51: ||grad|| = 0.26199598750494396\n",
      "[1.00072522 0.00152231]\n",
      "Loss: 0.1532568859004908\n",
      "Step 52: ||grad|| = 0.26199579041841575\n",
      "[1.00072522 0.0015227 ]\n",
      "Loss: 0.1532567827834464\n",
      "Step 53: ||grad|| = 0.2619955933320355\n",
      "[1.00072521 0.00152309]\n",
      "Loss: 0.1532566796665567\n",
      "Step 54: ||grad|| = 0.2619953962458038\n",
      "[1.00072521 0.00152349]\n",
      "Loss: 0.1532565765498216\n",
      "Step 55: ||grad|| = 0.26199519915972047\n",
      "[1.00072521 0.00152388]\n",
      "Loss: 0.1532564734332437\n",
      "Step 56: ||grad|| = 0.2619950020737852\n",
      "[1.00072521 0.00152427]\n",
      "Loss: 0.1532563703168183\n",
      "Step 57: ||grad|| = 0.2619948049879974\n",
      "[1.00072521 0.00152467]\n",
      "Loss: 0.1532562672005503\n",
      "Step 58: ||grad|| = 0.26199460790235907\n",
      "[1.00072521 0.00152506]\n",
      "Loss: 0.15325616408443607\n",
      "Step 59: ||grad|| = 0.2619944108168683\n",
      "[1.00072521 0.00152546]\n",
      "Loss: 0.1532560609684785\n",
      "Step 60: ||grad|| = 0.2619942137315258\n",
      "[1.00072521 0.00152585]\n",
      "Loss: 0.1532559578526743\n",
      "Step 61: ||grad|| = 0.26199401664633276\n",
      "[1.00072521 0.00152624]\n",
      "Loss: 0.15325585473702544\n",
      "Step 62: ||grad|| = 0.26199381956128687\n",
      "[1.00072521 0.00152664]\n",
      "Loss: 0.15325575162153218\n",
      "Step 63: ||grad|| = 0.26199362247638913\n",
      "[1.00072521 0.00152703]\n",
      "Loss: 0.15325564850619458\n",
      "Step 64: ||grad|| = 0.2619934253916397\n",
      "[1.00072521 0.00152742]\n",
      "Loss: 0.153255545391011\n",
      "Step 65: ||grad|| = 0.2619932283070383\n",
      "[1.00072521 0.00152782]\n",
      "Loss: 0.15325544227598353\n",
      "Step 66: ||grad|| = 0.261993031222585\n",
      "[1.00072521 0.00152821]\n",
      "Loss: 0.15325533916111062\n",
      "Step 67: ||grad|| = 0.26199283413828156\n",
      "[1.00072521 0.0015286 ]\n",
      "Loss: 0.15325523604639193\n",
      "Step 68: ||grad|| = 0.2619926370541238\n",
      "[1.00072521 0.001529  ]\n",
      "Loss: 0.15325513293182996\n",
      "Step 69: ||grad|| = 0.26199243997011634\n",
      "[1.00072521 0.00152939]\n",
      "Loss: 0.15325502981742323\n",
      "Step 70: ||grad|| = 0.2619922428862575\n",
      "[1.0007252  0.00152978]\n",
      "Loss: 0.15325492670317065\n",
      "Step 71: ||grad|| = 0.26199204580254576\n",
      "[1.0007252  0.00153018]\n",
      "Loss: 0.15325482358907294\n",
      "Step 72: ||grad|| = 0.2619918487189826\n",
      "[1.0007252  0.00153057]\n",
      "Loss: 0.15325472047513108\n",
      "Step 73: ||grad|| = 0.26199165163556715\n",
      "[1.0007252  0.00153097]\n",
      "Loss: 0.1532546173613448\n",
      "Step 74: ||grad|| = 0.26199145455230083\n",
      "[1.0007252  0.00153136]\n",
      "Loss: 0.15325451424771222\n",
      "Step 75: ||grad|| = 0.26199125746918167\n",
      "[1.0007252  0.00153175]\n",
      "Loss: 0.15325441113423635\n",
      "Step 76: ||grad|| = 0.26199106038621245\n",
      "[1.0007252  0.00153215]\n",
      "Loss: 0.15325430802091528\n",
      "Step 77: ||grad|| = 0.2619908633033907\n",
      "[1.0007252  0.00153254]\n",
      "Loss: 0.15325420490774944\n",
      "Step 78: ||grad|| = 0.26199066622071676\n",
      "[1.0007252  0.00153293]\n",
      "Loss: 0.15325410179473847\n",
      "Step 79: ||grad|| = 0.2619904691381919\n",
      "[1.0007252  0.00153333]\n",
      "Loss: 0.15325399868188222\n",
      "Step 80: ||grad|| = 0.2619902720558158\n",
      "[1.0007252  0.00153372]\n",
      "Loss: 0.15325389556918106\n",
      "Step 81: ||grad|| = 0.2619900749735856\n",
      "[1.0007252  0.00153411]\n",
      "Loss: 0.15325379245663465\n",
      "Step 82: ||grad|| = 0.2619898778915044\n",
      "[1.0007252  0.00153451]\n",
      "Loss: 0.15325368934424508\n",
      "Step 83: ||grad|| = 0.2619896808095724\n",
      "[1.0007252 0.0015349]\n",
      "Loss: 0.15325358623200902\n",
      "Step 84: ||grad|| = 0.2619894837277881\n",
      "[1.0007252  0.00153529]\n",
      "Loss: 0.15325348311992826\n",
      "Step 85: ||grad|| = 0.26198928664615206\n",
      "[1.0007252  0.00153569]\n",
      "Loss: 0.15325338000800418\n",
      "Step 86: ||grad|| = 0.261989089564665\n",
      "[1.0007252  0.00153608]\n",
      "Loss: 0.15325327689623303\n",
      "Step 87: ||grad|| = 0.2619888924833254\n",
      "[1.00072519 0.00153648]\n",
      "Loss: 0.15325317378461875\n",
      "Step 88: ||grad|| = 0.2619886954021341\n",
      "[1.00072519 0.00153687]\n",
      "Loss: 0.15325307067315855\n",
      "Step 89: ||grad|| = 0.26198849832109156\n",
      "[1.00072519 0.00153726]\n",
      "Loss: 0.1532529675618541\n",
      "Step 90: ||grad|| = 0.26198830124019684\n",
      "[1.00072519 0.00153766]\n",
      "Loss: 0.153252864450705\n",
      "Step 91: ||grad|| = 0.26198810415945034\n",
      "[1.00072519 0.00153805]\n",
      "Loss: 0.15325276133971055\n",
      "Step 92: ||grad|| = 0.26198790707885217\n",
      "[1.00072519 0.00153844]\n",
      "Loss: 0.15325265822887146\n",
      "Step 93: ||grad|| = 0.26198770999840215\n",
      "[1.00072519 0.00153884]\n",
      "Loss: 0.15325255511818647\n",
      "Step 94: ||grad|| = 0.26198751291810013\n",
      "[1.00072519 0.00153923]\n",
      "Loss: 0.15325245200765777\n",
      "Step 95: ||grad|| = 0.2619873158379475\n",
      "[1.00072519 0.00153962]\n",
      "Loss: 0.1532523488972843\n",
      "Step 96: ||grad|| = 0.26198711875794195\n",
      "[1.00072519 0.00154002]\n",
      "Loss: 0.1532522457870645\n",
      "Step 97: ||grad|| = 0.2619869216780851\n",
      "[1.00072519 0.00154041]\n",
      "Loss: 0.15325214267700144\n",
      "Step 98: ||grad|| = 0.2619867245983773\n",
      "[1.00072519 0.0015408 ]\n",
      "Loss: 0.15325203956709282\n",
      "Step 99: ||grad|| = 0.26198652751881657\n",
      "[1.00072519 0.0015412 ]\n",
      "Loss: 0.15325193645733873\n",
      "Step 100: ||grad|| = 0.26198633043940395\n",
      "[1.00072519 0.00154159]\n",
      "Loss: 0.15325183334774098\n",
      "Step 101: ||grad|| = 0.2619861333601402\n",
      "[1.00072519 0.00154199]\n",
      "Loss: 0.1532517302382993\n",
      "Step 102: ||grad|| = 0.26198593628102446\n",
      "[1.00072519 0.00154238]\n",
      "Loss: 0.1532516271290106\n",
      "Step 103: ||grad|| = 0.26198573920205703\n",
      "[1.00072519 0.00154277]\n",
      "Loss: 0.15325152401987724\n",
      "Step 104: ||grad|| = 0.2619855421232383\n",
      "[1.00072518 0.00154317]\n",
      "Loss: 0.15325142091089983\n",
      "Step 105: ||grad|| = 0.2619853450445677\n",
      "[1.00072518 0.00154356]\n",
      "Loss: 0.15325131780207651\n",
      "Step 106: ||grad|| = 0.2619851479660453\n",
      "[1.00072518 0.00154395]\n",
      "Loss: 0.15325121469340927\n",
      "Step 107: ||grad|| = 0.26198495088767126\n",
      "[1.00072518 0.00154435]\n",
      "Loss: 0.15325111158489743\n",
      "Step 108: ||grad|| = 0.26198475380944497\n",
      "[1.00072518 0.00154474]\n",
      "Loss: 0.15325100847653944\n",
      "Step 109: ||grad|| = 0.2619845567313673\n",
      "[1.00072518 0.00154513]\n",
      "Loss: 0.15325090536833766\n",
      "Step 110: ||grad|| = 0.26198435965343725\n",
      "[1.00072518 0.00154553]\n",
      "Loss: 0.1532508022602907\n",
      "Step 111: ||grad|| = 0.26198416257565604\n",
      "[1.00072518 0.00154592]\n",
      "Loss: 0.15325069915239914\n",
      "Step 112: ||grad|| = 0.2619839654980227\n",
      "[1.00072518 0.00154631]\n",
      "Loss: 0.1532505960446622\n",
      "Step 113: ||grad|| = 0.2619837684205384\n",
      "[1.00072518 0.00154671]\n",
      "Loss: 0.1532504929370804\n",
      "Step 114: ||grad|| = 0.2619835713432015\n",
      "[1.00072518 0.0015471 ]\n",
      "Loss: 0.1532503898296534\n",
      "Step 115: ||grad|| = 0.2619833742660129\n",
      "[1.00072518 0.0015475 ]\n",
      "Loss: 0.1532502867223833\n",
      "Step 116: ||grad|| = 0.26198317718897357\n",
      "[1.00072518 0.00154789]\n",
      "Loss: 0.1532501836152664\n",
      "Step 117: ||grad|| = 0.26198298011208127\n",
      "[1.00072518 0.00154828]\n",
      "Loss: 0.15325008050830682\n",
      "Step 118: ||grad|| = 0.2619827830353389\n",
      "[1.00072518 0.00154868]\n",
      "Loss: 0.153249977401499\n",
      "Step 119: ||grad|| = 0.26198258595874235\n",
      "[1.00072518 0.00154907]\n",
      "Loss: 0.15324987429484896\n",
      "Step 120: ||grad|| = 0.2619823888822953\n",
      "[1.00072518 0.00154946]\n",
      "Loss: 0.15324977118835328\n",
      "Step 121: ||grad|| = 0.26198219180599774\n",
      "[1.00072517 0.00154986]\n",
      "Loss: 0.15324966808201265\n",
      "Step 122: ||grad|| = 0.26198199472984607\n",
      "[1.00072517 0.00155025]\n",
      "Loss: 0.1532495649758269\n",
      "Step 123: ||grad|| = 0.2619817976538426\n",
      "[1.00072517 0.00155064]\n",
      "Loss: 0.153249461869796\n",
      "Step 124: ||grad|| = 0.2619816005779899\n",
      "[1.00072517 0.00155104]\n",
      "Loss: 0.15324935876392143\n",
      "Step 125: ||grad|| = 0.2619814035022841\n",
      "[1.00072517 0.00155143]\n",
      "Loss: 0.15324925565820174\n",
      "Step 126: ||grad|| = 0.26198120642672673\n",
      "[1.00072517 0.00155182]\n",
      "Loss: 0.15324915255263596\n",
      "Step 127: ||grad|| = 0.26198100935131613\n",
      "[1.00072517 0.00155222]\n",
      "Loss: 0.15324904944722698\n",
      "Step 128: ||grad|| = 0.261980812276056\n",
      "[1.00072517 0.00155261]\n",
      "Loss: 0.15324894634197153\n",
      "Step 129: ||grad|| = 0.2619806152009436\n",
      "[1.00072517 0.00155301]\n",
      "Loss: 0.15324884323687274\n",
      "Step 130: ||grad|| = 0.261980418125978\n",
      "[1.00072517 0.0015534 ]\n",
      "Loss: 0.15324874013192788\n",
      "Step 131: ||grad|| = 0.26198022105116264\n",
      "[1.00072517 0.00155379]\n",
      "Loss: 0.15324863702713834\n",
      "Step 132: ||grad|| = 0.2619800239764938\n",
      "[1.00072517 0.00155419]\n",
      "Loss: 0.15324853392250465\n",
      "Step 133: ||grad|| = 0.2619798269019748\n",
      "[1.00072517 0.00155458]\n",
      "Loss: 0.15324843081802453\n",
      "Step 134: ||grad|| = 0.2619796298276031\n",
      "[1.00072517 0.00155497]\n",
      "Loss: 0.1532483277137021\n",
      "Step 135: ||grad|| = 0.2619794327533817\n",
      "[1.00072517 0.00155537]\n",
      "Loss: 0.15324822460953208\n",
      "Step 136: ||grad|| = 0.26197923567930476\n",
      "[1.00072517 0.00155576]\n",
      "Loss: 0.15324812150551873\n",
      "Step 137: ||grad|| = 0.2619790386053783\n",
      "[1.00072517 0.00155615]\n",
      "Loss: 0.15324801840165975\n",
      "Step 138: ||grad|| = 0.2619788415315991\n",
      "[1.00072516 0.00155655]\n",
      "Loss: 0.15324791529795578\n",
      "Step 139: ||grad|| = 0.2619786444579693\n",
      "[1.00072516 0.00155694]\n",
      "Loss: 0.15324781219440772\n",
      "Step 140: ||grad|| = 0.26197844738448706\n",
      "[1.00072516 0.00155733]\n",
      "Loss: 0.15324770909101457\n",
      "Step 141: ||grad|| = 0.2619782503111534\n",
      "[1.00072516 0.00155773]\n",
      "Loss: 0.15324760598777676\n",
      "Step 142: ||grad|| = 0.2619780532379678\n",
      "[1.00072516 0.00155812]\n",
      "Loss: 0.1532475028846937\n",
      "Step 143: ||grad|| = 0.26197785616493013\n",
      "[1.00072516 0.00155851]\n",
      "Loss: 0.1532473997817654\n",
      "Step 144: ||grad|| = 0.261977659092041\n",
      "[1.00072516 0.00155891]\n",
      "Loss: 0.15324729667899226\n",
      "Step 145: ||grad|| = 0.2619774620193003\n",
      "[1.00072516 0.0015593 ]\n",
      "Loss: 0.1532471935763746\n",
      "Step 146: ||grad|| = 0.2619772649467078\n",
      "[1.00072516 0.0015597 ]\n",
      "Loss: 0.15324709047391188\n",
      "Step 147: ||grad|| = 0.26197706787426406\n",
      "[1.00072516 0.00156009]\n",
      "Loss: 0.1532469873716052\n",
      "Step 148: ||grad|| = 0.2619768708019674\n",
      "[1.00072516 0.00156048]\n",
      "Loss: 0.15324688426945282\n",
      "Step 149: ||grad|| = 0.26197667372981914\n",
      "[1.00072516 0.00156088]\n",
      "Loss: 0.15324678116745463\n",
      "Step 150: ||grad|| = 0.2619764766578195\n",
      "[1.00072516 0.00156127]\n",
      "Loss: 0.1532466780656137\n",
      "Step 151: ||grad|| = 0.261976279585969\n",
      "[1.00072516 0.00156166]\n",
      "Loss: 0.1532465749639265\n",
      "Step 152: ||grad|| = 0.26197608251426574\n",
      "[1.00072516 0.00156206]\n",
      "Loss: 0.15324647186239493\n",
      "Step 153: ||grad|| = 0.26197588544271105\n",
      "[1.00072516 0.00156245]\n",
      "Loss: 0.15324636876101771\n",
      "Step 154: ||grad|| = 0.2619756883713044\n",
      "[1.00072516 0.00156284]\n",
      "Loss: 0.1532462656597962\n",
      "Step 155: ||grad|| = 0.2619754913000457\n",
      "[1.00072515 0.00156324]\n",
      "Loss: 0.15324616255872914\n",
      "Step 156: ||grad|| = 0.2619752942289362\n",
      "[1.00072515 0.00156363]\n",
      "Loss: 0.1532460594578181\n",
      "Step 157: ||grad|| = 0.26197509715797296\n",
      "[1.00072515 0.00156402]\n",
      "Loss: 0.1532459563570616\n",
      "Step 158: ||grad|| = 0.2619749000871605\n",
      "[1.00072515 0.00156442]\n",
      "Loss: 0.15324585325646284\n",
      "Step 159: ||grad|| = 0.2619747030164991\n",
      "[1.00072515 0.00156481]\n",
      "Loss: 0.15324575015601397\n",
      "Step 160: ||grad|| = 0.26197450594597893\n",
      "[1.00072515 0.00156521]\n",
      "Loss: 0.15324564705572338\n",
      "Step 161: ||grad|| = 0.2619743088756095\n",
      "[1.00072515 0.0015656 ]\n",
      "Loss: 0.15324554395558773\n",
      "Step 162: ||grad|| = 0.26197411180538904\n",
      "[1.00072515 0.00156599]\n",
      "Loss: 0.15324544085560746\n",
      "Step 163: ||grad|| = 0.2619739147353165\n",
      "[1.00072515 0.00156639]\n",
      "Loss: 0.15324533775578122\n",
      "Step 164: ||grad|| = 0.2619737176653916\n",
      "[1.00072515 0.00156678]\n",
      "Loss: 0.1532452346561108\n",
      "Step 165: ||grad|| = 0.26197352059561557\n",
      "[1.00072515 0.00156717]\n",
      "Loss: 0.15324513155659508\n",
      "Step 166: ||grad|| = 0.26197332352598873\n",
      "[1.00072515 0.00156757]\n",
      "Loss: 0.15324502845723487\n",
      "Step 167: ||grad|| = 0.2619731264565093\n",
      "[1.00072515 0.00156796]\n",
      "Loss: 0.15324492535802992\n",
      "Step 168: ||grad|| = 0.26197292938717787\n",
      "[1.00072515 0.00156835]\n",
      "Loss: 0.1532448222589788\n",
      "Step 169: ||grad|| = 0.2619727323179951\n",
      "[1.00072515 0.00156875]\n",
      "Loss: 0.15324471916008492\n",
      "Step 170: ||grad|| = 0.26197253524896064\n",
      "[1.00072515 0.00156914]\n",
      "Loss: 0.15324461606134532\n",
      "Step 171: ||grad|| = 0.26197233818007426\n",
      "[1.00072515 0.00156953]\n",
      "Loss: 0.1532445129627606\n",
      "Step 172: ||grad|| = 0.261972141111336\n",
      "[1.00072514 0.00156993]\n",
      "Loss: 0.15324440986433074\n",
      "Step 173: ||grad|| = 0.2619719440427461\n",
      "[1.00072514 0.00157032]\n",
      "Loss: 0.15324430676605613\n",
      "Step 174: ||grad|| = 0.26197174697430436\n",
      "[1.00072514 0.00157072]\n",
      "Loss: 0.15324420366793692\n",
      "Step 175: ||grad|| = 0.2619715499060103\n",
      "[1.00072514 0.00157111]\n",
      "Loss: 0.15324410056997206\n",
      "Step 176: ||grad|| = 0.26197135283786505\n",
      "[1.00072514 0.0015715 ]\n",
      "Loss: 0.1532439974721643\n",
      "Step 177: ||grad|| = 0.2619711557698688\n",
      "[1.00072514 0.0015719 ]\n",
      "Loss: 0.15324389437450883\n",
      "Step 178: ||grad|| = 0.2619709587020186\n",
      "[1.00072514 0.00157229]\n",
      "Loss: 0.15324379127701027\n",
      "Step 179: ||grad|| = 0.26197076163431965\n",
      "[1.00072514 0.00157268]\n",
      "Loss: 0.1532436881796661\n",
      "Step 180: ||grad|| = 0.2619705645667675\n",
      "[1.00072514 0.00157308]\n",
      "Loss: 0.15324358508247815\n",
      "Step 181: ||grad|| = 0.2619703674993633\n",
      "[1.00072514 0.00157347]\n",
      "Loss: 0.15324348198544355\n",
      "Step 182: ||grad|| = 0.2619701704321078\n",
      "[1.00072514 0.00157386]\n",
      "Loss: 0.15324337888856526\n",
      "Step 183: ||grad|| = 0.2619699733650009\n",
      "[1.00072514 0.00157426]\n",
      "Loss: 0.15324327579184205\n",
      "Step 184: ||grad|| = 0.26196977629804147\n",
      "[1.00072514 0.00157465]\n",
      "Loss: 0.15324317269527432\n",
      "Step 185: ||grad|| = 0.2619695792312301\n",
      "[1.00072514 0.00157504]\n",
      "Loss: 0.15324306959886155\n",
      "Step 186: ||grad|| = 0.26196938216456744\n",
      "[1.00072514 0.00157544]\n",
      "Loss: 0.15324296650260247\n",
      "Step 187: ||grad|| = 0.26196918509805334\n",
      "[1.00072514 0.00157583]\n",
      "Loss: 0.1532428634065007\n",
      "Step 188: ||grad|| = 0.26196898803168667\n",
      "[1.00072514 0.00157622]\n",
      "Loss: 0.15324276031055212\n",
      "Step 189: ||grad|| = 0.26196879096546866\n",
      "[1.00072513 0.00157662]\n",
      "Loss: 0.15324265721475977\n",
      "Step 190: ||grad|| = 0.26196859389939836\n",
      "[1.00072513 0.00157701]\n",
      "Loss: 0.1532425541191219\n",
      "Step 191: ||grad|| = 0.2619683968334774\n",
      "[1.00072513 0.00157741]\n",
      "Loss: 0.15324245102363918\n",
      "Step 192: ||grad|| = 0.26196819976770386\n",
      "[1.00072513 0.0015778 ]\n",
      "Loss: 0.1532423479283123\n",
      "Step 193: ||grad|| = 0.261968002702079\n",
      "[1.00072513 0.00157819]\n",
      "Loss: 0.15324224483314483\n",
      "Step 194: ||grad|| = 0.26196780563661193\n",
      "[1.00072513 0.00157859]\n",
      "Loss: 0.15324214173812187\n",
      "Step 195: ||grad|| = 0.26196760857127316\n",
      "[1.00072513 0.00157898]\n",
      "Loss: 0.15324203864326\n",
      "Step 196: ||grad|| = 0.2619674115060933\n",
      "[1.00072513 0.00157937]\n",
      "Loss: 0.1532419355485528\n",
      "Step 197: ||grad|| = 0.2619672144410613\n",
      "[1.00072513 0.00157977]\n",
      "Loss: 0.153241832454001\n",
      "Step 198: ||grad|| = 0.2619670173761771\n",
      "[1.00072513 0.00158016]\n",
      "Loss: 0.15324172935960462\n",
      "Step 199: ||grad|| = 0.2619668203114422\n",
      "[1.00072513 0.00158055]\n",
      "Loss: 0.15324162626536342\n",
      "Step 200: ||grad|| = 0.26196662324685427\n",
      "[1.00072513 0.00158095]\n",
      "Loss: 0.1532415231712763\n",
      "Step 201: ||grad|| = 0.2619664261824142\n",
      "[1.00072513 0.00158134]\n",
      "Loss: 0.15324142007734526\n",
      "Step 202: ||grad|| = 0.2619662291181243\n",
      "[1.00072513 0.00158173]\n",
      "Loss: 0.15324131698356888\n",
      "Step 203: ||grad|| = 0.26196603205398217\n",
      "[1.00072513 0.00158213]\n",
      "Loss: 0.1532412138899477\n",
      "Step 204: ||grad|| = 0.2619658349899881\n",
      "[1.00072513 0.00158252]\n",
      "Loss: 0.1532411107964809\n",
      "Step 205: ||grad|| = 0.2619656379261415\n",
      "[1.00072513 0.00158291]\n",
      "Loss: 0.15324100770317034\n",
      "Step 206: ||grad|| = 0.26196544086244405\n",
      "[1.00072512 0.00158331]\n",
      "Loss: 0.15324090461001413\n",
      "Step 207: ||grad|| = 0.261965243798894\n",
      "[1.00072512 0.0015837 ]\n",
      "Loss: 0.15324080151701436\n",
      "Step 208: ||grad|| = 0.261965046735493\n",
      "[1.00072512 0.0015841 ]\n",
      "Loss: 0.1532406984241676\n",
      "Step 209: ||grad|| = 0.2619648496722396\n",
      "[1.00072512 0.00158449]\n",
      "Loss: 0.1532405953314771\n",
      "Step 210: ||grad|| = 0.2619646526091323\n",
      "[1.00072512 0.00158488]\n",
      "Loss: 0.1532404922389412\n",
      "Step 211: ||grad|| = 0.2619644555461778\n",
      "[1.00072512 0.00158528]\n",
      "Loss: 0.15324038914656155\n",
      "Step 212: ||grad|| = 0.2619642584833696\n",
      "[1.00072512 0.00158567]\n",
      "Loss: 0.15324028605433562\n",
      "Step 213: ||grad|| = 0.26196406142070955\n",
      "[1.00072512 0.00158606]\n",
      "Loss: 0.15324018296226558\n",
      "Step 214: ||grad|| = 0.2619638643581973\n",
      "[1.00072512 0.00158646]\n",
      "Loss: 0.1532400798703507\n",
      "Step 215: ||grad|| = 0.261963667295833\n",
      "[1.00072512 0.00158685]\n",
      "Loss: 0.15323997677859053\n",
      "Step 216: ||grad|| = 0.2619634702336184\n",
      "[1.00072512 0.00158724]\n",
      "Loss: 0.153239873686985\n",
      "Step 217: ||grad|| = 0.2619632731715508\n",
      "[1.00072512 0.00158764]\n",
      "Loss: 0.15323977059553553\n",
      "Step 218: ||grad|| = 0.2619630761096319\n",
      "[1.00072512 0.00158803]\n",
      "Loss: 0.15323966750424087\n",
      "Step 219: ||grad|| = 0.26196287904786125\n",
      "[1.00072512 0.00158842]\n",
      "Loss: 0.15323956441310538\n",
      "Step 220: ||grad|| = 0.26196268198624534\n",
      "[1.00072512 0.00158882]\n",
      "Loss: 0.15323946132211694\n",
      "Step 221: ||grad|| = 0.26196248492476465\n",
      "[1.00072512 0.00158921]\n",
      "Loss: 0.15323935823128756\n",
      "Step 222: ||grad|| = 0.2619622878634381\n",
      "[1.00072512 0.0015896 ]\n",
      "Loss: 0.15323925514061293\n",
      "Step 223: ||grad|| = 0.26196209080226024\n",
      "[1.00072511 0.00159   ]\n",
      "Loss: 0.15323915205009336\n",
      "Step 224: ||grad|| = 0.2619618937412303\n",
      "[1.00072511 0.00159039]\n",
      "Loss: 0.15323904895972998\n",
      "Step 225: ||grad|| = 0.26196169668034935\n",
      "[1.00072511 0.00159079]\n",
      "Loss: 0.1532389458695211\n",
      "Step 226: ||grad|| = 0.26196149961961607\n",
      "[1.00072511 0.00159118]\n",
      "Loss: 0.15323884277946695\n",
      "Step 227: ||grad|| = 0.2619613025590314\n",
      "[1.00072511 0.00159157]\n",
      "Loss: 0.15323873968956842\n",
      "Step 228: ||grad|| = 0.26196110549859436\n",
      "[1.00072511 0.00159197]\n",
      "Loss: 0.15323863659982423\n",
      "Step 229: ||grad|| = 0.2619609084383064\n",
      "[1.00072511 0.00159236]\n",
      "Loss: 0.15323853351023592\n",
      "Step 230: ||grad|| = 0.26196071137816634\n",
      "[1.00072511 0.00159275]\n",
      "Loss: 0.15323843042080237\n",
      "Step 231: ||grad|| = 0.26196051431817424\n",
      "[1.00072511 0.00159315]\n",
      "Loss: 0.15323832733152457\n",
      "Step 232: ||grad|| = 0.2619603172583309\n",
      "[1.00072511 0.00159354]\n",
      "Loss: 0.15323822424240086\n",
      "Step 233: ||grad|| = 0.261960120198636\n",
      "[1.00072511 0.00159393]\n",
      "Loss: 0.1532381211534324\n",
      "Step 234: ||grad|| = 0.26195992313908817\n",
      "[1.00072511 0.00159433]\n",
      "Loss: 0.15323801806461898\n",
      "Step 235: ||grad|| = 0.26195972607969015\n",
      "[1.00072511 0.00159472]\n",
      "Loss: 0.1532379149759614\n",
      "Step 236: ||grad|| = 0.2619595290204386\n",
      "[1.00072511 0.00159511]\n",
      "Loss: 0.15323781188745864\n",
      "Step 237: ||grad|| = 0.26195933196133603\n",
      "[1.00072511 0.00159551]\n",
      "Loss: 0.15323770879911064\n",
      "Step 238: ||grad|| = 0.2619591349023817\n",
      "[1.00072511 0.0015959 ]\n",
      "Loss: 0.15323760571091755\n",
      "Step 239: ||grad|| = 0.2619589378435762\n",
      "[1.00072511 0.00159629]\n",
      "Loss: 0.15323750262287986\n",
      "Step 240: ||grad|| = 0.2619587407849188\n",
      "[1.0007251  0.00159669]\n",
      "Loss: 0.15323739953499735\n",
      "Step 241: ||grad|| = 0.26195854372640875\n",
      "[1.0007251  0.00159708]\n",
      "Loss: 0.15323729644726966\n",
      "Step 242: ||grad|| = 0.2619583466680471\n",
      "[1.0007251  0.00159748]\n",
      "Loss: 0.15323719335969804\n",
      "Step 243: ||grad|| = 0.2619581496098346\n",
      "[1.0007251  0.00159787]\n",
      "Loss: 0.15323709027228025\n",
      "Step 244: ||grad|| = 0.2619579525517701\n",
      "[1.0007251  0.00159826]\n",
      "Loss: 0.15323698718501844\n",
      "Step 245: ||grad|| = 0.26195775549385353\n",
      "[1.0007251  0.00159866]\n",
      "Loss: 0.1532368840979109\n",
      "Step 246: ||grad|| = 0.26195755843608465\n",
      "[1.0007251  0.00159905]\n",
      "Loss: 0.15323678101095903\n",
      "Step 247: ||grad|| = 0.26195736137846454\n",
      "[1.0007251  0.00159944]\n",
      "Loss: 0.1532366779241626\n",
      "Step 248: ||grad|| = 0.26195716432099364\n",
      "[1.0007251  0.00159984]\n",
      "Loss: 0.1532365748375204\n",
      "Step 249: ||grad|| = 0.2619569672636691\n",
      "[1.0007251  0.00160023]\n",
      "Loss: 0.153236471751034\n",
      "Step 250: ||grad|| = 0.2619567702064939\n",
      "[1.0007251  0.00160062]\n",
      "Loss: 0.1532363686647028\n",
      "Step 251: ||grad|| = 0.2619565731494675\n",
      "[1.0007251  0.00160102]\n",
      "Loss: 0.15323626557852638\n",
      "Step 252: ||grad|| = 0.2619563760925884\n",
      "[1.0007251  0.00160141]\n",
      "Loss: 0.15323616249250513\n",
      "Step 253: ||grad|| = 0.2619561790358574\n",
      "[1.0007251 0.0016018]\n",
      "Loss: 0.15323605940663856\n",
      "Step 254: ||grad|| = 0.2619559819792746\n",
      "[1.0007251 0.0016022]\n",
      "Loss: 0.15323595632092707\n",
      "Step 255: ||grad|| = 0.26195578492284005\n",
      "[1.0007251  0.00160259]\n",
      "Loss: 0.1532358532353709\n",
      "Step 256: ||grad|| = 0.26195558786655493\n",
      "[1.0007251  0.00160298]\n",
      "Loss: 0.15323575014996998\n",
      "Step 257: ||grad|| = 0.2619553908104159\n",
      "[1.00072509 0.00160338]\n",
      "Loss: 0.15323564706472334\n",
      "Step 258: ||grad|| = 0.261955193754427\n",
      "[1.00072509 0.00160377]\n",
      "Loss: 0.1532355439796327\n",
      "Step 259: ||grad|| = 0.2619549966985862\n",
      "[1.00072509 0.00160417]\n",
      "Loss: 0.15323544089469743\n",
      "Step 260: ||grad|| = 0.2619547996428934\n",
      "[1.00072509 0.00160456]\n",
      "Loss: 0.1532353378099167\n",
      "Step 261: ||grad|| = 0.2619546025873489\n",
      "[1.00072509 0.00160495]\n",
      "Loss: 0.15323523472529085\n",
      "Step 262: ||grad|| = 0.26195440553195154\n",
      "[1.00072509 0.00160535]\n",
      "Loss: 0.15323513164082067\n",
      "Step 263: ||grad|| = 0.2619542084767041\n",
      "[1.00072509 0.00160574]\n",
      "Loss: 0.1532350285565053\n",
      "Step 264: ||grad|| = 0.2619540114216038\n",
      "[1.00072509 0.00160613]\n",
      "Loss: 0.1532349254723446\n",
      "Step 265: ||grad|| = 0.26195381436665177\n",
      "[1.00072509 0.00160653]\n",
      "Loss: 0.15323482238833924\n",
      "Step 266: ||grad|| = 0.2619536173118478\n",
      "[1.00072509 0.00160692]\n",
      "Loss: 0.15323471930448973\n",
      "Step 267: ||grad|| = 0.26195342025719226\n",
      "[1.00072509 0.00160731]\n",
      "Loss: 0.15323461622079476\n",
      "Step 268: ||grad|| = 0.26195322320268594\n",
      "[1.00072509 0.00160771]\n",
      "Loss: 0.1532345131372539\n",
      "Step 269: ||grad|| = 0.26195302614832644\n",
      "[1.00072509 0.0016081 ]\n",
      "Loss: 0.1532344100538695\n",
      "Step 270: ||grad|| = 0.2619528290941155\n",
      "[1.00072509 0.00160849]\n",
      "Loss: 0.15323430697064014\n",
      "Step 271: ||grad|| = 0.2619526320400538\n",
      "[1.00072509 0.00160889]\n",
      "Loss: 0.15323420388756528\n",
      "Step 272: ||grad|| = 0.2619524349861397\n",
      "[1.00072509 0.00160928]\n",
      "Loss: 0.15323410080464522\n",
      "Step 273: ||grad|| = 0.26195223793237277\n",
      "[1.00072509 0.00160967]\n",
      "Loss: 0.15323399772188076\n",
      "Step 274: ||grad|| = 0.2619520408787556\n",
      "[1.00072508 0.00161007]\n",
      "Loss: 0.1532338946392714\n",
      "Step 275: ||grad|| = 0.2619518438252858\n",
      "[1.00072508 0.00161046]\n",
      "Loss: 0.15323379155681766\n",
      "Step 276: ||grad|| = 0.2619516467719647\n",
      "[1.00072508 0.00161086]\n",
      "Loss: 0.15323368847451702\n",
      "Step 277: ||grad|| = 0.26195144971879125\n",
      "[1.00072508 0.00161125]\n",
      "Loss: 0.15323358539237344\n",
      "Step 278: ||grad|| = 0.2619512526657667\n",
      "[1.00072508 0.00161164]\n",
      "Loss: 0.15323348231038444\n",
      "Step 279: ||grad|| = 0.26195105561289045\n",
      "[1.00072508 0.00161204]\n",
      "Loss: 0.15323337922855074\n",
      "Step 280: ||grad|| = 0.2619508585601622\n",
      "[1.00072508 0.00161243]\n",
      "Loss: 0.1532332761468721\n",
      "Step 281: ||grad|| = 0.2619506615075822\n",
      "[1.00072508 0.00161282]\n",
      "Loss: 0.15323317306534823\n",
      "Step 282: ||grad|| = 0.2619504644551501\n",
      "[1.00072508 0.00161322]\n",
      "Loss: 0.15323306998397915\n",
      "Step 283: ||grad|| = 0.2619502674028659\n",
      "[1.00072508 0.00161361]\n",
      "Loss: 0.15323296690276578\n",
      "Step 284: ||grad|| = 0.2619500703507315\n",
      "[1.00072508 0.001614  ]\n",
      "Loss: 0.1532328638217073\n",
      "Step 285: ||grad|| = 0.26194987329874425\n",
      "[1.00072508 0.0016144 ]\n",
      "Loss: 0.15323276074080458\n",
      "Step 286: ||grad|| = 0.2619496762469054\n",
      "[1.00072508 0.00161479]\n",
      "Loss: 0.15323265766005506\n",
      "Step 287: ||grad|| = 0.2619494791952134\n",
      "[1.00072508 0.00161518]\n",
      "Loss: 0.15323255457946194\n",
      "Step 288: ||grad|| = 0.2619492821436711\n",
      "[1.00072508 0.00161558]\n",
      "Loss: 0.15323245149902417\n",
      "Step 289: ||grad|| = 0.26194908509227743\n",
      "[1.00072508 0.00161597]\n",
      "Loss: 0.15323234841874075\n",
      "Step 290: ||grad|| = 0.26194888804103084\n",
      "[1.00072507 0.00161636]\n",
      "Loss: 0.1532322453386129\n",
      "Step 291: ||grad|| = 0.26194869098993323\n",
      "[1.00072507 0.00161676]\n",
      "Loss: 0.15323214225863965\n",
      "Step 292: ||grad|| = 0.2619484939389844\n",
      "[1.00072507 0.00161715]\n",
      "Loss: 0.1532320391788212\n",
      "Step 293: ||grad|| = 0.2619482968881824\n",
      "[1.00072507 0.00161754]\n",
      "Loss: 0.15323193609915892\n",
      "Step 294: ||grad|| = 0.2619480998375292\n",
      "[1.00072507 0.00161794]\n",
      "Loss: 0.15323183301965157\n",
      "Step 295: ||grad|| = 0.2619479027870253\n",
      "[1.00072507 0.00161833]\n",
      "Loss: 0.1532317299402992\n",
      "Step 296: ||grad|| = 0.2619477057366673\n",
      "[1.00072507 0.00161873]\n",
      "Loss: 0.1532316268611007\n",
      "Step 297: ||grad|| = 0.2619475086864593\n",
      "[1.00072507 0.00161912]\n",
      "Loss: 0.1532315237820585\n",
      "Step 298: ||grad|| = 0.26194731163639995\n",
      "[1.00072507 0.00161951]\n",
      "Loss: 0.1532314207031714\n",
      "Step 299: ||grad|| = 0.2619471145864875\n",
      "[1.00072507 0.00161991]\n",
      "Loss: 0.15323131762443928\n",
      "Step 300: ||grad|| = 0.2619469175367243\n",
      "[1.00072507 0.0016203 ]\n",
      "Loss: 0.15323121454586236\n",
      "Step 301: ||grad|| = 0.26194672048710926\n",
      "[1.00072507 0.00162069]\n",
      "Loss: 0.1532311114674401\n",
      "Step 302: ||grad|| = 0.26194652343764124\n",
      "[1.00072507 0.00162109]\n",
      "Loss: 0.15323100838917209\n",
      "Step 303: ||grad|| = 0.2619463263883225\n",
      "[1.00072507 0.00162148]\n",
      "Loss: 0.15323090531106026\n",
      "Step 304: ||grad|| = 0.2619461293391517\n",
      "[1.00072507 0.00162187]\n",
      "Loss: 0.15323080223310348\n",
      "Step 305: ||grad|| = 0.2619459322901293\n",
      "[1.00072507 0.00162227]\n",
      "Loss: 0.15323069915530244\n",
      "Step 306: ||grad|| = 0.26194573524125425\n",
      "[1.00072507 0.00162266]\n",
      "Loss: 0.1532305960776542\n",
      "Step 307: ||grad|| = 0.2619455381925284\n",
      "[1.00072506 0.00162305]\n",
      "Loss: 0.15323049300016323\n",
      "Step 308: ||grad|| = 0.2619453411439502\n",
      "[1.00072506 0.00162345]\n",
      "Loss: 0.1532303899228269\n",
      "Step 309: ||grad|| = 0.2619451440955211\n",
      "[1.00072506 0.00162384]\n",
      "Loss: 0.1532302868456456\n",
      "Step 310: ||grad|| = 0.2619449470472398\n",
      "[1.00072506 0.00162423]\n",
      "Loss: 0.15323018376861938\n",
      "Step 311: ||grad|| = 0.2619447499991067\n",
      "[1.00072506 0.00162463]\n",
      "Loss: 0.15323008069174712\n",
      "Step 312: ||grad|| = 0.26194455295112146\n",
      "[1.00072506 0.00162502]\n",
      "Loss: 0.15322997761503063\n",
      "Step 313: ||grad|| = 0.26194435590328535\n",
      "[1.00072506 0.00162542]\n",
      "Loss: 0.15322987453847145\n",
      "Step 314: ||grad|| = 0.26194415885559613\n",
      "[1.00072506 0.00162581]\n",
      "Loss: 0.1532297714620644\n",
      "Step 315: ||grad|| = 0.2619439618080561\n",
      "[1.00072506 0.0016262 ]\n",
      "Loss: 0.1532296683858134\n",
      "Step 316: ||grad|| = 0.2619437647606642\n",
      "[1.00072506 0.0016266 ]\n",
      "Loss: 0.1532295653097178\n",
      "Step 317: ||grad|| = 0.2619435677134203\n",
      "[1.00072506 0.00162699]\n",
      "Loss: 0.15322946223377626\n",
      "Step 318: ||grad|| = 0.26194337066632467\n",
      "[1.00072506 0.00162738]\n",
      "Loss: 0.15322935915799116\n",
      "Step 319: ||grad|| = 0.26194317361937747\n",
      "[1.00072506 0.00162778]\n",
      "Loss: 0.15322925608235954\n",
      "Step 320: ||grad|| = 0.26194297657257726\n",
      "[1.00072506 0.00162817]\n",
      "Loss: 0.15322915300688386\n",
      "Step 321: ||grad|| = 0.2619427795259274\n",
      "[1.00072506 0.00162856]\n",
      "Loss: 0.15322904993156397\n",
      "Step 322: ||grad|| = 0.2619425824794243\n",
      "[1.00072506 0.00162896]\n",
      "Loss: 0.15322894685639907\n",
      "Step 323: ||grad|| = 0.2619423854330701\n",
      "[1.00072506 0.00162935]\n",
      "Loss: 0.15322884378138849\n",
      "Step 324: ||grad|| = 0.2619421883868635\n",
      "[1.00072505 0.00162974]\n",
      "Loss: 0.15322874070653247\n",
      "Step 325: ||grad|| = 0.2619419913408058\n",
      "[1.00072505 0.00163014]\n",
      "Loss: 0.15322863763183228\n",
      "Step 326: ||grad|| = 0.26194179429489706\n",
      "[1.00072505 0.00163053]\n",
      "Loss: 0.15322853455728658\n",
      "Step 327: ||grad|| = 0.26194159724913485\n",
      "[1.00072505 0.00163092]\n",
      "Loss: 0.1532284314828965\n",
      "Step 328: ||grad|| = 0.2619414002035213\n",
      "[1.00072505 0.00163132]\n",
      "Loss: 0.15322832840866132\n",
      "Step 329: ||grad|| = 0.26194120315805663\n",
      "[1.00072505 0.00163171]\n",
      "Loss: 0.15322822533458205\n",
      "Step 330: ||grad|| = 0.26194100611273946\n",
      "[1.00072505 0.0016321 ]\n",
      "Loss: 0.15322812226065613\n",
      "Step 331: ||grad|| = 0.26194080906757083\n",
      "[1.00072505 0.0016325 ]\n",
      "Loss: 0.15322801918688764\n",
      "Step 332: ||grad|| = 0.26194061202255037\n",
      "[1.00072505 0.00163289]\n",
      "Loss: 0.15322791611327125\n",
      "Step 333: ||grad|| = 0.2619404149776783\n",
      "[1.00072505 0.00163329]\n",
      "Loss: 0.15322781303981262\n",
      "Step 334: ||grad|| = 0.2619402179329543\n",
      "[1.00072505 0.00163368]\n",
      "Loss: 0.153227709966509\n",
      "Step 335: ||grad|| = 0.2619400208883789\n",
      "[1.00072505 0.00163407]\n",
      "Loss: 0.1532276068933594\n",
      "Step 336: ||grad|| = 0.2619398238439515\n",
      "[1.00072505 0.00163447]\n",
      "Loss: 0.15322750382036412\n",
      "Step 337: ||grad|| = 0.26193962679967225\n",
      "[1.00072505 0.00163486]\n",
      "Loss: 0.1532274007475262\n",
      "Step 338: ||grad|| = 0.2619394297555416\n",
      "[1.00072505 0.00163525]\n",
      "Loss: 0.1532272976748414\n",
      "Step 339: ||grad|| = 0.26193923271155867\n",
      "[1.00072505 0.00163565]\n",
      "Loss: 0.15322719460231154\n",
      "Step 340: ||grad|| = 0.2619390356677245\n",
      "[1.00072505 0.00163604]\n",
      "Loss: 0.15322709152993724\n",
      "Step 341: ||grad|| = 0.26193883862403744\n",
      "[1.00072504 0.00163643]\n",
      "Loss: 0.1532269884577181\n",
      "Step 342: ||grad|| = 0.2619386415804997\n",
      "[1.00072504 0.00163683]\n",
      "Loss: 0.15322688538565454\n",
      "Step 343: ||grad|| = 0.2619384445371097\n",
      "[1.00072504 0.00163722]\n",
      "Loss: 0.15322678231374606\n",
      "Step 344: ||grad|| = 0.2619382474938682\n",
      "[1.00072504 0.00163761]\n",
      "Loss: 0.15322667924199201\n",
      "Step 345: ||grad|| = 0.26193805045077395\n",
      "[1.00072504 0.00163801]\n",
      "Loss: 0.1532265761703926\n",
      "Step 346: ||grad|| = 0.2619378534078296\n",
      "[1.00072504 0.0016384 ]\n",
      "Loss: 0.15322647309894916\n",
      "Step 347: ||grad|| = 0.2619376563650324\n",
      "[1.00072504 0.00163879]\n",
      "Loss: 0.15322637002766043\n",
      "Step 348: ||grad|| = 0.2619374593223831\n",
      "[1.00072504 0.00163919]\n",
      "Loss: 0.15322626695652775\n",
      "Step 349: ||grad|| = 0.26193726227988234\n",
      "[1.00072504 0.00163958]\n",
      "Loss: 0.15322616388554952\n",
      "Step 350: ||grad|| = 0.2619370652375319\n",
      "[1.00072504 0.00163997]\n",
      "Loss: 0.15322606081472562\n",
      "Step 351: ||grad|| = 0.2619368681953263\n",
      "[1.00072504 0.00164037]\n",
      "Loss: 0.15322595774405665\n",
      "Step 352: ||grad|| = 0.2619366711532703\n",
      "[1.00072504 0.00164076]\n",
      "Loss: 0.15322585467354405\n",
      "Step 353: ||grad|| = 0.2619364741113634\n",
      "[1.00072504 0.00164116]\n",
      "Loss: 0.15322575160318594\n",
      "Step 354: ||grad|| = 0.26193627706960376\n",
      "[1.00072504 0.00164155]\n",
      "Loss: 0.15322564853298176\n",
      "Step 355: ||grad|| = 0.2619360800279927\n",
      "[1.00072504 0.00164194]\n",
      "Loss: 0.15322554546293427\n",
      "Step 356: ||grad|| = 0.2619358829865296\n",
      "[1.00072504 0.00164234]\n",
      "Loss: 0.15322544239304145\n",
      "Step 357: ||grad|| = 0.26193568594521444\n",
      "[1.00072504 0.00164273]\n",
      "Loss: 0.1532253393233016\n",
      "Step 358: ||grad|| = 0.261935488904047\n",
      "[1.00072503 0.00164312]\n",
      "Loss: 0.15322523625372145\n",
      "Step 359: ||grad|| = 0.26193529186302966\n",
      "[1.00072503 0.00164352]\n",
      "Loss: 0.15322513318429246\n",
      "Step 360: ||grad|| = 0.2619350948221597\n",
      "[1.00072503 0.00164391]\n",
      "Loss: 0.15322503011502056\n",
      "Step 361: ||grad|| = 0.26193489778143825\n",
      "[1.00072503 0.0016443 ]\n",
      "Loss: 0.1532249270459013\n",
      "Step 362: ||grad|| = 0.26193470074086406\n",
      "[1.00072503 0.0016447 ]\n",
      "Loss: 0.15322482397693932\n",
      "Step 363: ||grad|| = 0.2619345037004391\n",
      "[1.00072503 0.00164509]\n",
      "Loss: 0.1532247209081325\n",
      "Step 364: ||grad|| = 0.26193430666016215\n",
      "[1.00072503 0.00164548]\n",
      "Loss: 0.15322461783947974\n",
      "Step 365: ||grad|| = 0.26193410962003283\n",
      "[1.00072503 0.00164588]\n",
      "Loss: 0.15322451477098276\n",
      "Step 366: ||grad|| = 0.2619339125800527\n",
      "[1.00072503 0.00164627]\n",
      "Loss: 0.15322441170264073\n",
      "Step 367: ||grad|| = 0.26193371554022044\n",
      "[1.00072503 0.00164666]\n",
      "Loss: 0.1532243086344526\n",
      "Step 368: ||grad|| = 0.2619335185005356\n",
      "[1.00072503 0.00164706]\n",
      "Loss: 0.15322420556642122\n",
      "Step 369: ||grad|| = 0.26193332146100007\n",
      "[1.00072503 0.00164745]\n",
      "Loss: 0.15322410249854368\n",
      "Step 370: ||grad|| = 0.2619331244216118\n",
      "[1.00072503 0.00164784]\n",
      "Loss: 0.1532239994308211\n",
      "Step 371: ||grad|| = 0.26193292738237145\n",
      "[1.00072503 0.00164824]\n",
      "Loss: 0.15322389636325476\n",
      "Step 372: ||grad|| = 0.2619327303432808\n",
      "[1.00072503 0.00164863]\n",
      "Loss: 0.15322379329584318\n",
      "Step 373: ||grad|| = 0.26193253330433813\n",
      "[1.00072503 0.00164902]\n",
      "Loss: 0.15322369022858578\n",
      "Step 374: ||grad|| = 0.2619323362655425\n",
      "[1.00072503 0.00164942]\n",
      "Loss: 0.15322358716148368\n",
      "Step 375: ||grad|| = 0.2619321392268967\n",
      "[1.00072502 0.00164981]\n",
      "Loss: 0.15322348409453768\n",
      "Step 376: ||grad|| = 0.26193194218839755\n",
      "[1.00072502 0.00165021]\n",
      "Loss: 0.15322338102774524\n",
      "Step 377: ||grad|| = 0.26193174515004725\n",
      "[1.00072502 0.0016506 ]\n",
      "Loss: 0.15322327796110918\n",
      "Step 378: ||grad|| = 0.26193154811184505\n",
      "[1.00072502 0.00165099]\n",
      "Loss: 0.1532231748946273\n",
      "Step 379: ||grad|| = 0.26193135107379173\n",
      "[1.00072502 0.00165139]\n",
      "Loss: 0.1532230718283008\n",
      "Step 380: ||grad|| = 0.26193115403588524\n",
      "[1.00072502 0.00165178]\n",
      "Loss: 0.15322296876212918\n",
      "Step 381: ||grad|| = 0.261930956998128\n",
      "[1.00072502 0.00165217]\n",
      "Loss: 0.15322286569611326\n",
      "Step 382: ||grad|| = 0.26193075996051896\n",
      "[1.00072502 0.00165257]\n",
      "Loss: 0.15322276263025175\n",
      "Step 383: ||grad|| = 0.2619305629230583\n",
      "[1.00072502 0.00165296]\n",
      "Loss: 0.15322265956454625\n",
      "Step 384: ||grad|| = 0.26193036588574586\n",
      "[1.00072502 0.00165335]\n",
      "Loss: 0.1532225564989945\n",
      "Step 385: ||grad|| = 0.26193016884858106\n",
      "[1.00072502 0.00165375]\n",
      "Loss: 0.15322245343359853\n",
      "Step 386: ||grad|| = 0.2619299718115649\n",
      "[1.00072502 0.00165414]\n",
      "Loss: 0.15322235036835713\n",
      "Step 387: ||grad|| = 0.2619297747746973\n",
      "[1.00072502 0.00165453]\n",
      "Loss: 0.15322224730327144\n",
      "Step 388: ||grad|| = 0.26192957773797687\n",
      "[1.00072502 0.00165493]\n",
      "Loss: 0.1532221442383398\n",
      "Step 389: ||grad|| = 0.26192938070140503\n",
      "[1.00072502 0.00165532]\n",
      "Loss: 0.15322204117356447\n",
      "Step 390: ||grad|| = 0.26192918366498175\n",
      "[1.00072502 0.00165571]\n",
      "Loss: 0.1532219381089439\n",
      "Step 391: ||grad|| = 0.26192898662870706\n",
      "[1.00072502 0.00165611]\n",
      "Loss: 0.1532218350444779\n",
      "Step 392: ||grad|| = 0.2619287895925795\n",
      "[1.00072501 0.0016565 ]\n",
      "Loss: 0.15322173198016742\n",
      "Step 393: ||grad|| = 0.261928592556602\n",
      "[1.00072501 0.00165689]\n",
      "Loss: 0.1532216289160116\n",
      "Step 394: ||grad|| = 0.26192839552077063\n",
      "[1.00072501 0.00165729]\n",
      "Loss: 0.15322152585201157\n",
      "Step 395: ||grad|| = 0.2619281984850884\n",
      "[1.00072501 0.00165768]\n",
      "Loss: 0.15322142278816436\n",
      "Step 396: ||grad|| = 0.2619280014495544\n",
      "[1.00072501 0.00165808]\n",
      "Loss: 0.15322131972447467\n",
      "Step 397: ||grad|| = 0.2619278044141684\n",
      "[1.00072501 0.00165847]\n",
      "Loss: 0.15322121666093966\n",
      "Step 398: ||grad|| = 0.261927607378931\n",
      "[1.00072501 0.00165886]\n",
      "Loss: 0.15322111359755977\n",
      "Step 399: ||grad|| = 0.2619274103438411\n",
      "[1.00072501 0.00165926]\n",
      "Loss: 0.1532210105343342\n",
      "Step 400: ||grad|| = 0.26192721330890006\n",
      "[1.00072501 0.00165965]\n",
      "Loss: 0.15322090747126418\n",
      "Step 401: ||grad|| = 0.26192701627410736\n",
      "[1.00072501 0.00166004]\n",
      "Loss: 0.15322080440834857\n",
      "Step 402: ||grad|| = 0.26192681923946254\n",
      "[1.00072501 0.00166044]\n",
      "Loss: 0.15322070134558866\n",
      "Step 403: ||grad|| = 0.26192662220496593\n",
      "[1.00072501 0.00166083]\n",
      "Loss: 0.15322059828298426\n",
      "Step 404: ||grad|| = 0.26192642517061765\n",
      "[1.00072501 0.00166122]\n",
      "Loss: 0.15322049522053308\n",
      "Step 405: ||grad|| = 0.2619262281364177\n",
      "[1.00072501 0.00166162]\n",
      "Loss: 0.15322039215823918\n",
      "Step 406: ||grad|| = 0.26192603110236556\n",
      "[1.00072501 0.00166201]\n",
      "Loss: 0.15322028909609914\n",
      "Step 407: ||grad|| = 0.2619258340684623\n",
      "[1.00072501 0.0016624 ]\n",
      "Loss: 0.15322018603411452\n",
      "Step 408: ||grad|| = 0.26192563703470656\n",
      "[1.00072501 0.0016628 ]\n",
      "Loss: 0.15322008297228473\n",
      "Step 409: ||grad|| = 0.26192544000109913\n",
      "[1.000725   0.00166319]\n",
      "Loss: 0.15321997991060957\n",
      "Step 410: ||grad|| = 0.2619252429676408\n",
      "[1.000725   0.00166358]\n",
      "Loss: 0.15321987684908953\n",
      "Step 411: ||grad|| = 0.2619250459343294\n",
      "[1.000725   0.00166398]\n",
      "Loss: 0.15321977378772497\n",
      "Step 412: ||grad|| = 0.2619248489011666\n",
      "[1.000725   0.00166437]\n",
      "Loss: 0.1532196707265164\n",
      "Step 413: ||grad|| = 0.2619246518681528\n",
      "[1.000725   0.00166476]\n",
      "Loss: 0.15321956766546163\n",
      "Step 414: ||grad|| = 0.2619244548352867\n",
      "[1.000725   0.00166516]\n",
      "Loss: 0.15321946460456168\n",
      "Step 415: ||grad|| = 0.26192425780256917\n",
      "[1.000725   0.00166555]\n",
      "Loss: 0.15321936154381804\n",
      "Step 416: ||grad|| = 0.26192406076999925\n",
      "[1.000725   0.00166594]\n",
      "Loss: 0.1532192584832291\n",
      "Step 417: ||grad|| = 0.261923863737578\n",
      "[1.000725   0.00166634]\n",
      "Loss: 0.1532191554227946\n",
      "Step 418: ||grad|| = 0.2619236667053042\n",
      "[1.000725   0.00166673]\n",
      "Loss: 0.15321905236251582\n",
      "Step 419: ||grad|| = 0.2619234696731797\n",
      "[1.000725   0.00166713]\n",
      "Loss: 0.15321894930239124\n",
      "Step 420: ||grad|| = 0.2619232726412028\n",
      "[1.000725   0.00166752]\n",
      "Loss: 0.1532188462424224\n",
      "Step 421: ||grad|| = 0.2619230756093743\n",
      "[1.000725   0.00166791]\n",
      "Loss: 0.15321874318260797\n",
      "Step 422: ||grad|| = 0.261922878577694\n",
      "[1.000725   0.00166831]\n",
      "Loss: 0.15321864012294972\n",
      "Step 423: ||grad|| = 0.26192268154616094\n",
      "[1.000725  0.0016687]\n",
      "Loss: 0.15321853706344427\n",
      "Step 424: ||grad|| = 0.2619224845147773\n",
      "[1.000725   0.00166909]\n",
      "Loss: 0.1532184340040965\n",
      "Step 425: ||grad|| = 0.2619222874835416\n",
      "[1.000725   0.00166949]\n",
      "Loss: 0.15321833094490375\n",
      "Step 426: ||grad|| = 0.2619220904524544\n",
      "[1.00072499 0.00166988]\n",
      "Loss: 0.1532182278858645\n",
      "Step 427: ||grad|| = 0.26192189342151573\n",
      "[1.00072499 0.00167027]\n",
      "Loss: 0.15321812482698008\n",
      "Step 428: ||grad|| = 0.26192169639072366\n",
      "[1.00072499 0.00167067]\n",
      "Loss: 0.15321802176825114\n",
      "Step 429: ||grad|| = 0.2619214993600811\n",
      "[1.00072499 0.00167106]\n",
      "Loss: 0.15321791870967752\n",
      "Step 430: ||grad|| = 0.26192130232958605\n",
      "[1.00072499 0.00167145]\n",
      "Loss: 0.15321781565125994\n",
      "Step 431: ||grad|| = 0.26192110529924034\n",
      "[1.00072499 0.00167185]\n",
      "Loss: 0.15321771259299552\n",
      "Step 432: ||grad|| = 0.2619209082690416\n",
      "[1.00072499 0.00167224]\n",
      "Loss: 0.1532176095348879\n",
      "Step 433: ||grad|| = 0.26192071123899235\n",
      "[1.00072499 0.00167263]\n",
      "Loss: 0.15321750647693494\n",
      "Step 434: ||grad|| = 0.2619205142090903\n",
      "[1.00072499 0.00167303]\n",
      "Loss: 0.1532174034191362\n",
      "Step 435: ||grad|| = 0.26192031717933656\n",
      "[1.00072499 0.00167342]\n",
      "Loss: 0.15321730036149192\n",
      "Step 436: ||grad|| = 0.2619201201497313\n",
      "[1.00072499 0.00167381]\n",
      "Loss: 0.1532171973040045\n",
      "Step 437: ||grad|| = 0.26191992312027385\n",
      "[1.00072499 0.00167421]\n",
      "Loss: 0.15321709424667032\n",
      "Step 438: ||grad|| = 0.26191972609096476\n",
      "[1.00072499 0.0016746 ]\n",
      "Loss: 0.15321699118949278\n",
      "Step 439: ||grad|| = 0.26191952906180443\n",
      "[1.00072499 0.00167499]\n",
      "Loss: 0.1532168881324698\n",
      "Step 440: ||grad|| = 0.26191933203279205\n",
      "[1.00072499 0.00167539]\n",
      "Loss: 0.15321678507560116\n",
      "Step 441: ||grad|| = 0.26191913500392777\n",
      "[1.00072499 0.00167578]\n",
      "Loss: 0.15321668201888872\n",
      "Step 442: ||grad|| = 0.2619189379752115\n",
      "[1.00072499 0.00167617]\n",
      "Loss: 0.15321657896233115\n",
      "Step 443: ||grad|| = 0.2619187409466438\n",
      "[1.00072498 0.00167657]\n",
      "Loss: 0.1532164759059278\n",
      "Step 444: ||grad|| = 0.2619185439182235\n",
      "[1.00072498 0.00167696]\n",
      "Loss: 0.1532163728496798\n",
      "Step 445: ||grad|| = 0.26191834688995297\n",
      "[1.00072498 0.00167736]\n",
      "Loss: 0.15321626979358718\n",
      "Step 446: ||grad|| = 0.2619181498618293\n",
      "[1.00072498 0.00167775]\n",
      "Loss: 0.15321616673764907\n",
      "Step 447: ||grad|| = 0.2619179528338546\n",
      "[1.00072498 0.00167814]\n",
      "Loss: 0.15321606368186666\n",
      "Step 448: ||grad|| = 0.261917755806028\n",
      "[1.00072498 0.00167854]\n",
      "Loss: 0.15321596062624096\n",
      "Step 449: ||grad|| = 0.2619175587783527\n",
      "[1.00072498 0.00167893]\n",
      "Loss: 0.1532158575707666\n",
      "Step 450: ||grad|| = 0.2619173617508188\n",
      "[1.00072498 0.00167932]\n",
      "Loss: 0.15321575451544908\n",
      "Step 451: ||grad|| = 0.26191716472343646\n",
      "[1.00072498 0.00167972]\n",
      "Loss: 0.15321565146028646\n",
      "Step 452: ||grad|| = 0.2619169676962029\n",
      "[1.00072498 0.00168011]\n",
      "Loss: 0.15321554840527932\n",
      "Step 453: ||grad|| = 0.2619167706691171\n",
      "[1.00072498 0.0016805 ]\n",
      "Loss: 0.1532154453504271\n",
      "Step 454: ||grad|| = 0.2619165736421793\n",
      "[1.00072498 0.0016809 ]\n",
      "Loss: 0.153215342295729\n",
      "Step 455: ||grad|| = 0.261916376615391\n",
      "[1.00072498 0.00168129]\n",
      "Loss: 0.1532152392411869\n",
      "Step 456: ||grad|| = 0.26191617958874924\n",
      "[1.00072498 0.00168168]\n",
      "Loss: 0.15321513618679836\n",
      "Step 457: ||grad|| = 0.2619159825622564\n",
      "[1.00072498 0.00168208]\n",
      "Loss: 0.1532150331325673\n",
      "Step 458: ||grad|| = 0.26191578553591177\n",
      "[1.00072498 0.00168247]\n",
      "Loss: 0.15321493007849024\n",
      "Step 459: ||grad|| = 0.2619155885097152\n",
      "[1.00072498 0.00168286]\n",
      "Loss: 0.1532148270245686\n",
      "Step 460: ||grad|| = 0.2619153914836681\n",
      "[1.00072497 0.00168326]\n",
      "Loss: 0.15321472397080108\n",
      "Step 461: ||grad|| = 0.26191519445776795\n",
      "[1.00072497 0.00168365]\n",
      "Loss: 0.15321462091718827\n",
      "Step 462: ||grad|| = 0.2619149974320153\n",
      "[1.00072497 0.00168404]\n",
      "Loss: 0.15321451786373125\n",
      "Step 463: ||grad|| = 0.26191480040641163\n",
      "[1.00072497 0.00168444]\n",
      "Loss: 0.15321441481042905\n",
      "Step 464: ||grad|| = 0.26191460338095657\n",
      "[1.00072497 0.00168483]\n",
      "Loss: 0.15321431175728292\n",
      "Step 465: ||grad|| = 0.26191440635564944\n",
      "[1.00072497 0.00168522]\n",
      "Loss: 0.15321420870429034\n",
      "Step 466: ||grad|| = 0.26191420933049064\n",
      "[1.00072497 0.00168562]\n",
      "Loss: 0.15321410565145607\n",
      "Step 467: ||grad|| = 0.2619140123054843\n",
      "[1.00072497 0.00168601]\n",
      "Loss: 0.1532140025987723\n",
      "Step 468: ||grad|| = 0.26191381528061725\n",
      "[1.00072497 0.0016864 ]\n",
      "Loss: 0.15321389954624576\n",
      "Step 469: ||grad|| = 0.2619136182559028\n",
      "[1.00072497 0.0016868 ]\n",
      "Loss: 0.15321379649387376\n",
      "Step 470: ||grad|| = 0.261913421231337\n",
      "[1.00072497 0.00168719]\n",
      "Loss: 0.15321369344165678\n",
      "Step 471: ||grad|| = 0.2619132242069192\n",
      "[1.00072497 0.00168759]\n",
      "Loss: 0.15321359038959564\n",
      "Step 472: ||grad|| = 0.26191302718264947\n",
      "[1.00072497 0.00168798]\n",
      "Loss: 0.15321348733768778\n",
      "Step 473: ||grad|| = 0.2619128301585275\n",
      "[1.00072497 0.00168837]\n",
      "Loss: 0.1532133842859376\n",
      "Step 474: ||grad|| = 0.2619126331345548\n",
      "[1.00072497 0.00168877]\n",
      "Loss: 0.15321328123434055\n",
      "Step 475: ||grad|| = 0.26191243611072995\n",
      "[1.00072497 0.00168916]\n",
      "Loss: 0.15321317818289926\n",
      "Step 476: ||grad|| = 0.2619122390870521\n",
      "[1.00072497 0.00168955]\n",
      "Loss: 0.1532130751316132\n",
      "Step 477: ||grad|| = 0.26191204206352403\n",
      "[1.00072496 0.00168995]\n",
      "Loss: 0.15321297208048065\n",
      "Step 478: ||grad|| = 0.2619118450401442\n",
      "[1.00072496 0.00169034]\n",
      "Loss: 0.15321286902950446\n",
      "Step 479: ||grad|| = 0.26191164801691125\n",
      "[1.00072496 0.00169073]\n",
      "Loss: 0.15321276597868388\n",
      "Step 480: ||grad|| = 0.2619114509938278\n",
      "[1.00072496 0.00169113]\n",
      "Loss: 0.15321266292801827\n",
      "Step 481: ||grad|| = 0.26191125397089254\n",
      "[1.00072496 0.00169152]\n",
      "Loss: 0.15321255987750637\n",
      "Step 482: ||grad|| = 0.2619110569481045\n",
      "[1.00072496 0.00169191]\n",
      "Loss: 0.15321245682715046\n",
      "Step 483: ||grad|| = 0.2619108599254661\n",
      "[1.00072496 0.00169231]\n",
      "Loss: 0.1532123537769484\n",
      "Step 484: ||grad|| = 0.26191066290297405\n",
      "[1.00072496 0.0016927 ]\n",
      "Loss: 0.15321225072690345\n",
      "Step 485: ||grad|| = 0.2619104658806312\n",
      "[1.00072496 0.00169309]\n",
      "Loss: 0.15321214767701194\n",
      "Step 486: ||grad|| = 0.26191026885843627\n",
      "[1.00072496 0.00169349]\n",
      "Loss: 0.15321204462727542\n",
      "Step 487: ||grad|| = 0.26191007183638976\n",
      "[1.00072496 0.00169388]\n",
      "Loss: 0.15321194157769538\n",
      "Step 488: ||grad|| = 0.2619098748144927\n",
      "[1.00072496 0.00169427]\n",
      "Loss: 0.15321183852826883\n",
      "Step 489: ||grad|| = 0.2619096777927418\n",
      "[1.00072496 0.00169467]\n",
      "Loss: 0.15321173547899702\n",
      "Step 490: ||grad|| = 0.26190948077114\n",
      "[1.00072496 0.00169506]\n",
      "Loss: 0.15321163242988162\n",
      "Step 491: ||grad|| = 0.261909283749686\n",
      "[1.00072496 0.00169545]\n",
      "Loss: 0.1532115293809212\n",
      "Step 492: ||grad|| = 0.2619090867283799\n",
      "[1.00072496 0.00169585]\n",
      "Loss: 0.15321142633211535\n",
      "Step 493: ||grad|| = 0.26190888970722304\n",
      "[1.00072496 0.00169624]\n",
      "Loss: 0.15321132328346415\n",
      "Step 494: ||grad|| = 0.2619086926862143\n",
      "[1.00072495 0.00169663]\n",
      "Loss: 0.15321122023496855\n",
      "Step 495: ||grad|| = 0.2619084956653524\n",
      "[1.00072495 0.00169703]\n",
      "Loss: 0.15321111718662736\n",
      "Step 496: ||grad|| = 0.2619082986446412\n",
      "[1.00072495 0.00169742]\n",
      "Loss: 0.1532110141384422\n",
      "Step 497: ||grad|| = 0.26190810162407546\n",
      "[1.00072495 0.00169782]\n",
      "Loss: 0.15321091109041202\n",
      "Step 498: ||grad|| = 0.2619079046036606\n",
      "[1.00072495 0.00169821]\n",
      "Loss: 0.15321080804253615\n",
      "Step 499: ||grad|| = 0.26190770758339293\n",
      "[1.00072495 0.0016986 ]\n",
      "Loss: 0.15321070499481557\n",
      "Step 500: ||grad|| = 0.2619075105632725\n",
      "[1.00072495 0.001699  ]\n",
      "Loss: 0.1532106019472506\n",
      "Step 501: ||grad|| = 0.26190731354330066\n",
      "[1.00072495 0.00169939]\n",
      "Loss: 0.15321049889983915\n",
      "Step 502: ||grad|| = 0.2619071165234776\n",
      "[1.00072495 0.00169978]\n",
      "Loss: 0.1532103958525845\n",
      "Step 503: ||grad|| = 0.2619069195038029\n",
      "[1.00072495 0.00170018]\n",
      "Loss: 0.15321029280548418\n",
      "Step 504: ||grad|| = 0.2619067224842754\n",
      "[1.00072495 0.00170057]\n",
      "Loss: 0.15321018975853853\n",
      "Step 505: ||grad|| = 0.26190652546489746\n",
      "[1.00072495 0.00170096]\n",
      "Loss: 0.1532100867117485\n",
      "Step 506: ||grad|| = 0.2619063284456668\n",
      "[1.00072495 0.00170136]\n",
      "Loss: 0.15320998366511349\n",
      "Step 507: ||grad|| = 0.2619061314265836\n",
      "[1.00072495 0.00170175]\n",
      "Loss: 0.15320988061863228\n",
      "Step 508: ||grad|| = 0.26190593440764953\n",
      "[1.00072495 0.00170214]\n",
      "Loss: 0.15320977757230647\n",
      "Step 509: ||grad|| = 0.26190573738886325\n",
      "[1.00072495 0.00170254]\n",
      "Loss: 0.15320967452613637\n",
      "Step 510: ||grad|| = 0.26190554037022595\n",
      "[1.00072495 0.00170293]\n",
      "Loss: 0.15320957148012224\n",
      "Step 511: ||grad|| = 0.261905343351737\n",
      "[1.00072494 0.00170332]\n",
      "Loss: 0.15320946843426245\n",
      "Step 512: ||grad|| = 0.26190514633339496\n",
      "[1.00072494 0.00170372]\n",
      "Loss: 0.15320936538855745\n",
      "Step 513: ||grad|| = 0.26190494931520225\n",
      "[1.00072494 0.00170411]\n",
      "Loss: 0.15320926234300683\n",
      "Step 514: ||grad|| = 0.2619047522971563\n",
      "[1.00072494 0.0017045 ]\n",
      "Loss: 0.15320915929761228\n",
      "Step 515: ||grad|| = 0.2619045552792609\n",
      "[1.00072494 0.0017049 ]\n",
      "Loss: 0.15320905625237202\n",
      "Step 516: ||grad|| = 0.26190435826151215\n",
      "[1.00072494 0.00170529]\n",
      "Loss: 0.1532089532072862\n",
      "Step 517: ||grad|| = 0.2619041612439117\n",
      "[1.00072494 0.00170568]\n",
      "Loss: 0.15320885016235672\n",
      "Step 518: ||grad|| = 0.26190396422645973\n",
      "[1.00072494 0.00170608]\n",
      "Loss: 0.15320874711758142\n",
      "Step 519: ||grad|| = 0.2619037672091563\n",
      "[1.00072494 0.00170647]\n",
      "Loss: 0.15320864407296172\n",
      "Step 520: ||grad|| = 0.2619035701920009\n",
      "[1.00072494 0.00170686]\n",
      "Loss: 0.1532085410284967\n",
      "Step 521: ||grad|| = 0.2619033731749934\n",
      "[1.00072494 0.00170726]\n",
      "Loss: 0.15320843798418707\n",
      "Step 522: ||grad|| = 0.261903176158133\n",
      "[1.00072494 0.00170765]\n",
      "Loss: 0.15320833494003228\n",
      "Step 523: ||grad|| = 0.261902979141423\n",
      "[1.00072494 0.00170804]\n",
      "Loss: 0.15320823189603197\n",
      "Step 524: ||grad|| = 0.2619027821248599\n",
      "[1.00072494 0.00170844]\n",
      "Loss: 0.15320812885218793\n",
      "Step 525: ||grad|| = 0.2619025851084458\n",
      "[1.00072494 0.00170883]\n",
      "Loss: 0.15320802580849774\n",
      "Step 526: ||grad|| = 0.26190238809217936\n",
      "[1.00072494 0.00170922]\n",
      "Loss: 0.15320792276496303\n",
      "Step 527: ||grad|| = 0.2619021910760612\n",
      "[1.00072494 0.00170962]\n",
      "Loss: 0.15320781972158323\n",
      "Step 528: ||grad|| = 0.26190199406009057\n",
      "[1.00072493 0.00171001]\n",
      "Loss: 0.15320771667835806\n",
      "Step 529: ||grad|| = 0.26190179704426864\n",
      "[1.00072493 0.00171041]\n",
      "Loss: 0.15320761363528917\n",
      "Step 530: ||grad|| = 0.26190160002859547\n",
      "[1.00072493 0.0017108 ]\n",
      "Loss: 0.15320751059237495\n",
      "Step 531: ||grad|| = 0.26190140301306997\n",
      "[1.00072493 0.00171119]\n",
      "Loss: 0.15320740754961557\n",
      "Step 532: ||grad|| = 0.2619012059976927\n",
      "[1.00072493 0.00171159]\n",
      "Loss: 0.15320730450701045\n",
      "Step 533: ||grad|| = 0.26190100898246377\n",
      "[1.00072493 0.00171198]\n",
      "Loss: 0.15320720146456104\n",
      "Step 534: ||grad|| = 0.261900811967383\n",
      "[1.00072493 0.00171237]\n",
      "Loss: 0.15320709842226699\n",
      "Step 535: ||grad|| = 0.2619006149524502\n",
      "[1.00072493 0.00171277]\n",
      "Loss: 0.15320699538012827\n",
      "Step 536: ||grad|| = 0.2619004179376661\n",
      "[1.00072493 0.00171316]\n",
      "Loss: 0.1532068923381433\n",
      "Step 537: ||grad|| = 0.26190022092303017\n",
      "[1.00072493 0.00171355]\n",
      "Loss: 0.153206789296314\n",
      "Step 538: ||grad|| = 0.2619000239085424\n",
      "[1.00072493 0.00171395]\n",
      "Loss: 0.15320668625463998\n",
      "Step 539: ||grad|| = 0.26189982689420277\n",
      "[1.00072493 0.00171434]\n",
      "Loss: 0.15320658321312014\n",
      "Step 540: ||grad|| = 0.26189962988001064\n",
      "[1.00072493 0.00171473]\n",
      "Loss: 0.15320648017175587\n",
      "Step 541: ||grad|| = 0.26189943286596734\n",
      "[1.00072493 0.00171513]\n",
      "Loss: 0.15320637713054638\n",
      "Step 542: ||grad|| = 0.26189923585207253\n",
      "[1.00072493 0.00171552]\n",
      "Loss: 0.15320627408949258\n",
      "Step 543: ||grad|| = 0.2618990388383261\n",
      "[1.00072493 0.00171591]\n",
      "Loss: 0.15320617104859308\n",
      "Step 544: ||grad|| = 0.2618988418247272\n",
      "[1.00072492 0.00171631]\n",
      "Loss: 0.1532060680078492\n",
      "Step 545: ||grad|| = 0.26189864481127684\n",
      "[1.00072492 0.0017167 ]\n",
      "Loss: 0.1532059649672597\n",
      "Step 546: ||grad|| = 0.2618984477979744\n",
      "[1.00072492 0.00171709]\n",
      "Loss: 0.15320586192682503\n",
      "Step 547: ||grad|| = 0.26189825078482\n",
      "[1.00072492 0.00171749]\n",
      "Loss: 0.15320575888654706\n",
      "Step 548: ||grad|| = 0.26189805377181596\n",
      "[1.00072492 0.00171788]\n",
      "Loss: 0.1532056558464227\n",
      "Step 549: ||grad|| = 0.2618978567589561\n",
      "[1.00072492 0.00171827]\n",
      "Loss: 0.1532055528064534\n",
      "Step 550: ||grad|| = 0.26189765974624696\n",
      "[1.00072492 0.00171867]\n",
      "Loss: 0.15320544976663836\n",
      "Step 551: ||grad|| = 0.2618974627336862\n",
      "[1.00072492 0.00171906]\n",
      "Loss: 0.15320534672697972\n",
      "Step 552: ||grad|| = 0.26189726572127214\n",
      "[1.00072492 0.00171945]\n",
      "Loss: 0.1532052436874759\n",
      "Step 553: ||grad|| = 0.26189706870900825\n",
      "[1.00072492 0.00171985]\n",
      "Loss: 0.15320514064812712\n",
      "Step 554: ||grad|| = 0.2618968716968909\n",
      "[1.00072492 0.00172024]\n",
      "Loss: 0.1532050376089325\n",
      "Step 555: ||grad|| = 0.2618966746849226\n",
      "[1.00072492 0.00172063]\n",
      "Loss: 0.15320493456989437\n",
      "Step 556: ||grad|| = 0.26189647767310303\n",
      "[1.00072492 0.00172103]\n",
      "Loss: 0.15320483153100953\n",
      "Step 557: ||grad|| = 0.26189628066143017\n",
      "[1.00072492 0.00172142]\n",
      "Loss: 0.1532047284922813\n",
      "Step 558: ||grad|| = 0.26189608364990635\n",
      "[1.00072492 0.00172181]\n",
      "Loss: 0.1532046254537079\n",
      "Step 559: ||grad|| = 0.26189588663853064\n",
      "[1.00072492 0.00172221]\n",
      "Loss: 0.1532045224152876\n",
      "Step 560: ||grad|| = 0.26189568962730336\n",
      "[1.00072492 0.0017226 ]\n",
      "Loss: 0.15320441937702547\n",
      "Step 561: ||grad|| = 0.2618954926162244\n",
      "[1.00072491 0.001723  ]\n",
      "Loss: 0.15320431633891649\n",
      "Step 562: ||grad|| = 0.2618952956052929\n",
      "[1.00072491 0.00172339]\n",
      "Loss: 0.15320421330096207\n",
      "Step 563: ||grad|| = 0.2618950985945106\n",
      "[1.00072491 0.00172378]\n",
      "Loss: 0.15320411026316422\n",
      "Step 564: ||grad|| = 0.2618949015838757\n",
      "[1.00072491 0.00172418]\n",
      "Loss: 0.1532040072255198\n",
      "Step 565: ||grad|| = 0.26189470457338904\n",
      "[1.00072491 0.00172457]\n",
      "Loss: 0.1532039041880303\n",
      "Step 566: ||grad|| = 0.26189450756305077\n",
      "[1.00072491 0.00172496]\n",
      "Loss: 0.15320380115069646\n",
      "Step 567: ||grad|| = 0.2618943105528609\n",
      "[1.00072491 0.00172536]\n",
      "Loss: 0.15320369811351878\n",
      "Step 568: ||grad|| = 0.26189411354281894\n",
      "[1.00072491 0.00172575]\n",
      "Loss: 0.1532035950764954\n",
      "Step 569: ||grad|| = 0.2618939165329257\n",
      "[1.00072491 0.00172614]\n",
      "Loss: 0.15320349203962777\n",
      "Step 570: ||grad|| = 0.2618937195231801\n",
      "[1.00072491 0.00172654]\n",
      "Loss: 0.15320338900291247\n",
      "Step 571: ||grad|| = 0.26189352251358183\n",
      "[1.00072491 0.00172693]\n",
      "Loss: 0.15320328596635335\n",
      "Step 572: ||grad|| = 0.26189332550413297\n",
      "[1.00072491 0.00172732]\n",
      "Loss: 0.15320318292995064\n",
      "Step 573: ||grad|| = 0.26189312849483204\n",
      "[1.00072491 0.00172772]\n",
      "Loss: 0.15320307989370163\n",
      "Step 574: ||grad|| = 0.2618929314856803\n",
      "[1.00072491 0.00172811]\n",
      "Loss: 0.15320297685760761\n",
      "Step 575: ||grad|| = 0.26189273447667494\n",
      "[1.00072491 0.0017285 ]\n",
      "Loss: 0.15320287382167147\n",
      "Step 576: ||grad|| = 0.2618925374678191\n",
      "[1.00072491 0.0017289 ]\n",
      "Loss: 0.1532027707858862\n",
      "Step 577: ||grad|| = 0.2618923404591116\n",
      "[1.00072491 0.00172929]\n",
      "Loss: 0.15320266775025798\n",
      "Step 578: ||grad|| = 0.26189214345055145\n",
      "[1.0007249  0.00172968]\n",
      "Loss: 0.15320256471478388\n",
      "Step 579: ||grad|| = 0.26189194644213953\n",
      "[1.0007249  0.00173008]\n",
      "Loss: 0.15320246167946586\n",
      "Step 580: ||grad|| = 0.2618917494338758\n",
      "[1.0007249  0.00173047]\n",
      "Loss: 0.1532023586443025\n",
      "Step 581: ||grad|| = 0.2618915524257609\n",
      "[1.0007249  0.00173086]\n",
      "Loss: 0.1532022556092938\n",
      "Step 582: ||grad|| = 0.26189135541779324\n",
      "[1.0007249  0.00173126]\n",
      "Loss: 0.1532021525744412\n",
      "Step 583: ||grad|| = 0.2618911584099747\n",
      "[1.0007249  0.00173165]\n",
      "Loss: 0.1532020495397419\n",
      "Step 584: ||grad|| = 0.26189096140230483\n",
      "[1.0007249  0.00173204]\n",
      "Loss: 0.1532019465051987\n",
      "Step 585: ||grad|| = 0.2618907643947817\n",
      "[1.0007249  0.00173244]\n",
      "Loss: 0.15320184347081023\n",
      "Step 586: ||grad|| = 0.2618905673874075\n",
      "[1.0007249  0.00173283]\n",
      "Loss: 0.1532017404365774\n",
      "Step 587: ||grad|| = 0.26189037038018137\n",
      "[1.0007249  0.00173322]\n",
      "Loss: 0.15320163740249854\n",
      "Step 588: ||grad|| = 0.26189017337310405\n",
      "[1.0007249  0.00173362]\n",
      "Loss: 0.1532015343685754\n",
      "Step 589: ||grad|| = 0.26188997636617384\n",
      "[1.0007249  0.00173401]\n",
      "Loss: 0.1532014313348067\n",
      "Step 590: ||grad|| = 0.2618897793593918\n",
      "[1.0007249 0.0017344]\n",
      "Loss: 0.15320132830119368\n",
      "Step 591: ||grad|| = 0.2618895823527584\n",
      "[1.0007249 0.0017348]\n",
      "Loss: 0.15320122526773552\n",
      "Step 592: ||grad|| = 0.26188938534627426\n",
      "[1.0007249  0.00173519]\n",
      "Loss: 0.1532011222344325\n",
      "Step 593: ||grad|| = 0.26188918833993735\n",
      "[1.0007249  0.00173558]\n",
      "Loss: 0.15320101920128384\n",
      "Step 594: ||grad|| = 0.2618889913337484\n",
      "[1.0007249  0.00173598]\n",
      "Loss: 0.15320091616829112\n",
      "Step 595: ||grad|| = 0.2618887943277081\n",
      "[1.00072489 0.00173637]\n",
      "Loss: 0.15320081313545272\n",
      "Step 596: ||grad|| = 0.2618885973218163\n",
      "[1.00072489 0.00173677]\n",
      "Loss: 0.15320071010277014\n",
      "Step 597: ||grad|| = 0.261888400316072\n",
      "[1.00072489 0.00173716]\n",
      "Loss: 0.15320060707024125\n",
      "Step 598: ||grad|| = 0.2618882033104762\n",
      "[1.00072489 0.00173755]\n",
      "Loss: 0.15320050403786786\n",
      "Step 599: ||grad|| = 0.26188800630502806\n",
      "[1.00072489 0.00173795]\n",
      "Loss: 0.15320040100564924\n",
      "Step 600: ||grad|| = 0.26188780929972855\n",
      "[1.00072489 0.00173834]\n",
      "Loss: 0.15320029797358642\n",
      "Step 601: ||grad|| = 0.2618876122945776\n",
      "[1.00072489 0.00173873]\n",
      "Loss: 0.15320019494167786\n",
      "Step 602: ||grad|| = 0.2618874152895743\n",
      "[1.00072489 0.00173913]\n",
      "Loss: 0.15320009190992517\n",
      "Step 603: ||grad|| = 0.26188721828471945\n",
      "[1.00072489 0.00173952]\n",
      "Loss: 0.15319998887832764\n",
      "Step 604: ||grad|| = 0.2618870212800127\n",
      "[1.00072489 0.00173991]\n",
      "Loss: 0.15319988584688343\n",
      "Step 605: ||grad|| = 0.2618868242754541\n",
      "[1.00072489 0.00174031]\n",
      "Loss: 0.15319978281559563\n",
      "Step 606: ||grad|| = 0.26188662727104406\n",
      "[1.00072489 0.0017407 ]\n",
      "Loss: 0.15319967978446272\n",
      "Step 607: ||grad|| = 0.26188643026678143\n",
      "[1.00072489 0.00174109]\n",
      "Loss: 0.15319957675348508\n",
      "Step 608: ||grad|| = 0.2618862332626673\n",
      "[1.00072489 0.00174149]\n",
      "Loss: 0.15319947372266146\n",
      "Step 609: ||grad|| = 0.2618860362587022\n",
      "[1.00072489 0.00174188]\n",
      "Loss: 0.15319937069199308\n",
      "Step 610: ||grad|| = 0.2618858392548843\n",
      "[1.00072489 0.00174227]\n",
      "Loss: 0.15319926766148104\n",
      "Step 611: ||grad|| = 0.2618856422512157\n",
      "[1.00072489 0.00174267]\n",
      "Loss: 0.153199164631122\n",
      "Step 612: ||grad|| = 0.26188544524769397\n",
      "[1.00072488 0.00174306]\n",
      "Loss: 0.15319906160091928\n",
      "Step 613: ||grad|| = 0.26188524824432063\n",
      "[1.00072488 0.00174345]\n",
      "Loss: 0.1531989585708714\n",
      "Step 614: ||grad|| = 0.26188505124109646\n",
      "[1.00072488 0.00174385]\n",
      "Loss: 0.1531988555409777\n",
      "Step 615: ||grad|| = 0.26188485423801866\n",
      "[1.00072488 0.00174424]\n",
      "Loss: 0.15319875251123985\n",
      "Step 616: ||grad|| = 0.2618846572350922\n",
      "[1.00072488 0.00174463]\n",
      "Loss: 0.15319864948165662\n",
      "Step 617: ||grad|| = 0.26188446023231166\n",
      "[1.00072488 0.00174503]\n",
      "Loss: 0.15319854645222938\n",
      "Step 618: ||grad|| = 0.2618842632296794\n",
      "[1.00072488 0.00174542]\n",
      "Loss: 0.15319844342295574\n",
      "Step 619: ||grad|| = 0.2618840662271959\n",
      "[1.00072488 0.00174581]\n",
      "Loss: 0.15319834039383853\n",
      "Step 620: ||grad|| = 0.2618838692248599\n",
      "[1.00072488 0.00174621]\n",
      "Loss: 0.15319823736487598\n",
      "Step 621: ||grad|| = 0.26188367222267317\n",
      "[1.00072488 0.0017466 ]\n",
      "Loss: 0.15319813433606777\n",
      "Step 622: ||grad|| = 0.2618834752206343\n",
      "[1.00072488 0.00174699]\n",
      "Loss: 0.1531980313074146\n",
      "Step 623: ||grad|| = 0.26188327821874297\n",
      "[1.00072488 0.00174739]\n",
      "Loss: 0.15319792827891662\n",
      "Step 624: ||grad|| = 0.261883081217001\n",
      "[1.00072488 0.00174778]\n",
      "Loss: 0.15319782525057257\n",
      "Step 625: ||grad|| = 0.2618828842154054\n",
      "[1.00072488 0.00174817]\n",
      "Loss: 0.1531977222223866\n",
      "Step 626: ||grad|| = 0.2618826872139589\n",
      "[1.00072488 0.00174857]\n",
      "Loss: 0.1531976191943525\n",
      "Step 627: ||grad|| = 0.26188249021266047\n",
      "[1.00072488 0.00174896]\n",
      "Loss: 0.1531975161664751\n",
      "Step 628: ||grad|| = 0.2618822932115109\n",
      "[1.00072488 0.00174935]\n",
      "Loss: 0.15319741313875176\n",
      "Step 629: ||grad|| = 0.26188209621050906\n",
      "[1.00072487 0.00174975]\n",
      "Loss: 0.15319731011118484\n",
      "Step 630: ||grad|| = 0.26188189920965554\n",
      "[1.00072487 0.00175014]\n",
      "Loss: 0.153197207083771\n",
      "Step 631: ||grad|| = 0.261881702208949\n",
      "[1.00072487 0.00175053]\n",
      "Loss: 0.15319710405651385\n",
      "Step 632: ||grad|| = 0.2618815052083919\n",
      "[1.00072487 0.00175093]\n",
      "Loss: 0.15319700102941008\n",
      "Step 633: ||grad|| = 0.2618813082079834\n",
      "[1.00072487 0.00175132]\n",
      "Loss: 0.15319689800246208\n",
      "Step 634: ||grad|| = 0.2618811112077228\n",
      "[1.00072487 0.00175172]\n",
      "Loss: 0.15319679497566835\n",
      "Step 635: ||grad|| = 0.2618809142076104\n",
      "[1.00072487 0.00175211]\n",
      "Loss: 0.15319669194903088\n",
      "Step 636: ||grad|| = 0.261880717207646\n",
      "[1.00072487 0.0017525 ]\n",
      "Loss: 0.1531965889225488\n",
      "Step 637: ||grad|| = 0.26188052020783004\n",
      "[1.00072487 0.0017529 ]\n",
      "Loss: 0.15319648589622104\n",
      "Step 638: ||grad|| = 0.2618803232081614\n",
      "[1.00072487 0.00175329]\n",
      "Loss: 0.15319638287004822\n",
      "Step 639: ||grad|| = 0.26188012620864154\n",
      "[1.00072487 0.00175368]\n",
      "Loss: 0.15319627984402995\n",
      "Step 640: ||grad|| = 0.26187992920927033\n",
      "[1.00072487 0.00175408]\n",
      "Loss: 0.15319617681816722\n",
      "Step 641: ||grad|| = 0.2618797322100462\n",
      "[1.00072487 0.00175447]\n",
      "Loss: 0.15319607379245942\n",
      "Step 642: ||grad|| = 0.261879535210971\n",
      "[1.00072487 0.00175486]\n",
      "Loss: 0.1531959707669058\n",
      "Step 643: ||grad|| = 0.26187933821204396\n",
      "[1.00072487 0.00175526]\n",
      "Loss: 0.15319586774150878\n",
      "Step 644: ||grad|| = 0.2618791412132651\n",
      "[1.00072487 0.00175565]\n",
      "Loss: 0.1531957647162656\n",
      "Step 645: ||grad|| = 0.2618789442146347\n",
      "[1.00072487 0.00175604]\n",
      "Loss: 0.15319566169117763\n",
      "Step 646: ||grad|| = 0.26187874721615084\n",
      "[1.00072486 0.00175644]\n",
      "Loss: 0.15319555866624351\n",
      "Step 647: ||grad|| = 0.2618785502178173\n",
      "[1.00072486 0.00175683]\n",
      "Loss: 0.15319545564146675\n",
      "Step 648: ||grad|| = 0.2618783532196314\n",
      "[1.00072486 0.00175722]\n",
      "Loss: 0.15319535261684353\n",
      "Step 649: ||grad|| = 0.261878156221593\n",
      "[1.00072486 0.00175762]\n",
      "Loss: 0.15319524959237651\n",
      "Step 650: ||grad|| = 0.2618779592237043\n",
      "[1.00072486 0.00175801]\n",
      "Loss: 0.15319514656806324\n",
      "Step 651: ||grad|| = 0.26187776222596193\n",
      "[1.00072486 0.0017584 ]\n",
      "Loss: 0.15319504354390495\n",
      "Step 652: ||grad|| = 0.2618775652283682\n",
      "[1.00072486 0.0017588 ]\n",
      "Loss: 0.15319494051990276\n",
      "Step 653: ||grad|| = 0.2618773682309237\n",
      "[1.00072486 0.00175919]\n",
      "Loss: 0.15319483749605511\n",
      "Step 654: ||grad|| = 0.2618771712336264\n",
      "[1.00072486 0.00175958]\n",
      "Loss: 0.1531947344723624\n",
      "Step 655: ||grad|| = 0.26187697423647777\n",
      "[1.00072486 0.00175998]\n",
      "Loss: 0.1531946314488239\n",
      "Step 656: ||grad|| = 0.26187677723947733\n",
      "[1.00072486 0.00176037]\n",
      "Loss: 0.1531945284254407\n",
      "Step 657: ||grad|| = 0.26187658024262495\n",
      "[1.00072486 0.00176076]\n",
      "Loss: 0.15319442540221262\n",
      "Step 658: ||grad|| = 0.2618763832459204\n",
      "[1.00072486 0.00176116]\n",
      "Loss: 0.1531943223791411\n",
      "Step 659: ||grad|| = 0.2618761862493646\n",
      "[1.00072486 0.00176155]\n",
      "Loss: 0.1531942193562228\n",
      "Step 660: ||grad|| = 0.2618759892529565\n",
      "[1.00072486 0.00176194]\n",
      "Loss: 0.15319411633345945\n",
      "Step 661: ||grad|| = 0.2618757922566967\n",
      "[1.00072486 0.00176234]\n",
      "Loss: 0.15319401331085103\n",
      "Step 662: ||grad|| = 0.26187559526058624\n",
      "[1.00072486 0.00176273]\n",
      "Loss: 0.15319391028839963\n",
      "Step 663: ||grad|| = 0.26187539826462136\n",
      "[1.00072485 0.00176312]\n",
      "Loss: 0.1531938072661007\n",
      "Step 664: ||grad|| = 0.2618752012688077\n",
      "[1.00072485 0.00176352]\n",
      "Loss: 0.153193704243958\n",
      "Step 665: ||grad|| = 0.2618750042731401\n",
      "[1.00072485 0.00176391]\n",
      "Loss: 0.15319360122197015\n",
      "Step 666: ||grad|| = 0.2618748072776216\n",
      "[1.00072485 0.0017643 ]\n",
      "Loss: 0.15319349820013806\n",
      "Step 667: ||grad|| = 0.2618746102822503\n",
      "[1.00072485 0.0017647 ]\n",
      "Loss: 0.1531933951784601\n",
      "Step 668: ||grad|| = 0.26187441328703\n",
      "[1.00072485 0.00176509]\n",
      "Loss: 0.15319329215693597\n",
      "Step 669: ||grad|| = 0.26187421629195395\n",
      "[1.00072485 0.00176548]\n",
      "Loss: 0.15319318913556854\n",
      "Step 670: ||grad|| = 0.26187401929702836\n",
      "[1.00072485 0.00176588]\n",
      "Loss: 0.15319308611435595\n",
      "Step 671: ||grad|| = 0.261873822302251\n",
      "[1.00072485 0.00176627]\n",
      "Loss: 0.15319298309329754\n",
      "Step 672: ||grad|| = 0.26187362530762115\n",
      "[1.00072485 0.00176666]\n",
      "Loss: 0.15319288007239498\n",
      "Step 673: ||grad|| = 0.26187342831314\n",
      "[1.00072485 0.00176706]\n",
      "Loss: 0.15319277705164625\n",
      "Step 674: ||grad|| = 0.2618732313188061\n",
      "[1.00072485 0.00176745]\n",
      "Loss: 0.15319267403105385\n",
      "Step 675: ||grad|| = 0.2618730343246212\n",
      "[1.00072485 0.00176784]\n",
      "Loss: 0.15319257101061595\n",
      "Step 676: ||grad|| = 0.26187283733058464\n",
      "[1.00072485 0.00176824]\n",
      "Loss: 0.15319246799033304\n",
      "Step 677: ||grad|| = 0.261872640336696\n",
      "[1.00072485 0.00176863]\n",
      "Loss: 0.15319236497020494\n",
      "Step 678: ||grad|| = 0.26187244334295634\n",
      "[1.00072485 0.00176902]\n",
      "Loss: 0.15319226195023167\n",
      "Step 679: ||grad|| = 0.26187224634936346\n",
      "[1.00072485 0.00176942]\n",
      "Loss: 0.15319215893041424\n",
      "Step 680: ||grad|| = 0.2618720493559187\n",
      "[1.00072484 0.00176981]\n",
      "Loss: 0.15319205591075047\n",
      "Step 681: ||grad|| = 0.26187185236262345\n",
      "[1.00072484 0.0017702 ]\n",
      "Loss: 0.15319195289124338\n",
      "Step 682: ||grad|| = 0.2618716553694753\n",
      "[1.00072484 0.0017706 ]\n",
      "Loss: 0.15319184987189113\n",
      "Step 683: ||grad|| = 0.2618714583764767\n",
      "[1.00072484 0.00177099]\n",
      "Loss: 0.15319174685269213\n",
      "Step 684: ||grad|| = 0.2618712613836248\n",
      "[1.00072484 0.00177139]\n",
      "Loss: 0.1531916438336497\n",
      "Step 685: ||grad|| = 0.2618710643909218\n",
      "[1.00072484 0.00177178]\n",
      "Loss: 0.1531915408147609\n",
      "Step 686: ||grad|| = 0.261870867398367\n",
      "[1.00072484 0.00177217]\n",
      "Loss: 0.1531914377960283\n",
      "Step 687: ||grad|| = 0.26187067040596046\n",
      "[1.00072484 0.00177257]\n",
      "Loss: 0.15319133477745034\n",
      "Step 688: ||grad|| = 0.26187047341370223\n",
      "[1.00072484 0.00177296]\n",
      "Loss: 0.15319123175902774\n",
      "Step 689: ||grad|| = 0.2618702764215919\n",
      "[1.00072484 0.00177335]\n",
      "Loss: 0.15319112874075938\n",
      "Step 690: ||grad|| = 0.2618700794296288\n",
      "[1.00072484 0.00177375]\n",
      "Loss: 0.15319102572264584\n",
      "Step 691: ||grad|| = 0.26186988243781556\n",
      "[1.00072484 0.00177414]\n",
      "Loss: 0.15319092270468854\n",
      "Step 692: ||grad|| = 0.26186968544614897\n",
      "[1.00072484 0.00177453]\n",
      "Loss: 0.15319081968688505\n",
      "Step 693: ||grad|| = 0.2618694884546315\n",
      "[1.00072484 0.00177493]\n",
      "Loss: 0.15319071666923595\n",
      "Step 694: ||grad|| = 0.26186929146326215\n",
      "[1.00072484 0.00177532]\n",
      "Loss: 0.15319061365174372\n",
      "Step 695: ||grad|| = 0.2618690944720407\n",
      "[1.00072484 0.00177571]\n",
      "Loss: 0.15319051063440547\n",
      "Step 696: ||grad|| = 0.26186889748096764\n",
      "[1.00072484 0.00177611]\n",
      "Loss: 0.15319040761722327\n",
      "Step 697: ||grad|| = 0.2618687004900431\n",
      "[1.00072483 0.0017765 ]\n",
      "Loss: 0.1531903046001939\n",
      "Step 698: ||grad|| = 0.26186850349926644\n",
      "[1.00072483 0.00177689]\n",
      "Loss: 0.1531902015833218\n",
      "Step 699: ||grad|| = 0.261868306508638\n",
      "[1.00072483 0.00177729]\n",
      "Loss: 0.15319009856660268\n",
      "Step 700: ||grad|| = 0.26186810951815775\n",
      "[1.00072483 0.00177768]\n",
      "Loss: 0.15318999555003973\n",
      "Step 701: ||grad|| = 0.26186791252782565\n",
      "[1.00072483 0.00177807]\n",
      "Loss: 0.15318989253363213\n",
      "Step 702: ||grad|| = 0.2618677155376415\n",
      "[1.00072483 0.00177847]\n",
      "Loss: 0.15318978951737938\n",
      "Step 703: ||grad|| = 0.261867518547606\n",
      "[1.00072483 0.00177886]\n",
      "Loss: 0.15318968650128154\n",
      "Step 704: ||grad|| = 0.26186732155771963\n",
      "[1.00072483 0.00177925]\n",
      "Loss: 0.1531895834853378\n",
      "Step 705: ||grad|| = 0.2618671245679792\n",
      "[1.00072483 0.00177965]\n",
      "Loss: 0.15318948046954903\n",
      "Step 706: ||grad|| = 0.2618669275783867\n",
      "[1.00072483 0.00178004]\n",
      "Loss: 0.15318937745391653\n",
      "Step 707: ||grad|| = 0.2618667305889443\n",
      "[1.00072483 0.00178043]\n",
      "Loss: 0.15318927443843894\n",
      "Step 708: ||grad|| = 0.26186653359965084\n",
      "[1.00072483 0.00178083]\n",
      "Loss: 0.15318917142311486\n",
      "Step 709: ||grad|| = 0.2618663366105027\n",
      "[1.00072483 0.00178122]\n",
      "Loss: 0.15318906840794663\n",
      "Step 710: ||grad|| = 0.2618661396215042\n",
      "[1.00072483 0.00178161]\n",
      "Loss: 0.15318896539293272\n",
      "Step 711: ||grad|| = 0.2618659426326544\n",
      "[1.00072483 0.00178201]\n",
      "Loss: 0.15318886237807453\n",
      "Step 712: ||grad|| = 0.26186574564395254\n",
      "[1.00072483 0.0017824 ]\n",
      "Loss: 0.1531887593633715\n",
      "Step 713: ||grad|| = 0.2618655486553985\n",
      "[1.00072483 0.00178279]\n",
      "Loss: 0.15318865634882275\n",
      "Step 714: ||grad|| = 0.2618653516669929\n",
      "[1.00072482 0.00178319]\n",
      "Loss: 0.15318855333442974\n",
      "Step 715: ||grad|| = 0.26186515467873545\n",
      "[1.00072482 0.00178358]\n",
      "Loss: 0.15318845032019102\n",
      "Step 716: ||grad|| = 0.26186495769062584\n",
      "[1.00072482 0.00178397]\n",
      "Loss: 0.15318834730610803\n",
      "Step 717: ||grad|| = 0.26186476070266435\n",
      "[1.00072482 0.00178437]\n",
      "Loss: 0.1531882442921799\n",
      "Step 718: ||grad|| = 0.2618645637148528\n",
      "[1.00072482 0.00178476]\n",
      "Loss: 0.15318814127840596\n",
      "Step 719: ||grad|| = 0.2618643667271867\n",
      "[1.00072482 0.00178515]\n",
      "Loss: 0.15318803826478858\n",
      "Step 720: ||grad|| = 0.2618641697396704\n",
      "[1.00072482 0.00178555]\n",
      "Loss: 0.15318793525132413\n",
      "Step 721: ||grad|| = 0.261863972752302\n",
      "[1.00072482 0.00178594]\n",
      "Loss: 0.15318783223801616\n",
      "Step 722: ||grad|| = 0.26186377576508224\n",
      "[1.00072482 0.00178633]\n",
      "Loss: 0.15318772922486254\n",
      "Step 723: ||grad|| = 0.26186357877801064\n",
      "[1.00072482 0.00178673]\n",
      "Loss: 0.15318762621186394\n",
      "Step 724: ||grad|| = 0.2618633817910858\n",
      "[1.00072482 0.00178712]\n",
      "Loss: 0.1531875231990204\n",
      "Step 725: ||grad|| = 0.2618631848043103\n",
      "[1.00072482 0.00178751]\n",
      "Loss: 0.15318742018633252\n",
      "Step 726: ||grad|| = 0.2618629878176831\n",
      "[1.00072482 0.00178791]\n",
      "Loss: 0.15318731717379916\n",
      "Step 727: ||grad|| = 0.26186279083120295\n",
      "[1.00072482 0.0017883 ]\n",
      "Loss: 0.15318721416142045\n",
      "Step 728: ||grad|| = 0.2618625938448719\n",
      "[1.00072482 0.00178869]\n",
      "Loss: 0.15318711114919625\n",
      "Step 729: ||grad|| = 0.26186239685868967\n",
      "[1.00072482 0.00178909]\n",
      "Loss: 0.1531870081371273\n",
      "Step 730: ||grad|| = 0.26186219987265447\n",
      "[1.00072482 0.00178948]\n",
      "Loss: 0.15318690512521385\n",
      "Step 731: ||grad|| = 0.2618620028867679\n",
      "[1.00072481 0.00178987]\n",
      "Loss: 0.1531868021134559\n",
      "Step 732: ||grad|| = 0.26186180590102914\n",
      "[1.00072481 0.00179027]\n",
      "Loss: 0.1531866991018518\n",
      "Step 733: ||grad|| = 0.26186160891543947\n",
      "[1.00072481 0.00179066]\n",
      "Loss: 0.153186596090404\n",
      "Step 734: ||grad|| = 0.26186141192999685\n",
      "[1.00072481 0.00179105]\n",
      "Loss: 0.15318649307911048\n",
      "Step 735: ||grad|| = 0.26186121494470355\n",
      "[1.00072481 0.00179145]\n",
      "Loss: 0.1531863900679715\n",
      "Step 736: ||grad|| = 0.26186101795955746\n",
      "[1.00072481 0.00179184]\n",
      "Loss: 0.15318628705698656\n",
      "Step 737: ||grad|| = 0.2618608209745595\n",
      "[1.00072481 0.00179223]\n",
      "Loss: 0.1531861840461581\n",
      "Step 738: ||grad|| = 0.2618606239897099\n",
      "[1.00072481 0.00179263]\n",
      "Loss: 0.1531860810354847\n",
      "Step 739: ||grad|| = 0.2618604270050092\n",
      "[1.00072481 0.00179302]\n",
      "Loss: 0.15318597802496595\n",
      "Step 740: ||grad|| = 0.26186023002045633\n",
      "[1.00072481 0.00179341]\n",
      "Loss: 0.15318587501460193\n",
      "Step 741: ||grad|| = 0.26186003303605154\n",
      "[1.00072481 0.00179381]\n",
      "Loss: 0.15318577200439348\n",
      "Step 742: ||grad|| = 0.2618598360517954\n",
      "[1.00072481 0.0017942 ]\n",
      "Loss: 0.1531856689943391\n",
      "Step 743: ||grad|| = 0.261859639067687\n",
      "[1.00072481 0.00179459]\n",
      "Loss: 0.15318556598444075\n",
      "Step 744: ||grad|| = 0.2618594420837264\n",
      "[1.00072481 0.00179499]\n",
      "Loss: 0.15318546297469715\n",
      "Step 745: ||grad|| = 0.2618592450999141\n",
      "[1.00072481 0.00179538]\n",
      "Loss: 0.15318535996510815\n",
      "Step 746: ||grad|| = 0.26185904811625094\n",
      "[1.00072481 0.00179578]\n",
      "Loss: 0.15318525695567248\n",
      "Step 747: ||grad|| = 0.2618588511327315\n",
      "[1.00072481 0.00179617]\n",
      "Loss: 0.15318515394639573\n",
      "Step 748: ||grad|| = 0.26185865414936715\n",
      "[1.0007248  0.00179656]\n",
      "Loss: 0.1531850509372706\n",
      "Step 749: ||grad|| = 0.2618584571661484\n",
      "[1.0007248  0.00179696]\n",
      "Loss: 0.15318494792830165\n",
      "Step 750: ||grad|| = 0.261858260183077\n",
      "[1.0007248  0.00179735]\n",
      "Loss: 0.15318484491948783\n",
      "Step 751: ||grad|| = 0.2618580632001545\n",
      "[1.0007248  0.00179774]\n",
      "Loss: 0.1531847419108284\n",
      "Step 752: ||grad|| = 0.2618578662173803\n",
      "[1.0007248  0.00179814]\n",
      "Loss: 0.15318463890232498\n",
      "Step 753: ||grad|| = 0.26185766923475334\n",
      "[1.0007248  0.00179853]\n",
      "Loss: 0.15318453589398132\n",
      "Step 754: ||grad|| = 0.2618574722522847\n",
      "[1.0007248  0.00179892]\n",
      "Loss: 0.15318443288578215\n",
      "Step 755: ||grad|| = 0.26185727526994323\n",
      "[1.0007248  0.00179932]\n",
      "Loss: 0.15318432987774344\n",
      "Step 756: ||grad|| = 0.2618570782877621\n",
      "[1.0007248  0.00179971]\n",
      "Loss: 0.15318422686985905\n",
      "Step 757: ||grad|| = 0.26185688130572815\n",
      "[1.0007248 0.0018001]\n",
      "Loss: 0.153184123862129\n",
      "Step 758: ||grad|| = 0.2618566843238431\n",
      "[1.0007248 0.0018005]\n",
      "Loss: 0.15318402085455543\n",
      "Step 759: ||grad|| = 0.26185648734210515\n",
      "[1.0007248  0.00180089]\n",
      "Loss: 0.1531839178471359\n",
      "Step 760: ||grad|| = 0.2618562903605149\n",
      "[1.0007248  0.00180128]\n",
      "Loss: 0.15318381483987117\n",
      "Step 761: ||grad|| = 0.26185609337907445\n",
      "[1.0007248  0.00180168]\n",
      "Loss: 0.1531837118327613\n",
      "Step 762: ||grad|| = 0.2618558963977821\n",
      "[1.0007248  0.00180207]\n",
      "Loss: 0.15318360882580737\n",
      "Step 763: ||grad|| = 0.26185569941663717\n",
      "[1.0007248  0.00180246]\n",
      "Loss: 0.15318350581900836\n",
      "Step 764: ||grad|| = 0.26185550243564076\n",
      "[1.0007248  0.00180286]\n",
      "Loss: 0.15318340281236417\n",
      "Step 765: ||grad|| = 0.2618553054547913\n",
      "[1.00072479 0.00180325]\n",
      "Loss: 0.15318329980587467\n",
      "Step 766: ||grad|| = 0.26185510847409155\n",
      "[1.00072479 0.00180364]\n",
      "Loss: 0.15318319679953915\n",
      "Step 767: ||grad|| = 0.26185491149353923\n",
      "[1.00072479 0.00180404]\n",
      "Loss: 0.15318309379336023\n",
      "Step 768: ||grad|| = 0.2618547145131359\n",
      "[1.00072479 0.00180443]\n",
      "Loss: 0.15318299078733572\n",
      "Step 769: ||grad|| = 0.26185451753287925\n",
      "[1.00072479 0.00180482]\n",
      "Loss: 0.1531828877814671\n",
      "Step 770: ||grad|| = 0.2618543205527743\n",
      "[1.00072479 0.00180522]\n",
      "Loss: 0.15318278477575265\n",
      "Step 771: ||grad|| = 0.26185412357281257\n",
      "[1.00072479 0.00180561]\n",
      "Loss: 0.1531826817701928\n",
      "Step 772: ||grad|| = 0.2618539265930013\n",
      "[1.00072479 0.001806  ]\n",
      "Loss: 0.1531825787647892\n",
      "Step 773: ||grad|| = 0.2618537296133389\n",
      "[1.00072479 0.0018064 ]\n",
      "Loss: 0.1531824757595392\n",
      "Step 774: ||grad|| = 0.2618535326338239\n",
      "[1.00072479 0.00180679]\n",
      "Loss: 0.1531823727544439\n",
      "Step 775: ||grad|| = 0.26185333565445706\n",
      "[1.00072479 0.00180718]\n",
      "Loss: 0.15318226974950405\n",
      "Step 776: ||grad|| = 0.2618531386752383\n",
      "[1.00072479 0.00180758]\n",
      "Loss: 0.1531821667447195\n",
      "Step 777: ||grad|| = 0.2618529416961681\n",
      "[1.00072479 0.00180797]\n",
      "Loss: 0.1531820637400904\n",
      "Step 778: ||grad|| = 0.26185274471724623\n",
      "[1.00072479 0.00180836]\n",
      "Loss: 0.1531819607356152\n",
      "Step 779: ||grad|| = 0.26185254773847205\n",
      "[1.00072479 0.00180876]\n",
      "Loss: 0.1531818577312954\n",
      "Step 780: ||grad|| = 0.26185235075984675\n",
      "[1.00072479 0.00180915]\n",
      "Loss: 0.15318175472713016\n",
      "Step 781: ||grad|| = 0.26185215378136906\n",
      "[1.00072479 0.00180954]\n",
      "Loss: 0.15318165172312095\n",
      "Step 782: ||grad|| = 0.261851956803039\n",
      "[1.00072478 0.00180994]\n",
      "Loss: 0.15318154871926595\n",
      "Step 783: ||grad|| = 0.26185175982485787\n",
      "[1.00072478 0.00181033]\n",
      "Loss: 0.15318144571556644\n",
      "Step 784: ||grad|| = 0.2618515628468252\n",
      "[1.00072478 0.00181072]\n",
      "Loss: 0.15318134271202014\n",
      "Step 785: ||grad|| = 0.2618513658689405\n",
      "[1.00072478 0.00181112]\n",
      "Loss: 0.1531812397086317\n",
      "Step 786: ||grad|| = 0.261851168891204\n",
      "[1.00072478 0.00181151]\n",
      "Loss: 0.1531811367053963\n",
      "Step 787: ||grad|| = 0.2618509719136141\n",
      "[1.00072478 0.0018119 ]\n",
      "Loss: 0.15318103370231576\n",
      "Step 788: ||grad|| = 0.2618507749361742\n",
      "[1.00072478 0.0018123 ]\n",
      "Loss: 0.15318093069939084\n",
      "Step 789: ||grad|| = 0.261850577958883\n",
      "[1.00072478 0.00181269]\n",
      "Loss: 0.15318082769662034\n",
      "Step 790: ||grad|| = 0.26185038098173796\n",
      "[1.00072478 0.00181308]\n",
      "Loss: 0.1531807246940063\n",
      "Step 791: ||grad|| = 0.2618501840047428\n",
      "[1.00072478 0.00181348]\n",
      "Loss: 0.1531806216915463\n",
      "Step 792: ||grad|| = 0.26184998702789475\n",
      "[1.00072478 0.00181387]\n",
      "Loss: 0.15318051868924074\n",
      "Step 793: ||grad|| = 0.26184979005119574\n",
      "[1.00072478 0.00181426]\n",
      "Loss: 0.15318041568709065\n",
      "Step 794: ||grad|| = 0.2618495930746445\n",
      "[1.00072478 0.00181466]\n",
      "Loss: 0.15318031268509552\n",
      "Step 795: ||grad|| = 0.26184939609824087\n",
      "[1.00072478 0.00181505]\n",
      "Loss: 0.15318020968325455\n",
      "Step 796: ||grad|| = 0.2618491991219866\n",
      "[1.00072478 0.00181544]\n",
      "Loss: 0.15318010668156934\n",
      "Step 797: ||grad|| = 0.2618490021458797\n",
      "[1.00072478 0.00181584]\n",
      "Loss: 0.1531800036800393\n",
      "Step 798: ||grad|| = 0.2618488051699203\n",
      "[1.00072478 0.00181623]\n",
      "Loss: 0.1531799006786635\n",
      "Step 799: ||grad|| = 0.26184860819410943\n",
      "[1.00072477 0.00181662]\n",
      "Loss: 0.15317979767744314\n",
      "Step 800: ||grad|| = 0.2618484112184476\n",
      "[1.00072477 0.00181702]\n",
      "Loss: 0.15317969467637724\n",
      "Step 801: ||grad|| = 0.2618482142429335\n",
      "[1.00072477 0.00181741]\n",
      "Loss: 0.15317959167546827\n",
      "Step 802: ||grad|| = 0.26184801726756834\n",
      "[1.00072477 0.0018178 ]\n",
      "Loss: 0.15317948867471165\n",
      "Step 803: ||grad|| = 0.2618478202923502\n",
      "[1.00072477 0.0018182 ]\n",
      "Loss: 0.1531793856741119\n",
      "Step 804: ||grad|| = 0.2618476233172807\n",
      "[1.00072477 0.00181859]\n",
      "Loss: 0.15317928267366557\n",
      "Step 805: ||grad|| = 0.26184742634235936\n",
      "[1.00072477 0.00181898]\n",
      "Loss: 0.1531791796733762\n",
      "Step 806: ||grad|| = 0.2618472293675864\n",
      "[1.00072477 0.00181938]\n",
      "Loss: 0.15317907667323916\n",
      "Step 807: ||grad|| = 0.26184703239296164\n",
      "[1.00072477 0.00181977]\n",
      "Loss: 0.1531789736732586\n",
      "Step 808: ||grad|| = 0.2618468354184852\n",
      "[1.00072477 0.00182016]\n",
      "Loss: 0.15317887067343347\n",
      "Step 809: ||grad|| = 0.26184663844415546\n",
      "[1.00072477 0.00182056]\n",
      "Loss: 0.1531787676737619\n",
      "Step 810: ||grad|| = 0.26184644146997577\n",
      "[1.00072477 0.00182095]\n",
      "Loss: 0.1531786646742454\n",
      "Step 811: ||grad|| = 0.2618462444959429\n",
      "[1.00072477 0.00182134]\n",
      "Loss: 0.1531785616748848\n",
      "Step 812: ||grad|| = 0.26184604752205876\n",
      "[1.00072477 0.00182174]\n",
      "Loss: 0.15317845867567945\n",
      "Step 813: ||grad|| = 0.2618458505483225\n",
      "[1.00072477 0.00182213]\n",
      "Loss: 0.1531783556766287\n",
      "Step 814: ||grad|| = 0.26184565357473494\n",
      "[1.00072477 0.00182252]\n",
      "Loss: 0.1531782526777316\n",
      "Step 815: ||grad|| = 0.2618454566012951\n",
      "[1.00072477 0.00182292]\n",
      "Loss: 0.15317814967899188\n",
      "Step 816: ||grad|| = 0.26184525962800387\n",
      "[1.00072476 0.00182331]\n",
      "Loss: 0.15317804668040588\n",
      "Step 817: ||grad|| = 0.26184506265486035\n",
      "[1.00072476 0.0018237 ]\n",
      "Loss: 0.1531779436819743\n",
      "Step 818: ||grad|| = 0.2618448656818649\n",
      "[1.00072476 0.0018241 ]\n",
      "Loss: 0.15317784068369844\n",
      "Step 819: ||grad|| = 0.2618446687090185\n",
      "[1.00072476 0.00182449]\n",
      "Loss: 0.1531777376855769\n",
      "Step 820: ||grad|| = 0.2618444717363194\n",
      "[1.00072476 0.00182488]\n",
      "Loss: 0.1531776346876119\n",
      "Step 821: ||grad|| = 0.2618442747637687\n",
      "[1.00072476 0.00182528]\n",
      "Loss: 0.15317753168979975\n",
      "Step 822: ||grad|| = 0.26184407779136626\n",
      "[1.00072476 0.00182567]\n",
      "Loss: 0.15317742869214374\n",
      "Step 823: ||grad|| = 0.2618438808191117\n",
      "[1.00072476 0.00182606]\n",
      "Loss: 0.15317732569464226\n",
      "Step 824: ||grad|| = 0.26184368384700635\n",
      "[1.00072476 0.00182646]\n",
      "Loss: 0.15317722269729478\n",
      "Step 825: ||grad|| = 0.26184348687504866\n",
      "[1.00072476 0.00182685]\n",
      "Loss: 0.1531771197001044\n",
      "Step 826: ||grad|| = 0.2618432899032392\n",
      "[1.00072476 0.00182724]\n",
      "Loss: 0.15317701670306724\n",
      "Step 827: ||grad|| = 0.2618430929315768\n",
      "[1.00072476 0.00182764]\n",
      "Loss: 0.15317691370618605\n",
      "Step 828: ||grad|| = 0.2618428959600632\n",
      "[1.00072476 0.00182803]\n",
      "Loss: 0.15317681070945988\n",
      "Step 829: ||grad|| = 0.26184269898869983\n",
      "[1.00072476 0.00182842]\n",
      "Loss: 0.15317670771288858\n",
      "Step 830: ||grad|| = 0.26184250201748177\n",
      "[1.00072476 0.00182882]\n",
      "Loss: 0.1531766047164715\n",
      "Step 831: ||grad|| = 0.26184230504641237\n",
      "[1.00072476 0.00182921]\n",
      "Loss: 0.15317650172020986\n",
      "Step 832: ||grad|| = 0.26184210807549174\n",
      "[1.00072475 0.0018296 ]\n",
      "Loss: 0.1531763987241033\n",
      "Step 833: ||grad|| = 0.2618419111047187\n",
      "[1.00072475 0.00183   ]\n",
      "Loss: 0.15317629572815103\n",
      "Step 834: ||grad|| = 0.26184171413409435\n",
      "[1.00072475 0.00183039]\n",
      "Loss: 0.15317619273235453\n",
      "Step 835: ||grad|| = 0.26184151716361875\n",
      "[1.00072475 0.00183078]\n",
      "Loss: 0.15317608973671304\n",
      "Step 836: ||grad|| = 0.26184132019329087\n",
      "[1.00072475 0.00183118]\n",
      "Loss: 0.15317598674122593\n",
      "Step 837: ||grad|| = 0.2618411232231116\n",
      "[1.00072475 0.00183157]\n",
      "Loss: 0.15317588374589375\n",
      "Step 838: ||grad|| = 0.2618409262530789\n",
      "[1.00072475 0.00183196]\n",
      "Loss: 0.15317578075071742\n",
      "Step 839: ||grad|| = 0.26184072928319524\n",
      "[1.00072475 0.00183236]\n",
      "Loss: 0.15317567775569513\n",
      "Step 840: ||grad|| = 0.2618405323134602\n",
      "[1.00072475 0.00183275]\n",
      "Loss: 0.15317557476082827\n",
      "Step 841: ||grad|| = 0.2618403353438733\n",
      "[1.00072475 0.00183314]\n",
      "Loss: 0.15317547176611646\n",
      "Step 842: ||grad|| = 0.26184013837443354\n",
      "[1.00072475 0.00183354]\n",
      "Loss: 0.15317536877155846\n",
      "Step 843: ||grad|| = 0.2618399414051435\n",
      "[1.00072475 0.00183393]\n",
      "Loss: 0.15317526577715643\n",
      "Step 844: ||grad|| = 0.26183974443600083\n",
      "[1.00072475 0.00183432]\n",
      "Loss: 0.15317516278290935\n",
      "Step 845: ||grad|| = 0.2618395474670056\n",
      "[1.00072475 0.00183472]\n",
      "Loss: 0.15317505978881704\n",
      "Step 846: ||grad|| = 0.26183935049816015\n",
      "[1.00072475 0.00183511]\n",
      "Loss: 0.15317495679487964\n",
      "Step 847: ||grad|| = 0.26183915352946113\n",
      "[1.00072475 0.0018355 ]\n",
      "Loss: 0.1531748538010968\n",
      "Step 848: ||grad|| = 0.26183895656091166\n",
      "[1.00072475 0.0018359 ]\n",
      "Loss: 0.15317475080746956\n",
      "Step 849: ||grad|| = 0.2618387595925103\n",
      "[1.00072474 0.00183629]\n",
      "Loss: 0.15317464781399798\n",
      "Step 850: ||grad|| = 0.2618385626242559\n",
      "[1.00072474 0.00183668]\n",
      "Loss: 0.15317454482068016\n",
      "Step 851: ||grad|| = 0.2618383656561507\n",
      "[1.00072474 0.00183708]\n",
      "Loss: 0.15317444182751763\n",
      "Step 852: ||grad|| = 0.26183816868819343\n",
      "[1.00072474 0.00183747]\n",
      "Loss: 0.15317433883451023\n",
      "Step 853: ||grad|| = 0.2618379717203842\n",
      "[1.00072474 0.00183786]\n",
      "Loss: 0.15317423584165868\n",
      "Step 854: ||grad|| = 0.26183777475272374\n",
      "[1.00072474 0.00183826]\n",
      "Loss: 0.1531741328489598\n",
      "Step 855: ||grad|| = 0.26183757778521033\n",
      "[1.00072474 0.00183865]\n",
      "Loss: 0.1531740298564173\n",
      "Step 856: ||grad|| = 0.2618373808178457\n",
      "[1.00072474 0.00183904]\n",
      "Loss: 0.15317392686402925\n",
      "Step 857: ||grad|| = 0.2618371838506296\n",
      "[1.00072474 0.00183944]\n",
      "Loss: 0.15317382387179643\n",
      "Step 858: ||grad|| = 0.2618369868835613\n",
      "[1.00072474 0.00183983]\n",
      "Loss: 0.15317372087971795\n",
      "Step 859: ||grad|| = 0.2618367899166413\n",
      "[1.00072474 0.00184022]\n",
      "Loss: 0.15317361788779646\n",
      "Step 860: ||grad|| = 0.261836592949869\n",
      "[1.00072474 0.00184062]\n",
      "Loss: 0.1531735148960276\n",
      "Step 861: ||grad|| = 0.261836395983245\n",
      "[1.00072474 0.00184101]\n",
      "Loss: 0.15317341190441494\n",
      "Step 862: ||grad|| = 0.26183619901676897\n",
      "[1.00072474 0.0018414 ]\n",
      "Loss: 0.15317330891295705\n",
      "Step 863: ||grad|| = 0.2618360020504421\n",
      "[1.00072474 0.0018418 ]\n",
      "Loss: 0.1531732059216524\n",
      "Step 864: ||grad|| = 0.26183580508426274\n",
      "[1.00072474 0.00184219]\n",
      "Loss: 0.15317310293050543\n",
      "Step 865: ||grad|| = 0.2618356081182317\n",
      "[1.00072474 0.00184258]\n",
      "Loss: 0.15317299993951283\n",
      "Step 866: ||grad|| = 0.26183541115234904\n",
      "[1.00072473 0.00184298]\n",
      "Loss: 0.15317289694867334\n",
      "Step 867: ||grad|| = 0.26183521418661343\n",
      "[1.00072473 0.00184337]\n",
      "Loss: 0.1531727939579906\n",
      "Step 868: ||grad|| = 0.2618350172210275\n",
      "[1.00072473 0.00184376]\n",
      "Loss: 0.15317269096746125\n",
      "Step 869: ||grad|| = 0.2618348202555887\n",
      "[1.00072473 0.00184416]\n",
      "Loss: 0.1531725879770891\n",
      "Step 870: ||grad|| = 0.261834623290299\n",
      "[1.00072473 0.00184455]\n",
      "Loss: 0.15317248498687047\n",
      "Step 871: ||grad|| = 0.2618344263251568\n",
      "[1.00072473 0.00184494]\n",
      "Loss: 0.1531723819968057\n",
      "Step 872: ||grad|| = 0.26183422936016415\n",
      "[1.00072473 0.00184534]\n",
      "Loss: 0.1531722790068979\n",
      "Step 873: ||grad|| = 0.2618340323953172\n",
      "[1.00072473 0.00184573]\n",
      "Loss: 0.15317217601714436\n",
      "Step 874: ||grad|| = 0.2618338354306196\n",
      "[1.00072473 0.00184612]\n",
      "Loss: 0.15317207302754626\n",
      "Step 875: ||grad|| = 0.26183363846607\n",
      "[1.00072473 0.00184652]\n",
      "Loss: 0.153171970038102\n",
      "Step 876: ||grad|| = 0.2618334415016686\n",
      "[1.00072473 0.00184691]\n",
      "Loss: 0.15317186704881322\n",
      "Step 877: ||grad|| = 0.2618332445374156\n",
      "[1.00072473 0.0018473 ]\n",
      "Loss: 0.1531717640596794\n",
      "Step 878: ||grad|| = 0.2618330475733104\n",
      "[1.00072473 0.0018477 ]\n",
      "Loss: 0.1531716610707008\n",
      "Step 879: ||grad|| = 0.26183285060935396\n",
      "[1.00072473 0.00184809]\n",
      "Loss: 0.1531715580818774\n",
      "Step 880: ||grad|| = 0.2618326536455452\n",
      "[1.00072473 0.00184848]\n",
      "Loss: 0.15317145509320806\n",
      "Step 881: ||grad|| = 0.2618324566818853\n",
      "[1.00072473 0.00184888]\n",
      "Loss: 0.15317135210469332\n",
      "Step 882: ||grad|| = 0.26183225971837265\n",
      "[1.00072473 0.00184927]\n",
      "Loss: 0.1531712491163345\n",
      "Step 883: ||grad|| = 0.2618320627550085\n",
      "[1.00072472 0.00184966]\n",
      "Loss: 0.15317114612813032\n",
      "Step 884: ||grad|| = 0.26183186579179324\n",
      "[1.00072472 0.00185006]\n",
      "Loss: 0.15317104314008076\n",
      "Step 885: ||grad|| = 0.2618316688287252\n",
      "[1.00072472 0.00185045]\n",
      "Loss: 0.15317094015218657\n",
      "Step 886: ||grad|| = 0.2618314718658055\n",
      "[1.00072472 0.00185084]\n",
      "Loss: 0.15317083716444746\n",
      "Step 887: ||grad|| = 0.2618312749030335\n",
      "[1.00072472 0.00185124]\n",
      "Loss: 0.153170734176863\n",
      "Step 888: ||grad|| = 0.26183107794041144\n",
      "[1.00072472 0.00185163]\n",
      "Loss: 0.15317063118943344\n",
      "Step 889: ||grad|| = 0.26183088097793605\n",
      "[1.00072472 0.00185202]\n",
      "Loss: 0.15317052820215843\n",
      "Step 890: ||grad|| = 0.26183068401560866\n",
      "[1.00072472 0.00185242]\n",
      "Loss: 0.15317042521504057\n",
      "Step 891: ||grad|| = 0.26183048705342954\n",
      "[1.00072472 0.00185281]\n",
      "Loss: 0.15317032222807445\n",
      "Step 892: ||grad|| = 0.2618302900913983\n",
      "[1.00072472 0.0018532 ]\n",
      "Loss: 0.15317021924126525\n",
      "Step 893: ||grad|| = 0.2618300931295172\n",
      "[1.00072472 0.0018536 ]\n",
      "Loss: 0.15317011625461124\n",
      "Step 894: ||grad|| = 0.26182989616778213\n",
      "[1.00072472 0.00185399]\n",
      "Loss: 0.15317001326811092\n",
      "Step 895: ||grad|| = 0.2618296992061958\n",
      "[1.00072472 0.00185438]\n",
      "Loss: 0.15316991028176685\n",
      "Step 896: ||grad|| = 0.26182950224475915\n",
      "[1.00072472 0.00185478]\n",
      "Loss: 0.15316980729557697\n",
      "Step 897: ||grad|| = 0.2618293052834686\n",
      "[1.00072472 0.00185517]\n",
      "Loss: 0.15316970430954066\n",
      "Step 898: ||grad|| = 0.2618291083223272\n",
      "[1.00072472 0.00185556]\n",
      "Loss: 0.15316960132366111\n",
      "Step 899: ||grad|| = 0.26182891136133357\n",
      "[1.00072472 0.00185596]\n",
      "Loss: 0.1531694983379363\n",
      "Step 900: ||grad|| = 0.2618287144004879\n",
      "[1.00072471 0.00185635]\n",
      "Loss: 0.1531693953523664\n",
      "Step 901: ||grad|| = 0.2618285174397916\n",
      "[1.00072471 0.00185674]\n",
      "Loss: 0.1531692923669505\n",
      "Step 902: ||grad|| = 0.2618283204792423\n",
      "[1.00072471 0.00185714]\n",
      "Loss: 0.15316918938169083\n",
      "Step 903: ||grad|| = 0.2618281235188422\n",
      "[1.00072471 0.00185753]\n",
      "Loss: 0.1531690863965854\n",
      "Step 904: ||grad|| = 0.26182792655858833\n",
      "[1.00072471 0.00185792]\n",
      "Loss: 0.15316898341163498\n",
      "Step 905: ||grad|| = 0.2618277295984842\n",
      "[1.00072471 0.00185832]\n",
      "Loss: 0.15316888042683968\n",
      "Step 906: ||grad|| = 0.2618275326385277\n",
      "[1.00072471 0.00185871]\n",
      "Loss: 0.15316877744219956\n",
      "Step 907: ||grad|| = 0.2618273356787198\n",
      "[1.00072471 0.0018591 ]\n",
      "Loss: 0.1531686744577132\n",
      "Step 908: ||grad|| = 0.26182713871905966\n",
      "[1.00072471 0.0018595 ]\n",
      "Loss: 0.1531685714733822\n",
      "Step 909: ||grad|| = 0.26182694175954796\n",
      "[1.00072471 0.00185989]\n",
      "Loss: 0.15316846848920762\n",
      "Step 910: ||grad|| = 0.26182674480018403\n",
      "[1.00072471 0.00186028]\n",
      "Loss: 0.15316836550518678\n",
      "Step 911: ||grad|| = 0.26182654784096887\n",
      "[1.00072471 0.00186068]\n",
      "Loss: 0.15316826252132132\n",
      "Step 912: ||grad|| = 0.26182635088190137\n",
      "[1.00072471 0.00186107]\n",
      "Loss: 0.1531681595376105\n",
      "Step 913: ||grad|| = 0.2618261539229824\n",
      "[1.00072471 0.00186146]\n",
      "Loss: 0.1531680565540539\n",
      "Step 914: ||grad|| = 0.2618259569642109\n",
      "[1.00072471 0.00186186]\n",
      "Loss: 0.15316795357065383\n",
      "Step 915: ||grad|| = 0.26182576000558916\n",
      "[1.00072471 0.00186225]\n",
      "Loss: 0.15316785058740748\n",
      "Step 916: ||grad|| = 0.261825563047114\n",
      "[1.00072471 0.00186264]\n",
      "Loss: 0.15316774760431623\n",
      "Step 917: ||grad|| = 0.2618253660887865\n",
      "[1.0007247  0.00186304]\n",
      "Loss: 0.1531676446213799\n",
      "Step 918: ||grad|| = 0.26182516913060805\n",
      "[1.0007247  0.00186343]\n",
      "Loss: 0.1531675416385986\n",
      "Step 919: ||grad|| = 0.2618249721725784\n",
      "[1.0007247  0.00186382]\n",
      "Loss: 0.1531674386559717\n",
      "Step 920: ||grad|| = 0.26182477521469555\n",
      "[1.0007247  0.00186422]\n",
      "Loss: 0.1531673356735016\n",
      "Step 921: ||grad|| = 0.26182457825696237\n",
      "[1.0007247  0.00186461]\n",
      "Loss: 0.15316723269118512\n",
      "Step 922: ||grad|| = 0.2618243812993764\n",
      "[1.0007247 0.001865 ]\n",
      "Loss: 0.15316712970902346\n",
      "Step 923: ||grad|| = 0.2618241843419392\n",
      "[1.0007247 0.0018654]\n",
      "Loss: 0.15316702672701663\n",
      "Step 924: ||grad|| = 0.26182398738464974\n",
      "[1.0007247  0.00186579]\n",
      "Loss: 0.1531669237451651\n",
      "Step 925: ||grad|| = 0.26182379042750836\n",
      "[1.0007247  0.00186618]\n",
      "Loss: 0.153166820763469\n",
      "Step 926: ||grad|| = 0.2618235934705154\n",
      "[1.0007247  0.00186658]\n",
      "Loss: 0.15316671778192784\n",
      "Step 927: ||grad|| = 0.261823396513671\n",
      "[1.0007247  0.00186697]\n",
      "Loss: 0.15316661480054017\n",
      "Step 928: ||grad|| = 0.26182319955697375\n",
      "[1.0007247  0.00186736]\n",
      "Loss: 0.15316651181930804\n",
      "Step 929: ||grad|| = 0.26182300260042346\n",
      "[1.0007247  0.00186776]\n",
      "Loss: 0.15316640883823043\n",
      "Step 930: ||grad|| = 0.2618228056440248\n",
      "[1.0007247  0.00186815]\n",
      "Loss: 0.15316630585730953\n",
      "Step 931: ||grad|| = 0.26182260868777313\n",
      "[1.0007247  0.00186854]\n",
      "Loss: 0.15316620287654303\n",
      "Step 932: ||grad|| = 0.26182241173166765\n",
      "[1.0007247  0.00186894]\n",
      "Loss: 0.1531660998959305\n",
      "Step 933: ||grad|| = 0.26182221477571266\n",
      "[1.0007247  0.00186933]\n",
      "Loss: 0.15316599691547286\n",
      "Step 934: ||grad|| = 0.26182201781990455\n",
      "[1.00072469 0.00186972]\n",
      "Loss: 0.15316589393517072\n",
      "Step 935: ||grad|| = 0.2618218208642444\n",
      "[1.00072469 0.00187012]\n",
      "Loss: 0.15316579095502333\n",
      "Step 936: ||grad|| = 0.2618216239087331\n",
      "[1.00072469 0.00187051]\n",
      "Loss: 0.15316568797503058\n",
      "Step 937: ||grad|| = 0.2618214269533693\n",
      "[1.00072469 0.0018709 ]\n",
      "Loss: 0.15316558499519395\n",
      "Step 938: ||grad|| = 0.2618212299981547\n",
      "[1.00072469 0.0018713 ]\n",
      "Loss: 0.15316548201551178\n",
      "Step 939: ||grad|| = 0.2618210330430874\n",
      "[1.00072469 0.00187169]\n",
      "Loss: 0.15316537903598418\n",
      "Step 940: ||grad|| = 0.26182083608816853\n",
      "[1.00072469 0.00187208]\n",
      "Loss: 0.1531652760566111\n",
      "Step 941: ||grad|| = 0.26182063913339854\n",
      "[1.00072469 0.00187248]\n",
      "Loss: 0.15316517307739377\n",
      "Step 942: ||grad|| = 0.26182044217877626\n",
      "[1.00072469 0.00187287]\n",
      "Loss: 0.15316507009833108\n",
      "Step 943: ||grad|| = 0.26182024522430136\n",
      "[1.00072469 0.00187326]\n",
      "Loss: 0.15316496711942354\n",
      "Step 944: ||grad|| = 0.26182004826997546\n",
      "[1.00072469 0.00187366]\n",
      "Loss: 0.15316486414067015\n",
      "Step 945: ||grad|| = 0.2618198513157973\n",
      "[1.00072469 0.00187405]\n",
      "Loss: 0.1531647611620721\n",
      "Step 946: ||grad|| = 0.2618196543617672\n",
      "[1.00072469 0.00187444]\n",
      "Loss: 0.15316465818363034\n",
      "Step 947: ||grad|| = 0.2618194574078857\n",
      "[1.00072469 0.00187484]\n",
      "Loss: 0.15316455520534095\n",
      "Step 948: ||grad|| = 0.26181926045415177\n",
      "[1.00072469 0.00187523]\n",
      "Loss: 0.1531644522272088\n",
      "Step 949: ||grad|| = 0.2618190635005671\n",
      "[1.00072469 0.00187562]\n",
      "Loss: 0.15316434924923028\n",
      "Step 950: ||grad|| = 0.26181886654712916\n",
      "[1.00072469 0.00187602]\n",
      "Loss: 0.15316424627140643\n",
      "Step 951: ||grad|| = 0.2618186695938406\n",
      "[1.00072468 0.00187641]\n",
      "Loss: 0.15316414329373787\n",
      "Step 952: ||grad|| = 0.2618184726406996\n",
      "[1.00072468 0.0018768 ]\n",
      "Loss: 0.15316404031622496\n",
      "Step 953: ||grad|| = 0.26181827568770677\n",
      "[1.00072468 0.0018772 ]\n",
      "Loss: 0.15316393733886627\n",
      "Step 954: ||grad|| = 0.26181807873486246\n",
      "[1.00072468 0.00187759]\n",
      "Loss: 0.15316383436166325\n",
      "Step 955: ||grad|| = 0.2618178817821656\n",
      "[1.00072468 0.00187798]\n",
      "Loss: 0.15316373138461464\n",
      "Step 956: ||grad|| = 0.26181768482961715\n",
      "[1.00072468 0.00187838]\n",
      "Loss: 0.15316362840771985\n",
      "Step 957: ||grad|| = 0.26181748787721704\n",
      "[1.00072468 0.00187877]\n",
      "Loss: 0.15316352543098066\n",
      "Step 958: ||grad|| = 0.2618172909249653\n",
      "[1.00072468 0.00187916]\n",
      "Loss: 0.15316342245439815\n",
      "Step 959: ||grad|| = 0.26181709397286157\n",
      "[1.00072468 0.00187956]\n",
      "Loss: 0.15316331947796852\n",
      "Step 960: ||grad|| = 0.26181689702090577\n",
      "[1.00072468 0.00187995]\n",
      "Loss: 0.15316321650169448\n",
      "Step 961: ||grad|| = 0.26181670006909824\n",
      "[1.00072468 0.00188034]\n",
      "Loss: 0.15316311352557563\n",
      "Step 962: ||grad|| = 0.2618165031174386\n",
      "[1.00072468 0.00188074]\n",
      "Loss: 0.15316301054961082\n",
      "Step 963: ||grad|| = 0.2618163061659263\n",
      "[1.00072468 0.00188113]\n",
      "Loss: 0.15316290757380177\n",
      "Step 964: ||grad|| = 0.26181610921456416\n",
      "[1.00072468 0.00188152]\n",
      "Loss: 0.15316280459814685\n",
      "Step 965: ||grad|| = 0.2618159122633494\n",
      "[1.00072468 0.00188192]\n",
      "Loss: 0.15316270162264867\n",
      "Step 966: ||grad|| = 0.26181571531228287\n",
      "[1.00072468 0.00188231]\n",
      "Loss: 0.15316259864730428\n",
      "Step 967: ||grad|| = 0.26181551836136513\n",
      "[1.00072468 0.0018827 ]\n",
      "Loss: 0.1531624956721153\n",
      "Step 968: ||grad|| = 0.2618153214105943\n",
      "[1.00072467 0.0018831 ]\n",
      "Loss: 0.1531623926970795\n",
      "Step 969: ||grad|| = 0.2618151244599717\n",
      "[1.00072467 0.00188349]\n",
      "Loss: 0.15316228972220067\n",
      "Step 970: ||grad|| = 0.26181492750949753\n",
      "[1.00072467 0.00188388]\n",
      "Loss: 0.1531621867474762\n",
      "Step 971: ||grad|| = 0.2618147305591719\n",
      "[1.00072467 0.00188428]\n",
      "Loss: 0.1531620837729062\n",
      "Step 972: ||grad|| = 0.2618145336089939\n",
      "[1.00072467 0.00188467]\n",
      "Loss: 0.1531619807984906\n",
      "Step 973: ||grad|| = 0.2618143366589633\n",
      "[1.00072467 0.00188506]\n",
      "Loss: 0.15316187782423088\n",
      "Step 974: ||grad|| = 0.26181413970908207\n",
      "[1.00072467 0.00188546]\n",
      "Loss: 0.15316177485012575\n",
      "Step 975: ||grad|| = 0.2618139427593496\n",
      "[1.00072467 0.00188585]\n",
      "Loss: 0.15316167187617605\n",
      "Step 976: ||grad|| = 0.26181374580976335\n",
      "[1.00072467 0.00188624]\n",
      "Loss: 0.15316156890238014\n",
      "Step 977: ||grad|| = 0.2618135488603263\n",
      "[1.00072467 0.00188664]\n",
      "Loss: 0.1531614659287399\n",
      "Step 978: ||grad|| = 0.2618133519110385\n",
      "[1.00072467 0.00188703]\n",
      "Loss: 0.15316136295525495\n",
      "Step 979: ||grad|| = 0.2618131549618976\n",
      "[1.00072467 0.00188742]\n",
      "Loss: 0.15316125998192387\n",
      "Step 980: ||grad|| = 0.26181295801290483\n",
      "[1.00072467 0.00188782]\n",
      "Loss: 0.15316115700874833\n",
      "Step 981: ||grad|| = 0.261812761064061\n",
      "[1.00072467 0.00188821]\n",
      "Loss: 0.1531610540357284\n",
      "Step 982: ||grad|| = 0.26181256411536447\n",
      "[1.00072467 0.0018886 ]\n",
      "Loss: 0.15316095106286234\n",
      "Step 983: ||grad|| = 0.26181236716681683\n",
      "[1.00072467 0.001889  ]\n",
      "Loss: 0.1531608480901517\n",
      "Step 984: ||grad|| = 0.26181217021841646\n",
      "[1.00072467 0.00188939]\n",
      "Loss: 0.15316074511759642\n",
      "Step 985: ||grad|| = 0.26181197327016514\n",
      "[1.00072466 0.00188978]\n",
      "Loss: 0.1531606421451954\n",
      "Step 986: ||grad|| = 0.2618117763220607\n",
      "[1.00072466 0.00189018]\n",
      "Loss: 0.15316053917294903\n",
      "Step 987: ||grad|| = 0.26181157937410493\n",
      "[1.00072466 0.00189057]\n",
      "Loss: 0.15316043620085762\n",
      "Step 988: ||grad|| = 0.26181138242629826\n",
      "[1.00072466 0.00189096]\n",
      "Loss: 0.15316033322892122\n",
      "Step 989: ||grad|| = 0.2618111854786392\n",
      "[1.00072466 0.00189136]\n",
      "Loss: 0.15316023025714132\n",
      "Step 990: ||grad|| = 0.26181098853112833\n",
      "[1.00072466 0.00189175]\n",
      "Loss: 0.15316012728551404\n",
      "Step 991: ||grad|| = 0.2618107915837647\n",
      "[1.00072466 0.00189214]\n",
      "Loss: 0.15316002431404263\n",
      "Step 992: ||grad|| = 0.26181059463655026\n",
      "[1.00072466 0.00189254]\n",
      "Loss: 0.15315992134272655\n",
      "Step 993: ||grad|| = 0.2618103976894842\n",
      "[1.00072466 0.00189293]\n",
      "Loss: 0.15315981837156425\n",
      "Step 994: ||grad|| = 0.2618102007425654\n",
      "[1.00072466 0.00189332]\n",
      "Loss: 0.15315971540055875\n",
      "Step 995: ||grad|| = 0.26181000379579555\n",
      "[1.00072466 0.00189372]\n",
      "Loss: 0.15315961242970652\n",
      "Step 996: ||grad|| = 0.26180980684917315\n",
      "[1.00072466 0.00189411]\n",
      "Loss: 0.15315950945901013\n",
      "Step 997: ||grad|| = 0.2618096099026991\n",
      "[1.00072466 0.0018945 ]\n",
      "Loss: 0.15315940648846907\n",
      "Step 998: ||grad|| = 0.26180941295637356\n",
      "[1.00072466 0.0018949 ]\n",
      "Loss: 0.15315930351808155\n",
      "Step 999: ||grad|| = 0.2618092160101962\n",
      "[1.00072466 0.00189529]\n",
      "Loss: 0.15315920054784932\n",
      "Step 1000: ||grad|| = 0.2618090190641672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_8464\\1785926926.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  ys = float(y_k.T @ s_k)\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_8464\\1785926926.py:19: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  alpha_i = rho_i * float(s_i.T @ q)\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_8464\\1785926926.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  H0 = float(s_last.T @ y_last) / float(y_last.T @ y_last)\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_8464\\1785926926.py:34: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  beta_i = rho_i * float(y_i.T @ r)\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "data = get_data(1000)\n",
    "\n",
    "model.fit(data[:,0].reshape((-1, 1)), data[:,1], lr_fn=lambda i: 0.0000001 * 0.99**i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f29dca54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19915f04220>]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVrNJREFUeJzt3Qd8zef+B/BP9pSQIagQtQkNqjYlsSJCi1Z1GLl6i4gVYhRXVRGrkhitGr23xq32tiFWI7E3iS32CjKIJLLHOf/X7/FP6sTvtEZycsbn/XrlxvOc4/HL71by8fs+w0ipVCpBREREpEOMy/sCiIiIiF4WAwwRERHpHAYYIiIi0jkMMERERKRzGGCIiIhI5zDAEBERkc5hgCEiIiKdwwBDREREOscUekqhUOD+/fuoUKECjIyMyvtyiIiI6AVI++s+efIE1apVg7GxseEFGCm8uLq6lvdlEBER0Su4e/cuqlevbngBRnryUnQD7OzsyvtyiIiI6AWkp6eLBxBFP8cNLsAUlY2k8MIAQ0REpFv+bvoHJ/ESERGRzmGAISIiIp3DAENEREQ6hwGGiIiIdA4DDBEREekcBhgiIiLSOQwwREREpHMYYIiIiEjnMMAQERGR/geY/fv3o3fv3uKQJWmXvN9///25Q5hmzJiBqlWrwsrKCl5eXrh69arKe1JSUvDxxx+LHXIrVqwIPz8/ZGRkqLzn7Nmz6NChAywtLcWWwsHBwa/6NRIREVEpKFQoceByMsZuisXn/z6JVftvIK9AAZ0IMJmZmXjrrbewbNky2deloBESEoKVK1fi2LFjsLGxQffu3ZGTk1P8Him8XLhwAZGRkYiIiBCh6PPPP1c5B6Fbt26oWbMmTp06hQULFuBf//oXvv/++1f9OomIiOg1hJ++h/pf7sCna4/j99P38cfFRMzZfgn1p+/A3O0XoWlGSumRyav+ZiMj/Pbbb+jbt69oS0NJT2YmTJiAwMBA0ZeWlgYXFxesW7cOAwcOxKVLl9CoUSOcOHECb7/9tnjPzp074e3tjfj4ePH7V6xYgWnTpiEhIQHm5ubiPZMnTxZPe+Li4l7o2qQQZG9vL/58noVERET06nqHHsC5e+l/+Z5/dqyFKd6N8Lpe9Od3qc6BuXnzpggdUtmoiHQRrVq1wpEjR0Rb+iyVjYrCi0R6v7GxsXhiU/Sejh07FocXifQU5/Lly3j8+LHsn52bmyu+6Gc/iIiI6PVKRi1n//FceHnn7nn8+7/TYZX3Z3Vl1YGbGi0nlWqAkcKLRHri8iypXfSa9Lly5coqr5uamsLBwUHlPXJjPPtnlDR37lwRloo+pHkzRERE9Gq2nrmP2lO3Izkzv7jPWFEI/8ObsHHjVHS8FYuRRzcXv6ZQAv85cguaYgo9MWXKFIwfP764LT2BYYghIiJ6eX7rTiAqLkmlzynzMZZsXYQOt0+L9i/unljeeoDKe26nZEEnA0yVKlXE58TERLEKqYjU9vDwKH5PUpLqTSkoKBArk4p+v/RZ+j3PKmoXvackCwsL8UFERESvXjLyXLgHt1KyVfrb3D6DpVsXonLmY2SZWWB615H4tYnnc7+/poM1dLKEVKtWLREwoqKiVJ6ESHNb2rRpI9rS59TUVLG6qEh0dDQUCoWYK1P0HmllUn7+n4+tpBVL9evXR6VKlUrzkomIiAh/loyeDS9SyWjcgfVYv+lLEV4uO9WA72dLZMOLkRHwaRs37X0CI+3Xcu3aNZWJu6dPnxZzWGrUqIGxY8fi66+/Rt26dUWgmT59ulhZVLRSqWHDhujRoweGDx8ullpLIcXf31+sUJLeJxk0aBBmzZol9ocJCgrC+fPnsXTpUixZsqQ0v3YiIiICMGzdcUTHJav0VX7yCEsjFqLNnXOivbFpN8zy+hw5ZpayY/yjfS2Ymxpr7zLqvXv3onPnzs/1Dx48WCyVloabOXOm2LNFetLSvn17LF++HPXq1St+r1QukkLL1q1bxeqjfv36ib1jbG1tVTayGzVqlFhu7eTkhNGjR4sw86K4jJqIiOjvS0at50SqTNSVdLgZgyURi+CUlYYMcytM7T4KWxq9q3acro0qY9VnLVEaXvTn92vtA6PNGGCIiIjUB5elkVcQsufPiorERFGI8Qd+wqj/X110sXItjOozGTcd3lAzEhA20AM+HupfL6uf33qzComIiIj+3s7zDxCw6fRze7ZUSX+IkK3BeCf+6a66P3n0xGzP4cg1/XNPtme5VbJE1MQuMDE2QnlggCEiIjIQ288+wMgNMc/1v3v9BBZvWwKH7HQ8MbfC5B4B2Nawg9pxutR3wpqhTxfelBcGGCIiIgMQHnsPY/77dA+XIqaFBQjc/298cfx/on3OpbYoGd2p9OdWKCX5ta+J6T7uKG8MMERERHo+32XAisOIuZuq0v9GWhJCt8xH8/uXRXtti96Y++4w5JmayY4jrS8KG9QM3k2frhgubwwwREREelwy8t8Qg5InFHW9ehQLtn+LijkZSLOwwSTvMdhVr63acZq72mHziPblNt9FDgMMERGRHpodcRGrD95U6TMrzMeUPWsx7NQW0T5dtR78+wQh3l71/MGyXGVUWhhgiIiI9Kxk1H/5IcTGp6n0u6YmICx8Pt5KuCra37d8Dws6fYZ8E/mSkbONGY5O66pVT12exQBDRESk5yWjnnEHMX9HCOzysvDYsgICe41FVB31q4i61HfEmqGtoc0YYIiIiPS0ZGRRkIdp0avxWew20T75RkME+E7EfbvKsmNIz1pCtbRkVBIDDBERkR6WjNxS7iFsSzDcE6+L9opW/bGowycoMJH/0e/mYImowPLbmO5lMcAQERHp8AnSARtjUfJMoN4X92HurjDY5mXjkZUdxvtMwL43W6gdx7OBE1YPKd+N6V4WAwwREZEO8lt3AlFxSSp9Fvm5mBm1CoPO7BTtY67uCOgdiMQKTjpfMiqJAYaIiEjHSkaeC/fgVkq2Sn/tR3fFKqOGybeggBHC2nyApe0HodDYRC9KRiUxwBAREelQyWj0xtjn+t87H42v/1gOm/wcJNtUxFifQBxy89CrklFJDDBEREQ6YNi644iOS1bps8rLwazdK/HBud2ifahmU4z1mYhk20p6VzIqiQGGiIhIy0tGredEIjkzX6W/bvJtLAufj3qP7qDQyBhL230kykYKNSUjj+p2+HWkdh0H8DoYYIiIiHSpZKRUYsC5SHwV+R2sCnKRaOuAMb0DcbRGU70uGZXEAENERKQjJ0hb52Vjzq5leO/iXtHe79YM43wm4JFNRbVjDe/ghmm9GkPfMMAQERFpkZ3nH2DU+hgUltjcpWHSDbHKqHbKPRQYGWNxh0+wonV/KI2MZcep42yN7WM6wdxU/nVdxwBDRESkRWcZjdwQo9qpVIp9XWbu/h4Whfl4YOuI0X0m4WR19U9V/NrXxHQfd+gzBhgiIiItEB57D2P+e1qlzzY3C3N3hqJ33AHRjqrdEoHeY/HY2l52DOlZS9igZvBuWg36jgGGiIhIC5dIN064JlYZuaU+QL6xCYI7DsYP7/RVWzJq7mqHzSP0Z5XR32GAISIiKid5BQq0/PoPpOUU/tmpVOKzmAhM27MaFoUFiLerjNG+kxD7RgO144Tpyd4uL4MBhoiIqBzMjriI1QdvqvTZ5WRg/o4Q9LxyWLT/qNtalIzSLW1lx3C2McPRaV0N5qnLsxhgiIiINLxE2mvRXtx8lKXS/9b9ywjbEgzXtETkGZtibuehWNvCFzCSDydd6jtizdDWMFQMMEREROW8MZ3fyXAE7V0Hc0UB7ti7wL9PEM5Wraf3xwG8DgYYIiIiDfBbdwJRcUkqffbZT7Bw+xJ0vXZctLfVb4fJPQPwxMJGL0+QLk0MMERERGU8Ubfj/CgkPMlT6W8efwmhW4LxxpNk5JqYYnaX4fipmbfakpF7VVtEjOmkoavWfgwwREREGpyoa6RU4PPj/8PEff+GqVKBm5Wqwr/PZFxwqW1QZxm9LgYYIiKiMtA79ADO3UtX6XPISsOibYvR+cYp0Q5v2AlTu49CpoW12nEMcYn0i2CAISIiKuVVRl0WROP24xyV/pZ3z4uSUZWMFOSYmuNfnp9j01vd1ZaMnG3NcHSqYS6RfhEMMERERGW4ykgqGY08shnjD66HiVKB6w7VMapPEOIq11I7ztB2NTCzdxMNXLHuYoAhIiIqo1VGTpmPsThiMTreehpqfm3cGdO7jUSWuZXsGC4VzHAgyEtvT5AuTQwwREREZbDKqM3tM1i6dSEqZz5GlpkFZnQdgV+aeKkdx72aLSICuMroRTHAEBERveJcl9EbYrD9fIJKv7GiEAGHNyHg0CYYQ4nLTjUwqs9kXHOqoXYsv/Y1Md3HXQNXrT8YYIiIiF7SzvMP4L8hFgUKpUq/c0aKeOrS9s5Z0d7UtBv+5fU5cswsZcdxsTPDgUksGb0KBhgiIqKXsP3sA4zcEPNcf4ebMVgSsQhOWWnINLMUy6PDG3dWO87QdjUxszefurwqBhgiIqIXFB57D2P+e1qlz0RRiHEH14uVRlLJ6JKzmygZ3XCsrnYc7u3y+hhgiIiIXsCwdccRHZes0lcl/SGWbl2AVvEXRPsnj56Y3eUfyDWzkB3DxAhY9nFz9HCvqpFr1mcMMERERH8zWbf1nEgkZ+ar9L97/QQWb1sCh+x0PDG3wpQeoxHRsKPacZq52uGXEe25MV0pYYAhIiJ6iY3pTAsLELj/3/ji+P9E+5xLbfj3CcLtStXUjsNVRqWPAYaIiEjmqcuAFYcRczdVpb9aehJCw4PR4n6caK9t0Rtz3x2GPFMz2XGktUVhg5rBu6n6cEOvhgGGiIioxCoj/w0xUJTo97p6DAu3L0HFnAykW9hgYs8x2FW/rdpxvN1dEDqoBUtGZYQBhoiI6P/NjriI1QdvqvSZFeZj8t518DsZLtqnq9aFv28Q4itWkR2DxwFoBgMMEREZPKlk1H/5IcTGp6n0V09NQNiW+fB4cFW0V7Xsi+BOg5FvIl8ycq9qi4gxPA5AExhgiIgIhj5RN2BjLFT31AV6XD6E4B0hsMvNRKqlLSb0GoeoOq3UjuPZwAmrh6h/nUoXAwwRERksuROkLQryMHXPagyO2Sbap6o1wOg+k3DfrrLacbgxneYxwBARkUGWjDwX7sGtlGyV/pqP72NZ+Hy4J14X7ZWt+mFhh09RYCL/49LZxgxHp3XlRN1ywABDREQw9L1dJL0v7sM3u8JQIS8bj6zsMKHXeOyt/bbacbrUd8Saoa3L+GpJHQYYIiIy7JJRfi5mRq3CoDM7RftY9cYI8J2IxApOsmNIz1pCWTIqdwwwRERkEHxC9uP8/ScqfW8+isey8HlomHwLChghrM0HWNp+EAqNTWTHcHOwRFRgF5aMtAADDBER6bW8AgU6zNuNxAzVs4z6XtiDObuWwSY/B8nWFTHOZwIO1mqmdhyuMtIuDDBERKS3vtp6AWsO3VLps8zPwVeRK/HBud2ifbhGU4zpHYhkWwfZMVgy0k4MMEREpJerjNp8sxtJGXkq/XWTb4tVRvUe3UGhkTGWtvtIlI0UakpGPEFaezHAEBGR3p1lNHJDjGqnUokB53aLJy9WBblIsqkkJuoerdFU7Tg8QVq7McAQEZFen2VknZeN2X8sR78Le0R7v1szMd/lkU1F2TF4grRuKPWTpgoLCzF9+nTUqlULVlZWqF27NmbPng2l8s9NmqVfz5gxA1WrVhXv8fLywtWrT8+ZKJKSkoKPP/4YdnZ2qFixIvz8/JCRkVHal0tERHpSMuobduC58NIg6Sa2/jhOhBepZBTc8TMM/mCW2vAilYyufuPN8GKIT2Dmz5+PFStW4Mcff0Tjxo1x8uRJDB06FPb29ggICBDvCQ4ORkhIiHiPFHSkwNO9e3dcvHgRlpaW4j1SeHnw4AEiIyORn58vxvj888+xYcOG0r5kIiLSt43plEp8dGYX/rX7O1gU5uOBraMoGZ1wVV8SYslItxgpn300Ugp8fHzg4uKC1atXF/f169dPPGn56aefxNOXatWqYcKECQgMDBSvp6Wlid+zbt06DBw4EJcuXUKjRo1w4sQJvP32010Qd+7cCW9vb8THx4vf/3fS09NFaJLGlp7iEBGR/j11GbDiMGLupqr02+ZmiR11fS/tF+3oN98WBzE+traXHYclI+3yoj+/S72E1LZtW0RFReHKlSuifebMGRw8eBA9e/YU7Zs3byIhIUGUjYpIF9qqVSscOXJEtKXPUtmoKLxIpPcbGxvj2LFjpX3JRESkY3aef4B607Y/F14aJ17H1h/HiPCSb2yCb94dCr/+M9SGFzdHS5aMdFSpl5AmT54s0lODBg1gYmIi5sTMmTNHlIQkUniRSE9cniW1i16TPleurHrqp6mpKRwcHIrfU1Jubq74KCJdAxERGc4qo09jt+HL6B9gUViAeDtnBPhOQswbDdWOw5KRbiv1APPzzz9j/fr1Yq6KNAfm9OnTGDt2rCj7DB48GGVl7ty5mDVrVpmNT0RE5V8yWvLHZYTtfXpSdBG7nAzM2xEC7yuHRTuyTisEeo9FmlUF2XHsLU1w4stuMDct9SIE6XKAmThxongKI81lkTRp0gS3b98WAUMKMFWqVBH9iYmJYhVSEant4eEhfi29JylJ9bCtgoICsTKp6PeXNGXKFIwfP17lCYyrq2tpf3lERFROT13GbIpFvkJ12mbTB1cQFj4fNdISkWdsirmdh2JtC1/ASH7juc71HLF2GE+Q1gelHj+zsrLEXJVnSaUkhUIhfi2tOpJCiDRP5tmwIc1tadOmjWhLn1NTU3Hq1Kni90RHR4sxpLkyciwsLMRkn2c/iIhIP/Z2kUpGKuFFqcSwE+H45adJIrzcsXdB/0+CsfbtPmrDi1QyYnjRH6X+BKZ3795izkuNGjVECSk2NhaLFy/GsGHDxOtGRkaipPT111+jbt26xcuopRJT3759xXsaNmyIHj16YPjw4Vi5cqVYRu3v7y+e6rzICiQiItKPklH/5YcQG5+m0m+f/QQLt3+LrteeLurYXq8tJvcMQLqlrew4UpxZxlVGeqfUA0xoaKgIJCNHjhRlIClw/POf/xQb1xWZNGkSMjMzxb4u0pOW9u3bi2XSRXvASKR5NFJo8fT0FE90pKXY0t4xRERkGHu7BGyMRcl9Pprfu4SQLcGonp6MXBNTfN3lH/hPs15qn7pIq4yiJnThWUZ6qNT3gdEW3AeGiEg3+a07gag41XmQRkoFhh//DRP3/xtmikLcrFQV/n0m44JLbbXjhHzwFnybV9fAFVN5/PzmWUhERKQ1JSPPhXtwKyVbpb9SVhoWbVuCLjdOivaWhh0xtbs/MiysZcdxc7BEVCCfuug7BhgiItKKVUajNsQ8VzJqefc8QrYsQNWMR8gxNccsz8+x8a3uaktG7lVtETGmk0aumcoXAwwREWndCdJSyWjE0V8w/sBPMFUqcN2hOkb1CUJc5Vpqx/Fs4ITVQ+RXqpL+YYAhIqJyM2ztcURfTlbpc8xMxZKIReh46+kBjb827ozp3UYiy9xKdgypUhTyoQd8PN7QyDWTdmCAISIirZnv0ub2WSzdugCVMx8j29QCM7p+gc1NvNSWjJq72mHziPac72KAGGCIiEjjS6RHb3z6dKWIsaIQow//FwGHN8FEqcAVxxoY2XcyrjnVUDvO8A5umNarsQaumLQRAwwREWnMsHXHER2nWjJyzkjBtxEL0e72WdH+b5OumNn1n8gx+3NvsGfVdbbGtjGdeJaRgWOAISIijZSMWs+JRHJmvkp/+5uxYr6Lc1YqMs0sMa37KPzeuLPacUI+8IBvc851IQYYIiIqh5KRiaIQYw9uwKgjP8MYSlxydoN/nyBcd1R/CO9yHgdAz2CAISKiMnvqMmDFYcTcTVXpr5L+UEzUbRV/QbTXe/TAV12GI9fMQnYcM2MgdFBz9HCvqpHrJt3AAENERGWyMZ3/hhgoSvS/e/0kFm9bDIfsdDwxtxI76m5tpH7juR7uLlg2qAVXGdFzGGCIiKjMN6YzLSxA4IH/4Itjv4r2eZfaYmO625XUl4S4yoj+CgMMERGVWsmo//JDiI1PU+mvlp6E0PBgtLgfJ9rrmvtgbudhyDU1lx1HWlsUxvku9DcYYIiIqFQm6gZsjH3uLCOvq8ewcPsSVMzJQLqFDSb1DMDO+u3UjsON6ehFMcAQEdFr8Vt3AlFxSSp9ZoX5CNq7Dv84GS7ap6vWxWjfINytWEXtOGEDeRwAvTgGGCIiKtXjAKqnJiBsSzA8HlwR7R/e7oP57w5BvomZ7DjONmY4Oq0rn7rQS2GAISKiUtnbRdL98mEs2LEUdrmZSLW0RaD3OOyuq/6E6C71HbFmaOsyvlrSRwwwRET02iUj84J8TN2zGkNiIkT7VLUGCPCdhHv2lWXHkJ61hLJkRK+BAYaIiF6rZFTz8X2Ehc9Hk8Tror2yVT8s7PApCkzkf8S4OVgiKrALS0b0WhhgiIjolUtGPpf2Y+7OUFTIy0aKlR3G9xqHvbVbqh3Hs4ETVg9RX1IielEMMERE9NIlI4v8XMyIXoWPT+8U7ePVGyGg9yQk2DnJjsGSEZU2BhgiInqpktGbj+KxLHweGibfggJGWNbmA3zbfhAKjU1kx3FztETUBJaMqHQxwBAR0QuXjPpe2IM5u5bBJj8HD63tMdYnEAdrNVM7jl/7mpju417GV0uGiAGGiIhUDFt3HNFxySp9lvk5mBX5HT48Fynah2s0xZjegUi2dZAdw6WCGQ4EecHcVDoYgKj0McAQEVGxDvOjcPdxjkpfnYd3RMmo/sM7omS0tN1HCG37IRRqSkZd6jthzVBO1KWyxQBDRERivkurr//Aw6wClf7+53Zj9h8rYFWQiySbShjTeyKO1GyqdhyWjEhTGGCIiAyc3HwX67xszI5cgX7no0V7v1szjPcZj4c2lWTH4AnSpGkMMEREBvzUpf+KQ4i9m6bSXz/5Fpb9Pg91UuJRaGSMRR0+wYrW/aE0kp/P0szVDr/wBGnSMAYYIiIDJLvKSKnEwDO78K+o72FZkIcHto4I8J2IE67qS0IsGVF5YYAhIjKwpy4DVhxGzN1UlX7b3Cx8sysMvpf2i/aeN1tgfK/xeGxtLzsOS0ZU3hhgiIgMxPazD+C/IQaKEv2NE68jLHweaj1+gAIjYwR3GoxV77yntmTU3NUOm1kyonLGAENEZABmR1zE6oM3VTuVSnwSux3To1fBorAA9yo4Y3SfSYh5o6HaccJ4HABpCQYYIiJ9n6i7/BBi41Un6lbIzcS8HSHodfmQaEfWaYVA77FIs6ogO46TjRmOTevKpy6kNRhgiIj0eKJuwMZYKEv0N31wBWHh81EjLRF5xqaY9+5QrHnbFzCSDyeNq9hg29h3NXLNRC+KAYaIyEBOkJZKRkNPbcGUPWthrijAXXsXjOoThLNV66kdx7OBE1YP4a66pH0YYIiIDOAEafvsJ1iwYym6XT0q2jvqtUVQzwCkW9qqHYvzXUibMcAQEen5CdLN7sUhdMt8VE9PRq6JKb7u8g/8p1kvtSUjN0dLRE3owvkupNUYYIiI9LRkZKRU4B/Hf8ek/T/CTFGIWxWripLRhSp11I4T8sFb8G1eXQNXTPR6GGCIiPSwZFQpKw0Lt38Lz+snRHtrgw6Y0mM0MiysZcdxc7BEVCCfupDuYIAhItLhjelGbYh5bpXR2/EXEBoejKoZj5BrYoZ/ef0TG9/qrrZkxIm6pIsYYIiI9GRjOqlkNOLoLxh/4CeYKhW47vAGRvWZjLjKtWTHkOJMKCfqko5igCEi0jHD1h5H9OVklT7HzFQsiViEjreeTuL9X+PO+LLbSGSZW8mOwROkSdcxwBAR6Yi8AgU6zo9CwpM8lf7Wd85i6daFcMlIQbapBWZ0/Sc2N+mqtmTEE6RJHzDAEBHpaMnIWFEI/yM/Y8yhjTBRKnDFsYZYZXTVuabsGDxBmvQJAwwRkZbzCT2A8/fSVfqcMx7j24gFaHf7rGj/3MQLM72+QLa5pewYPEGa9A0DDBGRFi+RbvX1H3iYVaDS3+7WaXy7dSGcs1KRaWYp5rr85t5F7TghH3jAtzkn6pJ+YYAhItLC4LI08gpC9lxT6TdRFGLMwQ2ibGQMJS45u8G/TxCuO7qqHWs5S0akpxhgiIi0bG+XMZtika9Q3d3F5clDhGxdiFZ3z4v2hrd6YJbncOSaWciOY2YMhA5qjh7uVTVy3USaxgBDRKQl5my7iFUHVCfqSjrdOIXFEYvgmJ2ODHMrTOnuj62NOqkdp4e7C5YNasH5LqTXGGCIiLTArK0XsPbQLZU+08ICTDjwE0Yc+0W0z7vUhr/vJNxyUD+fZXgHN0zr1bjMr5eovDHAEBGV83yX/ssPITY+TaW/WnoSQrYswNv3Lon2j8174ZvOfsg1NZcdh0ukydAwwBARleN8F/8NMVCU6Pe8dgwLt32LSjlPkG5ujaCeAdjRoL3acbzdXRDKkhEZGAYYIiIt2ZjOrDAfk/b9iOEnfhftM1XqilVGdytWkR3DztIEJ7/sBnNT6fkLkWFhgCEi0oKSUfXUBIRtCYbHgyuivfrtPpj37hDkm5jJjuNa0QIHJntp5JqJtBEDDBGRhmw9cx8BG2OhukAa6H7lMBZsXwq73EykWdggsNc4RNZtrXacLvUdsWao+teJDAEDDBGRBvitO4GouCSVPvOCfEzZuwZDT20V7Zhq9THaNwj37CvLjiFNcQn50AM+HtxVl4gBhoiojEtGngv34FZKtkp/jccPELZlPpomPN1td+U772Nhx89QYCL/bbmZqx1+4VlGRMXKZObXvXv38Mknn8DR0RFWVlZo0qQJTp48Wfy6UqnEjBkzULVqVfG6l5cXrl69qjJGSkoKPv74Y9jZ2aFixYrw8/NDRkZGWVwuEVGZrTKqM3X7c+Gl16UD2LYuQISXFCs7DO0/E/M6D1MbXrwaOuO3UR0YXojKMsA8fvwY7dq1g5mZGXbs2IGLFy9i0aJFqFSpUvF7goODERISgpUrV+LYsWOwsbFB9+7dkZOTU/weKbxcuHABkZGRiIiIwP79+/H555+X9uUSEZXZKqORG2JU5rtY5Ofi613LsGzLfFTIy8bx6o3gPSQEe2q3lB1DiithAz3ww+B3NHbdRLrCSCk9DilFkydPxqFDh3DgwAHZ16U/rlq1apgwYQICAwNFX1paGlxcXLBu3ToMHDgQly5dQqNGjXDixAm8/fbb4j07d+6Et7c34uPjxe//O+np6bC3txdjS09xiIjKc5XRm4/isSx8Hhom34ICRljeZgCWtP8YhcYmsuO4OVgiKrALn7qQwUl/wZ/fpf4EZsuWLSJ0DBgwAJUrV0azZs2watWq4tdv3ryJhIQEUTYqIl1oq1atcOTIEdGWPktlo6LwIpHeb2xsLJ7YyMnNzRVf9LMfRESaXmUklYxKhpc+F/Zg649jRXh5aG2PwR/MEvNd1IUXzwZO2DvJk+GFSJOTeG/cuIEVK1Zg/PjxmDp1qniKEhAQAHNzcwwePFiEF4n0xOVZUrvoNemzFH5ULtTUFA4ODsXvKWnu3LmYNWtWaX85REQvZNi644iOS1bps8zPwb92f4+BZ/8Q7SM1mmCMTyCSKjjKjiHFldCBXGVEVC4BRqFQiCcn33zzjWhLT2DOnz8v5rtIAaasTJkyRYSmItITGFdX1zL784iIikpGredEIjkzX6W/zsM7omRU/+EdUTIKaTcQIW0HQqHmqQtXGRGVc4CRVhZJ81ee1bBhQ/z666/i11WqPN0SOzExUby3iNT28PAofk9Skup+CQUFBWJlUtHvL8nCwkJ8EBFpsmQ0emPsc/39z+3GV5ErYJ2fiySbShjTOxBHar6ldhy/9jUx3ce9jK+WSL+U+hwYaQXS5cuXVfquXLmCmjVril/XqlVLhJCoqCiVpyXS3JY2bdqItvQ5NTUVp06dKn5PdHS0eLojzZUhItKGjelKhhfrvGws2rYYC7d/K8LLgZoe8B4aoja8SN+Alw9qxvBCpA1PYMaNG4e2bduKEtIHH3yA48eP4/vvvxcfEiMjI4wdOxZff/016tatKwLN9OnTxcqivn37Fj+x6dGjB4YPHy5KT/n5+fD39xcrlF5kBRIRkaY3pquffAvLfp+HOinxKDQyxuL2H4uVRkoj+X8nelS3w68jWTIi0ppl1BJp3xZpToq0OZ0UUKS5KVIYKSL9kTNnzhShRnrS0r59eyxfvhz16tUrfo9ULpJCy9atW8Xqo379+om9Y2xtbV/oGriMmog0UjJSKvHh2T8wa/d3sCzIQ4KtAwJ8J+G4q/qnKtIqo9VD+DSZ6HV+fpdJgNEGDDBEVJpPXQasOIyYu6kq/Ta5Wfhm1zL0ubRPtPfWaoHxPuORYm2vdqzhHdwwrVfjMr9mIn3/+c2zkIiI/uY4AP8NMVCU6G+UeANh4fPw5uP7KDAyxoJOn+H7d95XWzKq62yNbWM6wdy0TE5wITI4DDBERH9xHMDqgzdVO5VKfBK7HdOjf4BFYT7uVXDGaN9JiKneUO040nEA3NuFqHQxwBARveBxABVyMzFvRwh6XT4k2pF13sFE77FItZJ/zO1sY4aj07pyoi5RGWCAISIqMVE3YGOsyiGMkiYPriJsy3zUTE1AvrEJ5r07FKvf7iMtrZQdp0t9R6wZ2loj10xkiBhgiIj+3z9+PIHdl5KeKxkNPbUFU/ashbmiAHftXeDvOwlnqtWXHYPHARBpBgMMERk8dSUju5wMLNj+LbpfPSraO+q1RVDPAKRbym/nwJIRkeYwwBCRQVNXMvK4fxlh4fNRPT0JuSammNPZD/9u7qO2ZORezRYRAZ00cs1ExABDRAZ+HEBUnGrJyEipgN+J3xG070eYKQpxq2JV+PcJwvkqddSOE/LBW/BtXl0DV0xERRhgiMgg+YTsx/n7T1T6KmanY9G2JfC8fkK0Ixp0wOQeo5FhYS07hpuDJaICu7BkRFQOGGCIyKDkFSjQYd5uJGbkq/S3iL+I0C3BqPbkIXJNzDDL63NseKuH2pIRjwMgKl8MMERk0BvTSSWjEUd/wfgDP8FUqcB1hzdEyehS5Tdlx+AqIyLtwABDRAahd+gBnLuXrtLnmJmKxdsWo9PNGNH+rdG7+LLbSGSqKRk1c7XDLyN4gjSRNmCAISK9XyLdZUE0bj/OUelvdeccQrYugEtGCrJNLTCj6z+xuUlXtSWjoe1qYmZv9SdME5FmMcAQkV4vkR69MValz1hRCP8jP2PMoY0wUSpw1dEVo/oE4Yqzm9pxeII0kfZhgCEi/dyYbsUhxN5V3ZjOOeMxlkQsRPvbZ0T75yZemOn1BbLNLWXHkQ6ODhnYDN5Nq2nkuonoxTHAEJHeP3WRtL11GksjFsI5MxVZZhaY1m0UfnPvonac0e/Wxthu9TnfhUhLMcAQkd48dRmw4jBi7qaq9JsoChFwaCNGH/4vjKHEJWc3scrouqOr2rGWD+JTFyJtxwBDRDpv+9kH8N8QA0WJ/spPHiF06wK0untetDe81R2zPD9HrpmF7DgmRsCyj5ujh3tVDVw1Eb0OBhgi0ru9XSSdbpzC4ohFcMxOR4a5FaZ298eWRurPKurh7oJlg1qwZESkIxhgiEivTpCWSkYTDvwHI4/+ItoXKr8pVhndclC/8VwYN6Yj0jkMMESkNyWjqunJCNmyAC3vXRTtfzfrhTld/JBrai47DktGRLqLAYaIdMqcbRex6sDzJaMu146Lgxgr5TxBurk1gnoGYEeD9mrHae5qh83cVZdIZzHAEJHOlIz815/CjguJKv1mhfmYtO9HDD/xu2ifqVJXrDK6W7GK2rFYMiLSfQwwRKQTJaOAjTEoUKr2V09LRGh4MJo9uCzaa1r4Yt67Q5FnaiY7jpujJaImdOFTFyI9wABDRDq5yqjblSNYsP1b2OdmIs3CBhO9x+KPem3UjhPywVvwbV69jK+WiDSFAYaItNawtccRfTlZpc+8IB9T9q7B0FNbRTumWn2M9g3CPfvKsmN4VLfDryM514VI3zDAEJFWznfxXLgHt1KyVfprPH6AsC3z0TThmmh/9877WNDxMxSYyH8r82zghNVDWmnkmolIsxhgiEgnzjLyjjuIeTtCYJeXhRQrO0zoNQ57ardUOw5PkCbSbwwwRKQ1hq07jug41ZKRRUEevoz+AZ/GbhftE280wmjfSUiwc5Ido7aTNXaM7QRz6ShpItJbDDBEpBUlo9ZzIpGcma/SXyvlHpaFz0OjpKeTeJe1HoDFHT5BobGJ7Dhd6jthzVCWjIgMAQMMEWllycj34l58s2sZbPOy8dDaHuN7jcf+N1uoHceroTN+GPxOGV8tEWkLBhgi0qqSkWV+Dmbu/h4fnf1DtI/UaIIxPoFIquAoO4a0uCjkQ25MR2RoGGCISGtKRrUf3hUlowYPb0MBI4S2HYil7QZCoaZkxBOkiQwXAwwRaUXJqN+5KMyOXA7r/Fwk21QUT10Ou3moHYerjIgMGwMMEWnsqcuAFYcRczdVpd8qLwezI1eg//ko0T5Q0wPjek/AQ5tKsuNIa4vCBjWDd9NqGrluItJODDBEpJGzjPw3xEBRor9e8i0s/30e6qTEo9DIGEvaD8Ly1gPUlox4gjQRFWGAISLNn2WkVOLDs39g1u7vYFmQhwRbB4zpPRHHajRROw5PkCaiZzHAEFGZlYz6Lz+E2Pg0lX6b3CzM+WMZ+l7cJ9p7a7XAeJ/xSLG2lx3H2cYMR6d15VMXIlLBAENEGisZNUq8gdAt81E75R4KjIyxsONn+K7V+1Aaye+a26W+I9YMba2RayYi3cIAQ0Slas62i1h14PmS0Send2B61CpYFObjfgUncRzAqeqN1I7DkhER/RUGGCIqtZKR//pT2HEhUaW/Qm4m5u4Mg0/cAdHeXbslAnuNQ6qVnew4LBkR0YtggCGiUtnbZcymWCiUqv1NHlxF2Jb5qJmagHxjE8zrNASrW/YFjOTDCUtGRPSiGGCI6LX4rTuBqLgk1U6lEkNObcXUPWtgrihAvF1l+PcJwulq9WXHkOJMKEtGRPQSGGCI6JX5hOzH+ftPVPrscjIQvGMpelw5Ito767XBpJ5jkG5pKzuGm4MlogK7sGRERC+FAYaIXlpegQId5u1GYobqWUYe9y8jLHw+qqcnIdfEFN909sOPzX3Ulow8Gzhh9ZBWGrpqItInDDBEVCob0/3jxG8I2vcjzBSFuF2xCkb1mYzzVerIjsGSERG9LgYYInphvUMP4Ny9dJW+itnpWLhtCbyunxDtiAYdMKWHP55Y2MiOwZIREZUGBhgieqEl0l0WROP24xyV/hbxFxGyZQHeeJKMXBMzfOU5HOs9ev7FKiMnrBnKkhERvT4GGCL6S+Gn72HMptMqfUZKBb449ism7P8PTJUK3KhUDf59JuOiy5tqx/FrXxPTfdw1cMVEZAgYYIjopUpGDllpWByxGO/ePCXavzfqhGndRiHTwlp2DFNjIGRgM3g3raaRayYiw8AAQ0SyJaPWcyKRnKm6yqjVnXNYunUBqmSkIMfUHDO8vsDPTbuqLRl5u7sgdFALzncholLHAENEz+2qO3pjrEqfsaIQo478jLGHNsJEqcBVR1eM6hOEK85usmNIRzOGDeJTFyIqOwwwRFRs2LrjiI5LVulzzniMJREL0f72GdH+xd0T07uOQLa5pewYbo6WiJrAVUZEVLYYYIhIbEzX8us/kJZTqNLf9tZpLI1YCOfMVGSZWWB615H4tYmn2nGGtquJmb05UZeIyh4DDJGBk9uYTioZjTm0CaMPb4IxlIhzqik2prvu5Kp2nDBuTEdEGsQAQ2TAE3W9Fu3FzUdZKv2VnzxCyNYFaH33vGhveKs7Znl+jlwzC9lxTIyAZR83Rw/3qhq5biKiorl2ZWrevHkwMjLC2LFji/tycnIwatQoODo6wtbWFv369UNiYqLK77tz5w569eoFa2trVK5cGRMnTkRBQQH/XyMqBdvPPkCdqdufCy8db5zC9nUBIrxkmFshoHcgpvYYrTa8NHe1w5U53gwvRKRfT2BOnDiB7777Dk2bNlXpHzduHLZt24bNmzfD3t4e/v7+eP/993Ho0CHxemFhoQgvVapUweHDh/HgwQN89tlnMDMzwzfffFOWl0xkkCUjE0UhJhz4D0Ye/UW0L1auJUpGNx3Ul4RYMiKi8mSkVCqVZTFwRkYGmjdvjuXLl+Prr7+Gh4cHvv32W6SlpcHZ2RkbNmxA//79xXvj4uLQsGFDHDlyBK1bt8aOHTvg4+OD+/fvw8XFRbxn5cqVCAoKQnJyMszNzf/2z09PTxfhSPrz7OzsyuJLJNK5klH/5YcQG5+m0l81PVkcB9Dy3kXR/nezXpjTxQ+5pvJ/z9wqWSJqIlcZEVHZeNGf32VWQpJKRNJTFC8vL5X+U6dOIT8/X6W/QYMGqFGjhggwEulzkyZNisOLpHv37uKLunDhguyfl5ubK15/9oOI/iwZ1Z26/bnw0vn6CWxfGyDCS7q5NUb2mYwZ3UaoDS+eDZywN8iT4YWI9LOEtGnTJsTExIgSUkkJCQniCUrFihVV+qWwIr1W9J5nw0vR60WvyZk7dy5mzZpVil8Fkf6WjEwLCzBp34/4/MRvon22Sh34+wbhTiX1c1lYMiIivQ4wd+/exZgxYxAZGQlLS/mNrsrClClTMH78+OK29ATG1VX9kk8iQy0ZVU9LRGh4MJo9uCzaa1v0xtx3hyHP1Ex2HGdbMxyd2pVPXYhIvwOMVCJKSkoS81+KSJNy9+/fj7CwMOzatQt5eXlITU1VeQojrUKSJu1KpM/Hjx9XGbdolVLRe0qysLAQH0T09DiAgI2xKDnBrduVI1iw/VvY52YizcIGE73H4o96bdSOM7RdDczs3aTMr5eIqNwDjKenJ86dO6fSN3ToUDHPRZqEKz0VkVYTRUVFieXTksuXL4tl023aPP1GKn2eM2eOCELSEmqJ9ERHmszTqFGj0r5kIr3it+4EouKSVPrMC/Ixee9aDDu1RbRjq9bH6D6TEG+vWqot4lLBDAeCvGAuHSVNRGQIAaZChQpwd1fdStzGxkbs+VLU7+fnJ8o9Dg4OIpSMHj1ahBZpBZKkW7duIqh8+umnCA4OFvNevvzySzExmE9ZiNSXjDwX7sGtlGyVftfUBCwLn4emCddE+/uW72FBp8+QbyJfMnKvaouIMZ00cs1ERDq1E++SJUtgbGwsnsBIq4ekFUbScusiJiYmiIiIwIgRI0SwkQLQ4MGD8dVXX5XH5RLp5AnSkp5xBzF/Rwjs8rLw2LICJvQah+g676gdR1pltHpIqzK+WiIiLd4HprxxHxgy5BOkLQryMC16NT6L3SbaJ95ohADfiXhg5yw7hjQ9N5SrjIhIh35+8ywkIh0uGbWeE4nkzHyVfreUe1gWPh+Nk26I9rLWA7Ck/ccoMJH/6+7mYImoQG5MR0S6hQGGSI9KRr4X9+GbXWGwzcvGIys7jPOZgP1vtlA7DktGRKSrGGCI9KFklJ+LmVHfY9CZXaJ91NUdAb0nIqmCo+wYLBkRka5jgCHS8ZJR7Yd3xSqjBg9vQwEjhLb9ECHtPkKhsYnsOM1c7fDLiPYsGRGRTmOAIdLhktH756Pw9R/LYZ2fi2SbihjrE4hDbh5qx/FrXxPTfVS3OSAi0kUMMERa/tRlwIrDiLmbqtJvlZeDryJXYsD53aJ9sOZbGOcTiGTbSrLjSNvRhQ1qBu+m1TRy3UREZY0BhkiLT5D23xADRYn+esm3xCqjuo/uotDIGN+2+wjL2nwAhZqSUXNXO2xmyYiI9AwDDJGOnCANpRIfnI3ErN3fwaogFwm2DhjTeyKO1VB/VhFPkCYifcUAQ6QDJ0jb5GaJuS7vXdwr2vtqNRdLpFOs7WXHcbYxw9FpPEGaiPQXAwyRlpeMGibdQFj4fNROuYcCI2Ms6vgpVrbqB6WR/EGLXeo7Ys3Qp+eKERHpKwYYIi0uGX18egdmRK2CRWE+7ldwwmjfSThVXf2J7CwZEZGhYIAhKmfD1h5H9GXVjelsc7Mwb2cofOIOiPbu2i0R2GscUq3kzwVhyYiIDA0DDFE5yStQoOP8KCQ8yVPpd0+4JkpGbqkPkG9sgvmdBuOHlu8BRvLhhCUjIjJEDDBE5WDOtotYdeD5ktHgmAhM3bMaFoUFiLerLEpGsW80kB2DxwEQkSFjgCHSgo3p7HIyELxjKXpcOSLau+q2xkTvsUi3tJUdhydIE5GhY4Ah0uBxAAEbY6Es0f/W/csI2xIM17RE5Bmb4pvOw7CuRW+1JSOeIE1ExABDVG4nSEslI78Tv2PyvnUwUxTidsUq8PcNwrmqdWXHYMmIiOhPDDBE5XCCtH32EyzcvgRdrx0X7Yj67TGl52g8sbCRHYclIyIiVQwwRGW4Md3IDTHP9TePv4TQLcF440kyck3MMNtzOH7y6MmSERHRS2CAIdLQxnRGSgX+eex/CNz/b5gqFbhRqRr8+0zGRZc3ZcdgyYiISD0GGCINnGXkkJWGRdsWo/ONU6L9e6NOmNZtFDItrGXHYcmIiOivMcAQlfFZRu/cPY+QLcGokpGCHFNzzPT6J/7btNtfbEznhDVDWTIiIvorDDBEpfDUZfSGGGw/n6DSb6woxMijmzHu4AaYKBW45lAdo/pOxmVnN7Vj+bWviek+7hq4aiIi3cYAQ/SaT10CNsagoMTmLk6Zj7Fk6yJ0uH1atH9x98T0riOQbW4pO46pMRAysBm8m1bTxGUTEek8Bhii0jwOAECb22cQsnUBnDNTkWVmgeldR+LXJp5qxwnoXBtjutbnfBciopfAAEP0CiUj//WnsONC4nMlozGHNmH04U0whhJxTjXh3ycI15xqyI7DE6SJiF4dAwxRKZSMKj95hKURC9HmzjnR3ti0G2Z5fY4cM/mSUed6jlg7jCdIExG9KgYYotfY20XS4WYMlkQsglNWGjLMrTC1+yhsafSu2nGGtquJmb05UZeI6HUwwBC94t4uJopCjD/wE0Yd3SzaFyvXwqg+k3HTQf3Gc8M7uGFar8Zlfs1ERPqOAYboFfZ2qZL+ECFbg/FO/EXR/k8zb3zd5R/INTWXHcfECAj9iKuMiIhKCwMM0UuuMnr3+gks3rYEDtnpeGJuhck9ArCtYQe143i7uyB0UAtO1iUiKkUMMEQvuMrItLBAnGP0xfH/ifbZKnXg7xuEO5Wqyo4j5ZUwPnUhIioTDDBEL7DK6I20JIRumY/m9y+L9toWvTH33WHIMzWTHaeZqx1+GdGeT12IiMoIAwzR36wy6nr1KBZuWwL73EykWdhgkvcY7KrXVu04PA6AiKjsMcAQARi29jiiLyer9JkV5mPKnrUYdmqLaJ+uWk9sTBdv7yI7Bo8DICLSHAYYgqHPd/FcuAe3UrJV+l1TExAWPh9vJVwV7e9bvocFnT5Dvol8yai5qx02s2RERKQxDDBksLaeuY/RG2Of6+8ZdxDzd4TALi8Ljy0rILDXWETVaaV2HO7tQkSkeQwwZJCGrTuO6DjVkpFFQR6mRa/GZ7HbRPvkGw0x2ncSHtg5y45R28kaO8Z2grlUOyIiIo1igCGDKxm1nhOJ5Mx8lX63lHsI2xIM98Tror28dX8sbv8JCkzk/4p0qe+ENUPVP5UhIqKyxQBDMPSSUe+L+zB3Vxhs87LxyMoO430mYN+bLdSO49XQGT8MfqeMr5aIiP4KAwwZbskoPxczo1Zh0Jmdon3M1R0BvQORWMFJdgxpfm7Ihx7w8VB/1hEREWkGAwwZZMmo9qO7YpVRw+RbUMAIYW0+wNL2g1BobCI7Tg93FyzjcQBERFqDAYYMrmT03vlofP3Hctjk5yDZpiLG+gTikJuH2nG4yoiISPswwJBePnUZsOIwYu6mqvRb5eXgq8iVGHB+t2gfqtkUY30mItm2kuw40tqisEHcmI6ISBsxwJDenWXkvyEGihL9dZNvY1n4fNR7dAeFRsZY2u4jUTZSqCkZcWM6IiLtxgBD+n2WkVKJAeci8VXkd7AqyEWirQPG9A7E0RpN1Y4TNpATdYmItB0DDOlFyaj/8kOIjU9T6bfOy8acXcvw3sW9or3frRnG+UzAI5uKsuM425jh6LSufOpCRKQDGGBI5yfqBmyMhbJEf8OkG2KVUe2UeygwMsaijp9iZat+UBrJ75rbpb4j1gxtrZFrJiKi18cAQzrLb90JRMUlqXYqlWJfl5m7v4dFYT7uV3BCgO9EnKyufhURS0ZERLqHAYZ0kk/Ifpy//0SlzzY3C3N3hqJ33AHRjqrdEoHeY/HY2l52DJaMiIh0FwMM6ZS8AgU6zNuNxAzVjekaJ1wTq4zcUh8g39gEwR0H44d3+rJkRESkpxhgSOdXGX0WE4Fpe1bDorAA8XaVxQnSsW80kB1DetYSypIREZHOY4AhnVhl5LVoL24+ylLpt8vJwPwdIeh55bBo/1G3tSgZpVvayo7j5mCJqMAuLBkREekBBhjSyeMA3rp/GWFbguGalog8Y1PM7TwUa1v4Akby4cSzgRNWD2mlgSsmIiJNYIAhnVtl5HcyHEF718FcUYDbFavA3zcI56rWlR2DJSMiIv3EAENaWTLyXLgHt1KyVfrts59g4fYl6HrtuGhvq98Ok3sG4ImFjew4zVzt8AuPAyAi0kvySzRew9y5c9GyZUtUqFABlStXRt++fXH58mWV9+Tk5GDUqFFwdHSEra0t+vXrh8TERJX33LlzB7169YK1tbUYZ+LEiSgoKCjtyyUtPMuoztTtz4WX5vGXsH1tgAgvuSZm+LLbSIzqM1ltePFrXxO/jerA8EJEpKdKPcDs27dPhJOjR48iMjIS+fn56NatGzIzM4vfM27cOGzduhWbN28W779//z7ef//94tcLCwtFeMnLy8Phw4fx448/Yt26dZgxY0ZpXy5p2SqjkRtiVHbVNVIq8M9jv+DnDUF440kyblSqhvc+XYSfmnnLzneR/oNePqgZpvu4a/TaiYhIs4yUSmXJXdhLVXJysniCIgWVjh07Ii0tDc7OztiwYQP69+8v3hMXF4eGDRviyJEjaN26NXbs2AEfHx8RbFxcXMR7Vq5ciaCgIDGeubn53/656enpsLe3F3+enZ1dWX6JVEZnGTlkpWHRtsXofOOUaIc37ISp3Uch08JadhyuMiIi0n0v+vO71J/AlCRdgMTBwUF8PnXqlHgq4+XlVfyeBg0aoEaNGiLASKTPTZo0KQ4vku7du4sv6sKFC7J/Tm5urnj92Q/SjVVGUsmoZHhpefc8tq8dLcJLjqk5Jnf3F6dIqwsv0iqjvZM8GV6IiAxEmU7iVSgUGDt2LNq1awd396eP9BMSEsQTlIoVVU8ElsKK9FrRe54NL0WvF72mbu7NrFmzyugrobJ46jJgxWHE3E1V6ZdKRiOPbMb4g+tholTgmkN1jOo7GZed3WTH4SojIiLDVKYBRpoLc/78eRw8eBBlbcqUKRg/fnxxW3oC4+rqWuZ/Lr28necfYNT6GBSWKF46ZT7G4ojF6Hjr6b4vv7p3wfSuI5BlbiU7TnNXO2zmKiMiIoNUZgHG398fERER2L9/P6pXr17cX6VKFTE5NzU1VeUpjLQKSXqt6D3Hjz9dKvvs60WvybGwsBAfpP2rjKSJuiW1uX0GS7cuROXMx8gys8CMriPwS5M/y4wlhXzgAd/mfOpCRGSoSn0OjDQnWAovv/32G6Kjo1GrVi2V11u0aAEzMzNERUUV90nLrKVl023atBFt6fO5c+eQlPTnJmbSiiZpMk+jRo1K+5JJQ8Jj7z0XXowVhRh7cD3Wb/pShJfLTjXg+9mSvwwv0iojhhciIsNmWhZlI2mFUXh4uNgLpmjOijSj2MrKSnz28/MT5R5pYq8USkaPHi1Ci7QCSSItu5aCyqefforg4GAxxpdffinG5lMW/Znv4pyRgpCtC9DmzjnR3tS0G/7l9TlyzCxlxzEzBkIHNUcP96oauW4iIjKgZdRGas6iWbt2LYYMGVK8kd2ECROwceNGsXpIWmG0fPlylfLQ7du3MWLECOzduxc2NjYYPHgw5s2bB1PTF8tcXEatPauMAjbGquztIulwMwZLIhbBKSsNmWaWYnl0eOPOasfp4e6CZYNacL4LEZGeS3/Bn99lvg9MeWGA0c6zjEwUhRh3cL1YaWQMJS5WriV21L3poL4kNLyDG6b1aqyBKyYiIl35+c2zkKjU5RUo0HF+FBKe5Kn0V0l/iKVbF6BV/NO9fH7y6InZXf6BXDP5sqCJERD6UTN4N62mkesmIiLdwQBDpX4cwOqDN5/rf/f6CSzetgQO2el4Ym6FyT0CsK1hB7XjBHSujTFd67NkREREshhgqNT0Dj2Ac/dUd0A2LSxA4P5/44vj/xPtcy614d8nCLcryT9V4XEARET0IhhgqFRKRh3m7UZiRr5Kf7X0JISGB6PF/TjRXtuiN+a+Owx5pmay47hXtUXEmE4auWYiItJtDDD0Wr7aegFrDt16rt/r6jEs3L4EFXMykG5hg4k9x2BX/bZqx5HOMlo9pFUZXy0REekLBhh6ZR2Do3EnJVulz6wwH5P3roPfyXDRPl21Lvx9gxBfUX4HZZ5lREREr4IBhl5pY7pWX/+Bh1kFKv3VUxMQtmU+PB5cFe1VLfsiuNNg5JvIl4zcHC0RNYHzXYiI6OUxwNBLb0w3euPTwxaf1ePyIQTvCIFdbiZSLW0xodc4RNVRXxLya18T032enlBORET0shhg6IWfuvRfcQixd9NU+i0K8jB1z2oMjtkm2iffaIgA34m4b1dZdhyXCmY4EOQFc9NSP4aLiIgMCAMMvdAJ0qM2xDx3HEDNx/exLHw+3BOvi/aKVv2xqMMnKDCR/8+KE3WJiKi0MMDQK21M1/viPnyzKwwV8rLxyMoOE3qNx97ab8uOwYm6RERU2hhgSK2ha49hz+WHKn0W+bmYGbUKg87sFO1j1RuLklFiBSfZMThRl4iIygIDDMnOd2k9JxLJmaob0735KB7LwuehYfItKGCEsDYfYGn7QSg0NpEdhxN1iYiorDDA0AutMup7YQ/m7FoGm/wcJFtXxDifCThYq5nsGHaWJjj5ZTdO1CUiojLDAEPFT10GrDiMmLupKv2W+Tn4KnIlPji3W7QP12iKMb0DkWzrIDtO4yo22Db2XY1cMxERGS4GGBKrjPw3xEBRor9u8m2xyqjeozsoNDLG0nYfibKRQk3JiKuMiIhIUxhgDJzsKiOlEgPO7RZPXqwKcpFo6yCeuhyt0VTtOGFcZURERBrEAGPIG9MtP4TYeNWN6azzsvH1H8vx/oU9or3frZmY7/LIpqLsOG4OlogK5CojIiLSLAYYA52oG7Ax9rmN6Rok3RQlo9op8SgwMsbiDp9gRev+UBrJT8Yd2q4GZvZuopFrJiIiehYDjIHxW3cCUXFJqp1KJT46swv/2v0dLArz8cDWEaP7TMLJ6o1lx5CetSwb1AzeTatp5qKJiIhKYIAxoJKR58I9uJWSrdJvm5sldtT1vbRftKPffFscxPjY2l52HGcbMxyd1pUlIyIiKlcMMAa8t0vjxOsIC5+HWo8fIN/YBMEdB+OHd/qqLRl1qe+INUNba+CKiYiI/hoDjIGWjD6N3YYvo3+ARWEB4u2cEeA7CTFvNJQdg2cZERGRtmGAMbCSkV1OBubtCIH3lcOiHVmnFQK9xyLNqoLsOFxlRERE2ogBxoBKRk0fXEFY+HzUSEtEnrEp5nYeirUtfAEj+XDCjemIiEhbMcDomWHrjiM6Llm1U6nEsJNbMHnvWpgrCnDH3gX+fYJwtmo92TFYMiIiIm3HAKPnJ0jbZz/Bwu3fouu1Y6K9vV5bTO4ZgHRLW9lxmrna4ZcR7VkyIiIircYAo8clo+b3LiFkSzCqpycj18QUs7sMx0/NvNWWjPza18R0H3cNXDEREdHrYYDRw5KRkVKB4cd/w8T9/4aZohA3K1WFf5/JuOBSW3YMadF0GDemIyIiHcIAo2clo0pZaVi0bQm63Dgp2lsadsTU7v7IsLCWHYclIyIi0kUMMHpUMmp59zxCtixA1YxHyDE1x788P8emt7qzZERERHqHAUbHnroMWHEYMXdTnysZjTj6C8Yf+AmmSgWuO1THqD5BiKtcS3YcloyIiEjXMcDoiO1nH8B/QwwUJfodM1OxJGIROt56+kTm18adMb3bSGSZW8mO09zVDptZMiIiIh3HAKMDZkdcxOqDN5/rb3P7LJZuXYDKmY+RbWqBGV2/wOYmXmpLRmHc24WIiPQEA4yWG7b2OKIvq64yMlYUYvTh/yLg8CaYKBW44lgDI/tOxjWnGrJj8ARpIiLSNwwwWiqvQIGO86OQ8CRPpd85IwXfRixEu9tnRfu/TbpiZtd/IsfMUnYcniBNRET6iAFGC83ZdhGrDjxfMmp/M1bMd3HOSkWmmSWmdR+F3xt3VjsOS0ZERKSvGGC0jN+6E4iKS1LpM1EUYuzBDRh15GcYQ4lLzm4Y1WcybjhWlx2DJSMiItJ3DDBatES637KDOH0vXaW/SvpDMVG3VfwF0V7v0QNfdRmOXDML2XFYMiIiIkPAAKMlG9MFbIyFskT/u9dPYvG2xXDITscTcytM6TEaEQ07yo7BE6SJiMiQMMBo4cZ0poUFCDzwH3xx7FfRPudSG/59gnC7kvzGc24OlogK7MKSERERGQwGGC176lItPQmh4cFocT9OtNc198E3nf2QZ2omO45nAyesHtJKA1dMRESkPRhgtGSirsTr6jEs3L4EFXMykG5hg0k9A7CzfjvZMVgyIiIiQ8YAo2E+Iftx/v4TlT6zwnwE7V2Hf5wMF+3TVevC3zcI8RWryI7BkhERERk6BhgNbkzXYd5uJGbkq/RXT01A2JZgeDy4Ito/vN0H898dgnwT+ZJRl/pOWDOUJSMiIjJsDDAamKg7ekMMtp9PeO617pcPY8GOpbDLzUSqpS0Cvcdhd1314cSvfU1M93Ev4ysmIiLSfgwwZWjneekE6VgUKFSn6poX5GPqntUYEhMh2qeqNUCA7yTcs68sO45UKQr7qBm8m8qvQiIiIjI0DDBluMpo9MbY5/prPr6PsPD5aJJ4XbRXtuqHhR0+RYGJ/P8Vbo6WiJrA+S5ERETPYoApg5KR//pT2HEh8bnXfC7tx9ydoaiQl40UKzuM7zUOe2u3VDsWS0ZERETyGGBK0fazDzB6YwwKS2zuYpGfixnRq/Dx6Z2ifax6Y4zpPREJdk6y49R1tsa2MZ1gbmqsicsmIiLSOQwwZXyC9JuP4rEsfB4aJt+CAkZY1uYDfNt+EAqNTWTHCfnAA77NubcLERHRX2GAKQWztl7A2kO3nuvve2EP5uxaBpv8HCRbV8Q4nwk4WKuZ2nGWD+JEXSIiohfBAPOa8136Lz+E2Pg0lX7L/BzMivwOH56LFO3DNZpiTO9AJNs6yI5jYgQs+7g5erhX1ch1ExER6ToGmNeY7+K/IQaKEv11Ht4RJaP6D++IktHSdh8htO2HUKgpGXm7uyB0UAuuMiIiInoJDDCvYHbERaw++Px8l/7ndmP2HytgVZCLJJtKYqLukZpNZcdwqWCGA0FenKhLRET0ChhgSqFkZJ2XjdmRK9DvfLRo73drhvE+4/HQppLsOO7VbBER0Ekj10xERKSPtPqf/8uWLYObmxssLS3RqlUrHD9+vFx31a03bftz4aVB0k1s+XGcCC+FRsYI7vgZBn8wS214kfZ2YXghIiLS0ycw//3vfzF+/HisXLlShJdvv/0W3bt3x+XLl1G5svyW+2UZXr74KUa1U6nER2d2YWbU97AsyMMDW0cE+E7ECVf5jedYMiIiIio9RkqlssS2a9pBCi0tW7ZEWFiYaCsUCri6umL06NGYPHny3/7+9PR02NvbIy0tDXZ2dq9VNmo7dzcSn+QV99nmZuGbXWHwvbRftPe82QLje43HY2t72TF4gjQREdGLedGf31r5BCYvLw+nTp3ClClTivuMjY3h5eWFI0eOyP6e3Nxc8fHsDSgNx2+mqISXxonXERY+D7UeP0CBVDLqNBir3nkPSiP5Jys8DoCIiKj0aWWAefjwIQoLC+Hi4qLSL7Xj4uJkf8/cuXMxa9asUr+WpCc5xb82UiqwcNsSEV7i7ZzFCdIxbzSU/X08QZqIiKjs6M2EDOlpjfS4qejj7t27pTJu5QqWxb+WnrJIu+lG1G+PXkNC1IYXNwdLXJ3jzfBCRERkSE9gnJycYGJigsRE1ROdpXaVKlVkf4+FhYX4KG3v1HKASwXz4jJSXOVa8O+rfg6OZwMnrB7C+S5EREQG9wTG3NwcLVq0QFRUVHGfNIlXardp00aj1yLtkDurz9/PYZH20Q0b6MHwQkREZKgBRiItoV61ahV+/PFHXLp0CSNGjEBmZiaGDh2q8WuRziha+UlzWJurPw7g2jfe8PHgKdJEREQGW0KSfPjhh0hOTsaMGTOQkJAADw8P7Ny587mJvZoMMV0bVcHhqw/xa2w8svIK0dLNAYPbunFvFyIiIg3T2n1gXldp7QNDRERE2vfzm48OiIiISOcwwBAREZHOYYAhIiIincMAQ0RERDqHAYaIiIh0DgMMERER6RwGGCIiItI5DDBERESkcxhgiIiISOdo7VECr6tog2FpRz8iIiLSDUU/t//uoAC9DTBPnjwRn11dXcv7UoiIiOgVfo5LRwoY3FlICoUC9+/fR4UKFWBkZFSqyVAKRXfv3uUZS2WM91ozeJ81g/dZM3ifdf9eS7FECi/VqlWDsbGx4T2Bkb7o6tWrl9n40v9Z/MuhGbzXmsH7rBm8z5rB+6zb9/qvnrwU4SReIiIi0jkMMERERKRzGGBekoWFBWbOnCk+U9nivdYM3mfN4H3WDN5nw7nXejuJl4iIiPQXn8AQERGRzmGAISIiIp3DAENEREQ6hwGGiIiIdA4DzEtatmwZ3NzcYGlpiVatWuH48ePlfUk6Ze7cuWjZsqXYIbly5cro27cvLl++rPKenJwcjBo1Co6OjrC1tUW/fv2QmJio8p47d+6gV69esLa2FuNMnDgRBQUFGv5qdMO8efPEbtRjx44t7uM9Lj337t3DJ598Iu6llZUVmjRpgpMnTxa/Lq2TmDFjBqpWrSpe9/LywtWrV1XGSElJwccffyw2A6tYsSL8/PyQkZFRDl+NdiosLMT06dNRq1YtcQ9r166N2bNnq5yVw/v8avbv34/evXuLXW+l7xO///67yuuldV/Pnj2LDh06iJ+d0u69wcHBr3jFqhdHL2jTpk1Kc3Nz5Zo1a5QXLlxQDh8+XFmxYkVlYmJieV+azujevbty7dq1yvPnzytPnz6t9Pb2VtaoUUOZkZFR/J4vvvhC6erqqoyKilKePHlS2bp1a2Xbtm2LXy8oKFC6u7srvby8lLGxscrt27crnZyclFOmTCmnr0p7HT9+XOnm5qZs2rSpcsyYMcX9vMelIyUlRVmzZk3lkCFDlMeOHVPeuHFDuWvXLuW1a9eK3zNv3jylvb298vfff1eeOXNG6evrq6xVq5YyOzu7+D09evRQvvXWW8qjR48qDxw4oKxTp47yo48+KqevSvvMmTNH6ejoqIyIiFDevHlTuXnzZqWtra1y6dKlxe/hfX410t/tadOmKf/3v/9JaVD522+/qbxeGvc1LS1N6eLiovz444/F9/6NGzcqrayslN99953ydTDAvIR33nlHOWrUqOJ2YWGhslq1asq5c+eW63XpsqSkJPGXZt++faKdmpqqNDMzE9+gily6dEm858iRI8V/4YyNjZUJCQnF71mxYoXSzs5OmZubWw5fhXZ68uSJsm7dusrIyEhlp06digMM73HpCQoKUrZv317t6wqFQlmlShXlggULivuk+29hYSG+iUsuXrwo7v2JEyeK37Njxw6lkZGR8t69e2X8FeiGXr16KYcNG6bS9/7774sfiBLe59JRMsCU1n1dvny5slKlSirfO6S/O/Xr13+t62UJ6QXl5eXh1KlT4vHZs+ctSe0jR46U67XpsrS0NPHZwcFBfJbucX5+vsp9btCgAWrUqFF8n6XP0mN6FxeX4vd0795dHCx24cIFjX8N2koqEUkloGfvpYT3uPRs2bIFb7/9NgYMGCDKbM2aNcOqVauKX7958yYSEhJU7rV0xotUfn72XkuP3aVxikjvl76/HDt2TMNfkXZq27YtoqKicOXKFdE+c+YMDh48iJ49e4o273PZKK37Kr2nY8eOMDc3V/l+Ik0fePz48Stfn94e5ljaHj58KOqwz35Dl0jtuLi4crsuXT8xXJqX0a5dO7i7u4s+6S+L9B+59Bei5H2WXit6j9z/D0WvEbBp0ybExMTgxIkTz73Ge1x6bty4gRUrVmD8+PGYOnWquN8BAQHi/g4ePLj4Xsndy2fvtRR+nmVqaipCPe/1U5MnTxbhWQraJiYm4nvxnDlzxLwLCe9z2Sit+yp9luYvlRyj6LVKlSq90vUxwFC5PiE4f/68+JcUlR7paPsxY8YgMjJSTJijsg3h0r88v/nmG9GWnsBI/02vXLlSBBgqHT///DPWr1+PDRs2oHHjxjh9+rT4x4808ZT32XCxhPSCnJycRPIvuVJDalepUqXcrktX+fv7IyIiAnv27EH16tWL+6V7KZXrUlNT1d5n6bPc/w9Frxk6qUSUlJSE5s2bi38JSR/79u1DSEiI+LX0Lx/e49Ihrcxo1KiRSl/Dhg3FCq5n79Vffd+QPkv/fz1LWu0lrezgvX5KWgEnPYUZOHCgKG1++umnGDdunFjVKOF9LhuldV/L6vsJA8wLkh4Jt2jRQtRhn/3Xl9Ru06ZNuV6bLpHmiUnh5bfffkN0dPRzjxWle2xmZqZyn6U6qfQDoeg+S5/PnTun8pdGetogLeEr+cPEEHl6eor7I/0rtehDekogPW4v+jXvcemQyp8ltwGQ5mnUrFlT/Fr671v6Bv3svZZKIdLcgGfvtRQmpeBZRPq7IX1/keYaEJCVlSXmVDxL+geldI8kvM9lo7Tuq/Qeabm2NPfu2e8n9evXf+XykfBaU4ANcBm1NPt63bp1Yub1559/LpZRP7tSg/7aiBEjxJK8vXv3Kh88eFD8kZWVpbLEV1paHR0dLZb4tmnTRnyUXOLbrVs3sRR7586dSmdnZy7x/QvPrkKS8B6X3jJ1U1NTscz36tWryvXr1yutra2VP/30k8oyVOn7RHh4uPLs2bPKPn36yC5DbdasmViKffDgQbF6zNCX9z5r8ODByjfeeKN4GbW05Fda1j9p0qTi9/A+v/pqRWmrBOlDigSLFy8Wv759+3ap3Vdp5ZK0jPrTTz8Vy6iln6XS3xMuo9aw0NBQ8Y1f2g9GWlYtrXunFyf9BZH7kPaGKSL9xRg5cqRYdif9R/7ee++JkPOsW7duKXv27Cn2EpC+kU2YMEGZn59fDl+RbgYY3uPSs3XrVhH2pH/cNGjQQPn999+rvC4tRZ0+fbr4Bi69x9PTU3n58mWV9zx69Eh8w5f2NpGWqg8dOlT8YKGn0tPTxX+/0vdeS0tL5Ztvvin2Lnl2WS7v86vZs2eP7PdkKTSW5n2V9pCRthyQxpDCqBSMXpeR9D+v/vyGiIiISPM4B4aIiIh0DgMMERER6RwGGCIiItI5DDBERESkcxhgiIiISOcwwBAREZHOYYAhIiIincMAQ0RERDqHAYaIiIh0DgMMERER6RwGGCIiItI5DDBEREQEXfN/g0pybVxb0g8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(data[:, 0], data[:, 1])\n",
    "\n",
    "x_vals = np.sort(data[:, 0])\n",
    "y_vals = model.predict(x_vals.reshape(-1, 1))\n",
    "\n",
    "plt.plot(x_vals, y_vals, color='red')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
