{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7dbea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8caae505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    x,y,z = x\n",
    "    return (x - z)**2 + (2*y + z)**2 + (4*x - 2*y + z)**2 + x + y\n",
    "\n",
    "def grad_f1(x):\n",
    "    x,y,z = x[0], x[1], x[2]\n",
    "    dx = 2*(x - z) + 8*(4*x - 2*y + z) + 1\n",
    "    dy = 4*(2*y + z) - 4*(4*x - 2*y + z) + 1\n",
    "    dz = -2*(x - z) + 2*(2*y + z) + 2*(4*x - 2*y + z)\n",
    "    \n",
    "    return np.array([dx, dy, dz])\n",
    "\n",
    "def hess_f1(x):\n",
    "    dxx = 34 # 2 + 32\n",
    "    dxy = -16\n",
    "    dxz = 6 #-2 + 8\n",
    "    dyy = 0\n",
    "    dyz = 8\n",
    "    dzz = 6\n",
    "    \n",
    "    return np.array([[dxx, dxy, dxz], [dxy, dyy, dyz], [dxz, dyz, dzz]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08eaaa03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(0),\n",
       " array([1, 1, 0]),\n",
       " array([[ 34, -16,   6],\n",
       "        [-16,   0,   8],\n",
       "        [  6,   8,   6]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_x1 = np.array([0,0,0]) \n",
    "f1(f1_x1), grad_f1(f1_x1), hess_f1(f1_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c45d29e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x):\n",
    "    x,y,z = x\n",
    "    return (x - 1)**2 + (y - 1)**2 + 100*(y-x**2)**2 + 100*(z-y**2)**2\n",
    "\n",
    "def grad_f2(x):\n",
    "    x,y,z = x[0], x[1], x[2]\n",
    "    dx = 2*x - 2 - 400 * x * y + 400 * x**3\n",
    "    dy = 2*y - 2 + 200 * (y - x**2) - 400*z*y + 400 * y**3\n",
    "    dz = 200 * (z - y**2)\n",
    "        \n",
    "    return np.array([dx, dy, dz])\n",
    "\n",
    "def hess_f2(x):\n",
    "    x,y,z = x[0], x[1], x[2]\n",
    "    \n",
    "    dxx = 2 - 400 * y + 1200 * x**2\n",
    "    dxy = -400 * x\n",
    "    dxz = 0\n",
    "    dyy = 2 + 200 - 400 * z + 1200 * y**2\n",
    "    dyz = -400 * y\n",
    "    dzz = 200\n",
    "    return np.array([[dxx, dxy, dxz], [dxy, dyy, dyz], [dxz, dyz, dzz]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fff7db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(11.6),\n",
       " array([115.6,  67.6, -48. ]),\n",
       " array([[1250., -480.,    0.],\n",
       "        [-480., 1450., -480.],\n",
       "        [   0., -480.,  200.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2_x1 = np.array([1.2, 1.2, 1.2])\n",
    "f2(f2_x1), grad_f2(f2_x1), hess_f2(f2_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2ac8e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(x):\n",
    "    x,y = x\n",
    "    return (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2\n",
    "\n",
    "def grad_f3(x):\n",
    "    x,y = x[0], x[1]\n",
    "    \n",
    "    dx = -12.75 * 6*x + 3*y - 4*x*y - 2*x*y**2 + 4.5*y**2 + 2*x*y**4 - 4*x*y**3 + 5.25 * y**3 + 2*x*y**6\n",
    "    dy = 3*x + 9*y*x - 4*x**2*y - 2*x**2 + 15.75*y**2 *x + 2*x**2 * y - 6 * x**2 * y**2 + 4*x**3*y**3 + 6*x**2*y**5\n",
    "    \n",
    "    return np.array([dx, dy])\n",
    "\n",
    "def hess_f3(x):\n",
    "    x,y = x[0], x[1]\n",
    "    \n",
    "    dxx = 6 - 4*y - 2*y**2 + 2*y**4 - 4*y**3 + 2*y**6\n",
    "    dxy = 3 - 4*x - 4*x*y + 9*y + 8*x*y**3 - 12*x*y**2 + 15.75*y**2 + 12*x*y**5\n",
    "    dyy = 9*x - 4*x**2 + 31.5*y*x + 2*x**2 - 12*x**2*y + 12*x**2*y**2 + 30*x**2*y**4\n",
    "    return np.array([[dxx, dxy], [dxy, dyy]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6582d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(14.203125),\n",
       " array([-69.75,  27.75]),\n",
       " array([[ 0.  , 27.75],\n",
       "        [27.75, 68.5 ]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f3_x1 = np.array([1,1])\n",
    "f3(f3_x1), grad_f3(f3_x1), hess_f3(f3_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdd8fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate(i):\n",
    "    return 0.01 * 0.99**i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f3a8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(x, grad_f, lr_fn, n_steps, t=None):\n",
    "    \n",
    "    if t is not None:\n",
    "        i = 0\n",
    "        start = time.time()\n",
    "        while time.time() - start < t:\n",
    "            x = x - lr_fn(i) * grad_f(x)\n",
    "            i += 1\n",
    "    else:\n",
    "        for i in range(n_steps):\n",
    "            x = x - lr_fn(i) * grad_f(x)\n",
    "        \n",
    "    return x \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06be5673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyak(x: np.ndarray, grad_f, lr_fn, mu, n_steps, t=None):\n",
    "    x_prev = x\n",
    "    \n",
    "    if t is not None:\n",
    "        start = time.time()\n",
    "        i = 0\n",
    "        while time.time() - start < t:\n",
    "            dx = grad_f(x)\n",
    "            x = x - lr_fn(i) * dx  + mu * (x - x_prev)\n",
    "        \n",
    "            x_prev = x\n",
    "            i+=1\n",
    "    else:\n",
    "        for i in range(n_steps):\n",
    "            dx = grad_f(x)\n",
    "            x = x - lr_fn(i) * dx  + mu * (x - x_prev)\n",
    "            \n",
    "            x_prev = x\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42bb25bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov(x, f, grad_f, lr_fn, mu, n_steps, t=None):\n",
    "    x_prev = x\n",
    "    \n",
    "    if t is not None:\n",
    "        start = time.time()\n",
    "        i = 0\n",
    "        while time.time() - start < t:\n",
    "            dx = grad_f(x + mu * (x - x_prev))\n",
    "            x = x - lr_fn(i) * dx + mu * (x_prev - x)\n",
    "        \n",
    "            x_prev = x\n",
    "            i+=1\n",
    "    else:\n",
    "        for i in range(n_steps):\n",
    "            dx = grad_f(x + mu * (x - x_prev))\n",
    "            x = x - lr_fn(i) * dx + mu * (x_prev - x)\n",
    "            \n",
    "            x_prev = x\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "044fa1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adagrad(x, grad_f, lr_fn, n_steps, eps=1e-8, t=None):\n",
    "    x = np.array(x, dtype=float)\n",
    "    n = len(x)\n",
    "    \n",
    "    grad_sq_sum = np.zeros(n)\n",
    "    \n",
    "     \n",
    "    if t is not None:\n",
    "        start = time.time()\n",
    "        i = 0\n",
    "        while time.time() - start < t:\n",
    "            grad = np.array(grad_f(x))\n",
    "        \n",
    "            # Accumulate squared gradients\n",
    "            grad_sq_sum += grad ** 2\n",
    "            \n",
    "            # Compute D_k^{-1/2}\n",
    "            D_inv_sqrt = np.diag(1.0 / (np.sqrt(grad_sq_sum) + eps))\n",
    "            \n",
    "            # Adaptive step\n",
    "            step = lr_fn(i) * D_inv_sqrt @ grad\n",
    "            x_new = x - step\n",
    "            \n",
    "            x = x_new\n",
    "            i+= 1\n",
    "    else:\n",
    "        for i in range(n_steps):\n",
    "            grad = np.array(grad_f(x))\n",
    "            \n",
    "            # Accumulate squared gradients\n",
    "            grad_sq_sum += grad ** 2\n",
    "            \n",
    "            # Compute D_k^{-1/2}\n",
    "            D_inv_sqrt = np.diag(1.0 / (np.sqrt(grad_sq_sum) + eps))\n",
    "            \n",
    "            # Adaptive step\n",
    "            step = lr_fn(i) * D_inv_sqrt @ grad\n",
    "            x_new = x - step\n",
    "            \n",
    "            x = x_new\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93b767ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-0.15969334544827538)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(adagrad(f1_x1, grad_f1, learning_rate, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94603437",
   "metadata": {},
   "source": [
    "Newton and BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e15c33bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def is_positive_definite(H):\n",
    "    try:\n",
    "        np.linalg.cholesky(H)\n",
    "        return True\n",
    "    except np.linalg.LinAlgError:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb4858f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(x0, grad_f, hess_f, n_steps=100, t=None, lr=None):\n",
    "    x = np.array(x0, dtype=float)\n",
    "    \n",
    "    if t is not None:\n",
    "        start = time.time()\n",
    "        i = 0\n",
    "        while time.time() - start < t:\n",
    "            grad = np.array(grad_f(x))\n",
    "            hess = np.array(hess_f(x))\n",
    "            \n",
    "            hess_inv = np.linalg.inv(hess)\n",
    "            \n",
    "            lr = lr if lr is not None else learning_rate(i)\n",
    "            # Compute Newton step\n",
    "            x = x - learning_rate(i) * hess_inv @ grad\n",
    "            i += 1\n",
    "\n",
    "    else:\n",
    "        for i in range(n_steps):\n",
    "            grad = np.array(grad_f(x))\n",
    "            grad = np.clip(grad, -10, 10)\n",
    "\n",
    "            hess = np.array(hess_f(x)) + 1e-4 * np.eye(len(x))\n",
    "            \n",
    "            hess_inv = np.linalg.inv(hess)\n",
    "            \n",
    "            \n",
    "            # Compute Newton step\n",
    "            x = x - learning_rate(i) * np.linalg.solve(hess, grad)\n",
    "\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e96eed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Newton: \", f1(newton(np.array([0,0,0]), grad_f1, hess_f1, 100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9266eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(x, grad_f, lr_fn, n_steps=100, t=None):\n",
    "    x = np.array(x, dtype=float)\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        grad = grad_f(x) \n",
    "        x = x - lr_fn(i) * grad\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fb9f53",
   "metadata": {},
   "source": [
    "## TODO: BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f38b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfgs(x, grad_f, lr_fn, n_steps, t=None):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7063c60a",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2eb0fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum2(f):\n",
    "    \"\"\"Finds approximate minimum of f(x)\"\"\"\n",
    "    xs = np.linspace(-3, 3, 1000)\n",
    "    ys = np.linspace(-3, 3, 1000)\n",
    "    X, Y = np.meshgrid(xs, ys)  \n",
    "    Z = f(np.array([X, Y]))  \n",
    "    \n",
    "    min_idx = np.unravel_index(np.argmin(Z), Z.shape)  \n",
    "    bx, by, bz = X[min_idx], Y[min_idx], Z[min_idx]\n",
    "    return np.array([bx, by, bz])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf59e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum3(f):\n",
    "    \"\"\"Finds approximate minimum of f(x)\"\"\"\n",
    "    xs = np.linspace(-3, 3, 100)\n",
    "    ys = np.linspace(-3, 3, 100)\n",
    "    zs = np.linspace(-3, 3, 100)\n",
    "    \n",
    "    X, Y, Z = np.meshgrid(xs, ys, zs)  \n",
    "    W = f(np.array([X, Y, Z]))  \n",
    "    \n",
    "    min_idx = np.unravel_index(np.argmin(W), W.shape)  \n",
    "    bx, by, bz, bw = X[min_idx], Y[min_idx], Z[min_idx], W[min_idx]\n",
    "    return np.array([bx, by, bz, bw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8be12379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 minimum: [-0.15151515 -0.21212121  0.15151515 -0.19651056]\n",
      "f2 minimum: [1. 1. 1. 0.]\n",
      "f3 minimum: [3.00000000e+00 5.01501502e-01 5.21242614e-05]\n"
     ]
    }
   ],
   "source": [
    "f1_xopt = minimum3(f1)\n",
    "f2_xopt = minimum3(f2)\n",
    "f3_xopt = minimum2(f3)\n",
    "print(\"f1 minimum:\", f1_xopt)\n",
    "print(\"f2 minimum:\", f2_xopt)\n",
    "print(\"f3 minimum:\", f3_xopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0293c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum(f):\n",
    "    if f == f1 or f == f2:\n",
    "        return minimum3(f)\n",
    "    return minimum2(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cac05bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_steps(f, grad_f, hess_f, x0, steps):\n",
    "    print(\"True minimum\", minimum(f)[-1], \"at \", minimum(f)[:-1])\n",
    "    for step in steps:\n",
    "        print(\"Steps, \", step)\n",
    "        print(\"GD: \", f(gd(x0, grad_f, learning_rate, step)))\n",
    "        print(\"Polyak: \", f(polyak(x0, grad_f, learning_rate, 0.05, step)))\n",
    "        print(\"Nesterov: \", f(nesterov(x0, f, grad_f, learning_rate, 0.05, step)))\n",
    "        print(\"Adagrad: \", f(adagrad(x0, grad_f, learning_rate, step)))\n",
    "        print(\"Newton: \", f(newton(x0, grad_f, hess_f, step)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9009436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True minimum -0.19651056014692375 at  [-0.15151515 -0.21212121  0.15151515]\n",
      "Steps,  2\n",
      "GD:  -0.035101752536\n",
      "Polyak:  -0.035101752536\n",
      "Nesterov:  -0.035101752536\n",
      "Adagrad:  -0.03156182988530955\n",
      "Newton:  0.0015077275337369443\n",
      "\n",
      "Steps,  5\n",
      "GD:  -0.07159791032768308\n",
      "Polyak:  -0.07159791032768308\n",
      "Nesterov:  -0.07159791032768308\n",
      "Adagrad:  -0.055560190582874064\n",
      "Newton:  0.00381477728519076\n",
      "\n",
      "Steps,  10\n",
      "GD:  -0.10989895086006889\n",
      "Polyak:  -0.10989895086006889\n",
      "Nesterov:  -0.10989895086006889\n",
      "Adagrad:  -0.07927804062158532\n",
      "Newton:  0.0077552790699727935\n",
      "\n",
      "Steps,  100\n",
      "GD:  -0.1945330755830976\n",
      "Polyak:  -0.1945330755830976\n",
      "Nesterov:  -0.1945330755830976\n",
      "Adagrad:  -0.15969334544827538\n",
      "Newton:  0.07391146130764983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1x0 = np.array([0, 0, 0])\n",
    "f1x1 = np.array([1, 1, 0])\n",
    "f2x0 = np.array([1.2, 1.2, 1.2])\n",
    "f2x1 = np.array([-1, 1.2, 1.2])\n",
    "f3x0 = np.array([1,1])\n",
    "f3x1 = np.array([4.5, 4.5])\n",
    "\n",
    "steps = [2, 5, 10, 100]\n",
    "f = f1\n",
    "grad_f = grad_f1\n",
    "hess_f = hess_f1\n",
    "\n",
    "test_steps(f, grad_f, hess_f, f1x0, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e10ac6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True minimum -0.19651056014692375 at  [-0.15151515 -0.21212121  0.15151515]\n",
      "Steps,  2\n",
      "GD:  6.055305723432\n",
      "Polyak:  6.055305723432\n",
      "Nesterov:  6.055305723432\n",
      "Adagrad:  10.564434729571323\n",
      "Newton:  10.835699959970688\n",
      "\n",
      "Steps,  5\n",
      "GD:  3.7295389463962967\n",
      "Polyak:  3.7295389463962967\n",
      "Nesterov:  3.7295389463962967\n",
      "Adagrad:  10.19757537130621\n",
      "Newton:  10.602248804037101\n",
      "\n",
      "Steps,  10\n",
      "GD:  1.9655816298846445\n",
      "Polyak:  1.9655816298846445\n",
      "Nesterov:  1.9655816298846445\n",
      "Adagrad:  9.796470605386531\n",
      "Newton:  10.245229150891145\n",
      "\n",
      "Steps,  100\n",
      "GD:  -0.14664799141363444\n",
      "Polyak:  -0.14664799141363444\n",
      "Nesterov:  -0.14664799141363444\n",
      "Adagrad:  7.890825377305587\n",
      "Newton:  7.5083292980024545\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_steps(f, grad_f, hess_f, f1x1, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f765bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate(i):\n",
    "    return 0.001 * 0.99**i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8c15a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2 1.2 1.2]\n",
      "True minimum 0.0 at  [1. 1. 1.]\n",
      "Steps,  2\n",
      "GD:  0.04360204395724923\n",
      "Polyak:  0.04360204395724923\n",
      "Nesterov:  0.04360204395724923\n",
      "Adagrad:  11.212136336931973\n",
      "Newton:  11.5928148957022\n",
      "\n",
      "Steps,  5\n",
      "GD:  0.018222098858116126\n",
      "Polyak:  0.018222098858116126\n",
      "Nesterov:  0.018222098858116126\n",
      "Adagrad:  10.884354328434366\n",
      "Newton:  11.5823087149097\n",
      "\n",
      "Steps,  10\n",
      "GD:  0.018114796902295175\n",
      "Polyak:  0.018114796902295175\n",
      "Nesterov:  0.018114796902295175\n",
      "Adagrad:  10.524769246910026\n",
      "Newton:  11.565497827499115\n",
      "\n",
      "Steps,  100\n",
      "GD:  0.017367128919067866\n",
      "Polyak:  0.017367128919067866\n",
      "Nesterov:  0.017367128919067866\n",
      "Adagrad:  8.795304386701261\n",
      "Newton:  11.3722662231166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = f2\n",
    "grad_f = grad_f2\n",
    "hess_f = hess_f2\n",
    "\n",
    "print(f2x0)\n",
    "test_steps(f, grad_f, hess_f, f2x0, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f03af44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True minimum 0.0 at  [1. 1. 1.]\n",
      "Steps,  2\n",
      "GD:  5.229583521614642\n",
      "Polyak:  5.229583521614642\n",
      "Nesterov:  5.229583521614642\n",
      "Adagrad:  13.332066264049159\n",
      "Newton:  13.792268870689957\n",
      "\n",
      "Steps,  5\n",
      "GD:  4.227341057837024\n",
      "Polyak:  4.227341057837024\n",
      "Nesterov:  4.227341057837024\n",
      "Adagrad:  12.938862640765567\n",
      "Newton:  13.78097380802975\n",
      "\n",
      "Steps,  10\n",
      "GD:  4.20332860424492\n",
      "Polyak:  4.20332860424492\n",
      "Nesterov:  4.20332860424492\n",
      "Adagrad:  12.50992146936056\n",
      "Newton:  13.76292415334478\n",
      "\n",
      "Steps,  100\n",
      "GD:  4.162038855665383\n",
      "Polyak:  4.162038855665383\n",
      "Nesterov:  4.162038855665383\n",
      "Adagrad:  10.487659531267003\n",
      "Newton:  13.557524559378626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_steps(f, grad_f, hess_f, f2x1, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81bf0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate(i):\n",
    "    return 0.00002 * 0.99**i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e04b971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True minimum 5.2124261432689534e-05 at  [3.        0.5015015]\n",
      "Steps,  2\n",
      "GD:  14.172429833366285\n",
      "Polyak:  14.172429833366285\n",
      "Nesterov:  14.172429833366285\n",
      "Adagrad:  14.20218149283947\n",
      "Newton:  14.203522987258475\n",
      "\n",
      "Steps,  5\n",
      "GD:  14.127347739705236\n",
      "Polyak:  14.127347739705236\n",
      "Nesterov:  14.127347739705236\n",
      "Adagrad:  14.201359799053826\n",
      "Newton:  14.204105085750374\n",
      "\n",
      "Steps,  10\n",
      "GD:  14.054718428955553\n",
      "Polyak:  14.054718428955553\n",
      "Nesterov:  14.054718428955553\n",
      "Adagrad:  14.200432977586338\n",
      "Newton:  14.205036881865112\n",
      "\n",
      "Steps,  100\n",
      "GD:  13.175395207257178\n",
      "Polyak:  13.175395207257178\n",
      "Nesterov:  13.175395207257178\n",
      "Adagrad:  14.19557041905663\n",
      "Newton:  14.215781649394348\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = f3\n",
    "grad_f = grad_f3\n",
    "hess_f = hess_f3\n",
    "\n",
    "test_steps(f, grad_f, hess_f, f3x0, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa674423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True minimum 5.2124261432689534e-05 at  [3.        0.5015015]\n",
      "Steps,  2\n",
      "GD:  12.43281596089588\n",
      "Polyak:  12.43281596089588\n",
      "Nesterov:  12.43281596089588\n",
      "Adagrad:  174802.90334024787\n",
      "Newton:  174813.3627645243\n",
      "\n",
      "Steps,  5\n",
      "GD:  12.513887697772656\n",
      "Polyak:  12.513887697772656\n",
      "Nesterov:  12.513887697772656\n",
      "Adagrad:  174793.794192175\n",
      "Newton:  174813.36200865183\n",
      "\n",
      "Steps,  10\n",
      "GD:  12.646195813283086\n",
      "Polyak:  12.646195813283086\n",
      "Nesterov:  12.646195813283086\n",
      "Adagrad:  174783.51999161392\n",
      "Newton:  174813.36079842356\n",
      "\n",
      "Steps,  100\n",
      "GD:  14.418963181952417\n",
      "Polyak:  14.418963181952417\n",
      "Nesterov:  14.418963181952417\n",
      "Adagrad:  174729.62288745432\n",
      "Newton:  174813.34681957096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_steps(f, grad_f, hess_f, f3x1, steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b4c5f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time(f, grad_f, hess_f, x0, step, times):\n",
    "    print(\"True minimum\", f1_xopt[-1])\n",
    "    for _time in times:\n",
    "        print(\"Time, \", _time)\n",
    "        print(\"GD: \", f(gd(x0, grad_f, learning_rate, step, t=_time)))\n",
    "        print(\"Polyak: \", f(polyak(x0, grad_f, learning_rate, 0.9, step, t=_time)))\n",
    "        print(\"Nesterov: \", f(nesterov(x0, f, grad_f, learning_rate, 0.9, step, t=_time)))\n",
    "        print(\"Adagrad: \", f(adagrad(x0, grad_f, learning_rate, 0.9, step, t=_time)))\n",
    "        print(\"Newton: \", f(newton(x0, grad_f, hess_f, step, t=_time)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7546db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "\n",
    "times = [.1, 1, 2]\n",
    "f = f1\n",
    "grad_f = grad_f1\n",
    "hess_f = hess_f1\n",
    "step = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a2447",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a412aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(N):\n",
    "    return np.array([np.array([i, i + np.random.random(1).item()]) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4be77daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.mu = 0.02\n",
    "        self.steps = 1000\n",
    "\n",
    "    def gd(self, theta, loss_grad, lr_fn):\n",
    "        return gd(theta, loss_grad, lr_fn, self.steps)\n",
    "    \n",
    "    def newton(self, theta, loss_grad, hess_grad, lr):\n",
    "        return newton(theta, loss_grad, hess_grad, self.steps, lr=lr)\n",
    "\n",
    "    def sgd(self, theta, grad_f, lr_fn):\n",
    "        return sgd(theta, grad_f, lr_fn, n_steps=self.steps)\n",
    "\n",
    "\n",
    "    def fit(self, X, y, lr_fn, t=None):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Add bias column\n",
    "        X_aug = np.hstack([X, np.ones((n_samples, 1))])\n",
    "        theta = np.array([0,0])#np.random.rand(n_features + 1)\n",
    "\n",
    "        def loss_grad(theta):\n",
    "            print(theta)\n",
    "            preds = X_aug @ theta\n",
    "            grad = (2 / n_samples) * X_aug.T @ (preds - y)\n",
    "\n",
    "            # Print least squares\n",
    "            loss = np.mean((preds - y) ** 2)\n",
    "            print(\"Loss:\", loss)\n",
    "\n",
    "            return grad\n",
    "        \n",
    "        def sgd_loss_grad(theta):\n",
    "            i = np.random.randint(n_samples)\n",
    "            Xi = X_aug[i:i+1]  # shape (1, d+1)\n",
    "            yi = y[i]\n",
    "            pred = Xi @ theta\n",
    "            \n",
    "            print(theta)\n",
    "            loss = np.mean((X_aug @ theta - y) ** 2)\n",
    "            print(\"Loss:\", loss)\n",
    "            \n",
    "            return 2 * Xi.T @ (pred - yi)  # shape (d+1,)\n",
    "\n",
    "        def loss_hessian(theta):\n",
    "            return (2 / n_samples) * X_aug.T @ X_aug\n",
    "\n",
    "        # theta_opt = self.gd(theta, loss_grad, lr_fn)\n",
    "        # theta_opt = self.newton(theta, loss_grad, loss_hessian, .5)\n",
    "        theta_opt = self.sgd(theta, sgd_loss_grad, lr_fn)\n",
    "        \n",
    "        self.coef_ = theta_opt[:-1]\n",
    "        self.intercept_ = theta_opt[-1]\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "        return X @ self.coef_ + self.intercept_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ffa08a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "Loss: 333339.14496299555\n",
      "[0.0711475  0.00011938]\n",
      "Loss: 287627.33213661803\n",
      "[0.10743717 0.00020111]\n",
      "Loss: 265609.1119585277\n",
      "[0.11764556 0.00024347]\n",
      "Loss: 259573.28913010022\n",
      "[0.11870132 0.000257  ]\n",
      "Loss: 258953.01299881336\n",
      "[0.11874065 0.00025962]\n",
      "Loss: 258929.91497967744\n",
      "[0.21565564 0.00038714]\n",
      "Loss: 205154.2298007369\n",
      "[0.21601376 0.00039445]\n",
      "Loss: 204967.10845923985\n",
      "[0.24727459 0.00046212]\n",
      "Loss: 188962.32707013647\n",
      "[0.25551293 0.00049602]\n",
      "Loss: 184852.79310917167\n",
      "[0.37378431 0.00062292]\n",
      "Loss: 130835.73386967716\n",
      "[0.41770457 0.00069353]\n",
      "Loss: 113147.34955667038\n",
      "[0.43672918 0.00073809]\n",
      "Loss: 105883.97025229836\n",
      "[0.44019964 0.00075674]\n",
      "Loss: 104584.96327982243\n",
      "[0.46871073 0.00080974]\n",
      "Loss: 94216.7164979218\n",
      "[0.46999359 0.0008207 ]\n",
      "Loss: 93762.91236861519\n",
      "[0.55205685 0.00090727]\n",
      "Loss: 67010.34268671827\n",
      "[0.55262966 0.00091393]\n",
      "Loss: 66839.3584996445\n",
      "[0.57479386 0.00095482]\n",
      "Loss: 60391.17499703239\n",
      "[0.57768763 0.00096922]\n",
      "Loss: 59573.4278784355\n",
      "[0.64609552 0.00103832]\n",
      "Loss: 41865.65825489501\n",
      "[0.68884376 0.00108808]\n",
      "Loss: 32381.571650659866\n",
      "[0.73529582 0.00113652]\n",
      "Loss: 23454.87324862382\n",
      "[0.755197   0.00116562]\n",
      "Loss: 20069.970005156676\n",
      "[0.7554196  0.00116858]\n",
      "Loss: 20033.5986795911\n",
      "[0.75656347 0.00117523]\n",
      "Loss: 19847.223668774714\n",
      "[0.75941918 0.0011857 ]\n",
      "Loss: 19385.734484319757\n",
      "[0.76339616 0.00119789]\n",
      "Loss: 18752.090359152167\n",
      "[0.77155047 0.0012151 ]\n",
      "Loss: 17485.80934766153\n",
      "[0.78003618 0.00123221]\n",
      "Loss: 16215.060023214404\n",
      "[0.78553269 0.00124571]\n",
      "Loss: 15417.52838609676\n",
      "[0.78571493 0.00124814]\n",
      "Loss: 15391.430366178241\n",
      "[0.78639559 0.0012528 ]\n",
      "Loss: 15294.148754093174\n",
      "[0.78640111 0.00125326]\n",
      "Loss: 15293.361513699865\n",
      "[0.80098989 0.00127447]\n",
      "Loss: 13282.525146986582\n",
      "[0.81338966 0.0012932 ]\n",
      "Loss: 11684.79397797183\n",
      "[0.81700549 0.00130294]\n",
      "Loss: 11238.161644691265\n",
      "[0.8358477 0.0013249]\n",
      "Loss: 9051.593435468669\n",
      "[0.84058906 0.0013353 ]\n",
      "Loss: 8538.591913184706\n",
      "[0.84074687 0.0013372 ]\n",
      "Loss: 8521.774162046198\n",
      "[0.85271574 0.00135329]\n",
      "Loss: 7294.605671626652\n",
      "[0.85305312 0.00135589]\n",
      "Loss: 7261.394885511557\n",
      "[0.85884718 0.00136656]\n",
      "Loss: 6702.885570064497\n",
      "[0.86263464 0.00137494]\n",
      "Loss: 6349.877789604023\n",
      "[0.8707517  0.00138701]\n",
      "Loss: 5625.49446013913\n",
      "[0.88231996 0.00140094]\n",
      "Loss: 4668.914054578721\n",
      "[0.88421547 0.00140632]\n",
      "Loss: 4520.668185451624\n",
      "[0.88452427 0.0014085 ]\n",
      "Loss: 4496.743420824567\n",
      "[0.89029828 0.00141762]\n",
      "Loss: 4061.090570700249\n",
      "[0.89064362 0.0014198 ]\n",
      "Loss: 4035.7378870084826\n",
      "[0.89871585 0.00143025]\n",
      "Loss: 3465.739488000333\n",
      "[0.89947557 0.00143333]\n",
      "Loss: 3414.3266792099053\n",
      "[0.91146284 0.00144536]\n",
      "Loss: 2653.9745273832364\n",
      "[0.91154129 0.00144633]\n",
      "Loss: 2649.3133278891123\n",
      "[0.91733831 0.00145412]\n",
      "Loss: 2316.2249447017366\n",
      "[0.92068132 0.00145981]\n",
      "Loss: 2134.3100534207547\n",
      "[0.92710105 0.00146748]\n",
      "Loss: 1805.8313536688152\n",
      "[0.92989411 0.00147231]\n",
      "Loss: 1671.4827969369717\n",
      "[0.93345478 0.00147765]\n",
      "Loss: 1507.741806716141\n",
      "[0.93790821 0.00148344]\n",
      "Loss: 1314.8252750311826\n",
      "[0.94016754 0.00148741]\n",
      "Loss: 1222.0020774133097\n",
      "[0.94573362 0.0014935 ]\n",
      "Loss: 1007.8205817188201\n",
      "[0.9458052 0.0014942]\n",
      "Loss: 1005.2006108020461\n",
      "[0.94603242 0.00149538]\n",
      "Loss: 996.9061476352142\n",
      "[0.94694642 0.00149767]\n",
      "Loss: 963.8883688610196\n",
      "[0.94702363 0.00149838]\n",
      "Loss: 961.1249355757772\n",
      "[0.94814572 0.00150089]\n",
      "Loss: 921.4084597595012\n",
      "[0.94825411 0.0015017 ]\n",
      "Loss: 917.6160616006654\n",
      "[0.9485854  0.00150306]\n",
      "Loss: 906.0742250420835\n",
      "[0.95018228 0.00150597]\n",
      "Loss: 851.4650153948235\n",
      "[0.95378643 0.00151024]\n",
      "Loss: 734.451758142101\n",
      "[0.95543101 0.00151301]\n",
      "Loss: 683.9313381523785\n",
      "[0.955508  0.0015136]\n",
      "Loss: 681.6103737538826\n",
      "[0.958778  0.0015174]\n",
      "Loss: 586.674608635558\n",
      "[0.96083812 0.00152027]\n",
      "Loss: 530.5191911045346\n",
      "[0.96125617 0.00152153]\n",
      "Loss: 519.4687189663529\n",
      "[0.96131119 0.00152202]\n",
      "Loss: 518.0231563801824\n",
      "[0.96181626 0.00152342]\n",
      "Loss: 504.8459679220668\n",
      "[0.96370472 0.001526  ]\n",
      "Loss: 457.0811958439294\n",
      "[0.96482698 0.00152794]\n",
      "Loss: 429.82064386340534\n",
      "[0.96540551 0.00152933]\n",
      "Loss: 416.0949893561715\n",
      "[0.96806198 0.0015322 ]\n",
      "Loss: 355.9310763010464\n",
      "[0.96904917 0.00153391]\n",
      "Loss: 334.7704109639161\n",
      "[0.97099887 0.00153625]\n",
      "Loss: 294.8836772873725\n",
      "[0.97154257 0.00153746]\n",
      "Loss: 284.2120879691228\n",
      "[0.97166113 0.00153803]\n",
      "Loss: 281.9109520910437\n",
      "[0.97258535 0.00153953]\n",
      "Loss: 264.2950880342566\n",
      "[0.97378873 0.00154122]\n",
      "Loss: 242.21016611496364\n",
      "[0.97381379 0.00154146]\n",
      "Loss: 241.76053739809998\n",
      "[0.97386469 0.00154181]\n",
      "Loss: 240.84853309016424\n",
      "[0.97388987 0.00154207]\n",
      "Loss: 240.39791116177452\n",
      "[0.97403529 0.00154265]\n",
      "Loss: 237.80412796343668\n",
      "[0.97577346 0.00154458]\n",
      "Loss: 207.89176848603893\n",
      "[0.97690293 0.00154609]\n",
      "Loss: 189.53261479451106\n",
      "[0.97784277 0.00154743]\n",
      "Loss: 174.903204431618\n",
      "[0.97784641 0.00154752]\n",
      "Loss: 174.84762311223346\n",
      "[0.97838649 0.00154849]\n",
      "Loss: 166.7081751493484\n",
      "[0.9785155  0.00154898]\n",
      "Loss: 164.7925585394479\n",
      "[0.97978423 0.00155045]\n",
      "Loss: 146.5443210592895\n",
      "[0.98010261 0.00155115]\n",
      "Loss: 142.13330389998137\n",
      "[0.98047709 0.00155192]\n",
      "Loss: 137.0313021106686\n",
      "[0.98158021 0.00155318]\n",
      "Loss: 122.5447545857903\n",
      "[0.98182953 0.0015538 ]\n",
      "Loss: 119.38279029257707\n",
      "[0.98251615 0.00155475]\n",
      "Loss: 110.88881206216574\n",
      "[0.98252492 0.00155486]\n",
      "Loss: 110.78235097912193\n",
      "[0.98271061 0.00155537]\n",
      "Loss: 108.5401275127924\n",
      "[0.98376109 0.00155653]\n",
      "Loss: 96.28831658909662\n",
      "[0.98376222 0.0015566 ]\n",
      "Loss: 96.27558208891246\n",
      "[0.98383562 0.00155692]\n",
      "Loss: 95.44698502411559\n",
      "[0.98386449 0.00155711]\n",
      "Loss: 95.12200363055555\n",
      "[0.98403894 0.00155756]\n",
      "Loss: 93.17052146745242\n",
      "[0.98405872 0.00155774]\n",
      "Loss: 92.95054769913841\n",
      "[0.98490577 0.0015587 ]\n",
      "Loss: 83.77408402870464\n",
      "[0.98491625 0.00155882]\n",
      "Loss: 83.6635246609738\n",
      "[0.98507344 0.00155924]\n",
      "Loss: 82.01433222783481\n",
      "[0.98519206 0.00155959]\n",
      "Loss: 80.78061256038768\n",
      "[0.9852395  0.00155982]\n",
      "Loss: 80.28983802119492\n",
      "[0.98579528 0.00156055]\n",
      "Loss: 74.65172365706641\n",
      "[0.98646228 0.00156132]\n",
      "Loss: 68.15697270867945\n",
      "[0.98694264 0.00156195]\n",
      "Loss: 63.66289260283277\n",
      "[0.98709434 0.00156232]\n",
      "Loss: 62.275645039996775\n",
      "[0.98755375 0.00156293]\n",
      "Loss: 58.16770354564173\n",
      "[0.98796906 0.00156349]\n",
      "Loss: 54.57499716305366\n",
      "[0.98858007 0.00156416]\n",
      "Loss: 49.4981728691651\n",
      "[0.98858498 0.00156425]\n",
      "Loss: 49.458351404363746\n",
      "[0.98870319 0.00156456]\n",
      "Loss: 48.5052574505663\n",
      "[0.98870579 0.00156461]\n",
      "Loss: 48.48436116650769\n",
      "[0.9892711  0.00156524]\n",
      "Loss: 44.055830310187474\n",
      "[0.989637   0.00156571]\n",
      "Loss: 41.30290264905289\n",
      "[0.99006168 0.00156623]\n",
      "Loss: 38.219365406521796\n",
      "[0.99016976 0.00156648]\n",
      "Loss: 37.45380957338054\n",
      "[0.99025544 0.0015667 ]\n",
      "Loss: 36.85242617283284\n",
      "[0.99079249 0.00156725]\n",
      "Loss: 33.19422960645106\n",
      "[0.99107995 0.00156763]\n",
      "Loss: 31.31505830266709\n",
      "[0.99108746 0.0015677 ]\n",
      "Loss: 31.26673107707889\n",
      "[0.99132738 0.00156806]\n",
      "Loss: 29.741631657693684\n",
      "[0.99139453 0.00156826]\n",
      "Loss: 29.321598977579285\n",
      "[0.99155976 0.00156854]\n",
      "Loss: 28.300994493215434\n",
      "[0.99181026 0.00156889]\n",
      "Loss: 26.788238297667625\n",
      "[0.99186372 0.00156905]\n",
      "Loss: 26.470835594694407\n",
      "[0.99223334 0.00156945]\n",
      "Loss: 24.328236468542386\n",
      "[0.99227954 0.00156958]\n",
      "Loss: 24.066830350483944\n",
      "[0.99230699 0.00156968]\n",
      "Loss: 23.912173302843904\n",
      "[0.99253226 0.00156998]\n",
      "Loss: 22.661979651815333\n",
      "[0.99271767 0.00157026]\n",
      "Loss: 21.65831231919214\n",
      "[0.99304526 0.00157061]\n",
      "Loss: 19.941024433695173\n",
      "[0.99331417 0.00157092]\n",
      "Loss: 18.58466181170825\n",
      "[0.9933983 0.0015711]\n",
      "Loss: 18.170221047811637\n",
      "[0.99371678 0.00157142]\n",
      "Loss: 16.64398310235194\n",
      "[0.99371714 0.00157143]\n",
      "Loss: 16.642282190941415\n",
      "[0.99378996 0.0015716 ]\n",
      "Loss: 16.302828723155777\n",
      "[0.99399277 0.00157184]\n",
      "Loss: 15.375955666858678\n",
      "[0.99420358 0.00157208]\n",
      "Loss: 14.441570484165059\n",
      "[0.99436005 0.00157229]\n",
      "Loss: 13.767164423697652\n",
      "[0.9945096 0.0015725]\n",
      "Loss: 13.137813852312695\n",
      "[0.99473388 0.00157275]\n",
      "Loss: 12.221900104676278\n",
      "[0.99477843 0.00157286]\n",
      "Loss: 12.043957116458907\n",
      "[0.99499124 0.00157309]\n",
      "Loss: 11.21210594417628\n",
      "[0.99502575 0.00157319]\n",
      "Loss: 11.080071191059671\n",
      "[0.99504727 0.00157326]\n",
      "Loss: 10.998136588865094\n",
      "[0.9951182  0.00157339]\n",
      "Loss: 10.730212413127758\n",
      "[0.99515618 0.00157348]\n",
      "Loss: 10.588142985725879\n",
      "[0.99515639 0.00157352]\n",
      "Loss: 10.58737136594777\n",
      "[0.99521595 0.00157362]\n",
      "Loss: 10.366515153958783\n",
      "[0.99523349 0.00157369]\n",
      "Loss: 10.301933033464284\n",
      "[0.99534796 0.00157384]\n",
      "Loss: 9.885419631772459\n",
      "[0.99536299 0.0015739 ]\n",
      "Loss: 9.831391578436476\n",
      "[0.99541826 0.00157401]\n",
      "Loss: 9.633944783878118\n",
      "[0.99544276 0.00157409]\n",
      "Loss: 9.547101477031019\n",
      "[0.99561462 0.00157427]\n",
      "Loss: 8.948996880950894\n",
      "[0.99566493 0.00157436]\n",
      "Loss: 8.77762586592244\n",
      "[0.99585246 0.00157455]\n",
      "Loss: 8.153705293358264\n",
      "[0.9958941  0.00157463]\n",
      "Loss: 8.018345962688691\n",
      "[0.99603255 0.00157479]\n",
      "Loss: 7.576571362201573\n",
      "[0.99605107 0.00157484]\n",
      "Loss: 7.518448145693863\n",
      "[0.99605281 0.00157486]\n",
      "Loss: 7.513001645284915\n",
      "[0.9960678  0.00157491]\n",
      "Loss: 7.466119856141235\n",
      "[0.99607004 0.00157495]\n",
      "Loss: 7.459133089086536\n",
      "[0.99617047 0.00157507]\n",
      "Loss: 7.149193328135808\n",
      "[0.99621602 0.00157515]\n",
      "Loss: 7.010800474045471\n",
      "[0.99628282 0.00157525]\n",
      "Loss: 6.810373486754704\n",
      "[0.99630741 0.00157531]\n",
      "Loss: 6.737342390139903\n",
      "[0.996436   0.00157545]\n",
      "Loss: 6.362003549600777\n",
      "[0.99655414 0.00157558]\n",
      "Loss: 6.02686187278279\n",
      "[0.99664815 0.00157568]\n",
      "Loss: 5.766804478248129\n",
      "[0.99669075 0.00157576]\n",
      "Loss: 5.650903022250796\n",
      "[0.99676852 0.00157586]\n",
      "Loss: 5.442428530326563\n",
      "[0.99686423 0.00157596]\n",
      "Loss: 5.191370646901352\n",
      "[0.99687285 0.001576  ]\n",
      "Loss: 5.169063924799882\n",
      "[0.99696242 0.00157611]\n",
      "Loss: 4.940157163358206\n",
      "[0.99704482 0.0015762 ]\n",
      "Loss: 4.734304196078817\n",
      "[0.99712897 0.00157628]\n",
      "Loss: 4.52873163441102\n",
      "[0.99718443 0.00157636]\n",
      "Loss: 4.395829542263904\n",
      "[0.99721797 0.00157643]\n",
      "Loss: 4.316436700680898\n",
      "[0.99723027 0.00157647]\n",
      "Loss: 4.287527293891714\n",
      "[0.99724024 0.00157651]\n",
      "Loss: 4.264137396475418\n",
      "[0.99728982 0.00157658]\n",
      "Loss: 4.148908620998577\n",
      "[0.99729082 0.00157661]\n",
      "Loss: 4.146591228495222\n",
      "[0.9972946  0.00157663]\n",
      "Loss: 4.137885181636157\n",
      "[0.99731629 0.00157669]\n",
      "Loss: 4.088056553140261\n",
      "[0.99731696 0.0015767 ]\n",
      "Loss: 4.086525615337884\n",
      "[0.99731997 0.00157672]\n",
      "Loss: 4.079639207251655\n",
      "[0.9973945 0.0015768]\n",
      "Loss: 3.9109733807828553\n",
      "[0.99745983 0.00157687]\n",
      "Loss: 3.7661763188412714\n",
      "[0.99748047 0.0015769 ]\n",
      "Loss: 3.721030143646108\n",
      "[0.99750396 0.00157695]\n",
      "Loss: 3.6699804241548133\n",
      "[0.9975823  0.00157703]\n",
      "Loss: 3.502390291805529\n",
      "[0.99765361 0.0015771 ]\n",
      "Loss: 3.3534029379542467\n",
      "[0.9976705  0.00157714]\n",
      "Loss: 3.3186131161244594\n",
      "[0.99768698 0.00157717]\n",
      "Loss: 3.2848354680586294\n",
      "[0.99769151 0.0015772 ]\n",
      "Loss: 3.2755820044745283\n",
      "[0.99772477 0.00157725]\n",
      "Loss: 3.2080987932917577\n",
      "[0.99778037 0.00157731]\n",
      "Loss: 3.096896875553179\n",
      "[0.99778992 0.00157734]\n",
      "Loss: 3.078006660438113\n",
      "[0.99780613 0.00157738]\n",
      "Loss: 3.0460971864430983\n",
      "[0.99787224 0.00157745]\n",
      "Loss: 2.9177004387296135\n",
      "[0.99789072 0.00157748]\n",
      "Loss: 2.8823274946291235\n",
      "[0.99792179 0.00157753]\n",
      "Loss: 2.823372882511502\n",
      "[0.99793057 0.00157756]\n",
      "Loss: 2.8068228541691105\n",
      "[0.9979435  0.00157759]\n",
      "Loss: 2.7825546755608617\n",
      "[0.99797879 0.00157763]\n",
      "Loss: 2.7168834363249044\n",
      "[0.99797891 0.00157764]\n",
      "Loss: 2.7166558596533914\n",
      "[0.9980201  0.00157768]\n",
      "Loss: 2.641060501939475\n",
      "[0.99802084 0.00157769]\n",
      "Loss: 2.639716419405721\n",
      "[0.99806053 0.00157773]\n",
      "Loss: 2.5679491644924632\n",
      "[0.99806066 0.00157775]\n",
      "Loss: 2.56771864048566\n",
      "[0.99810319 0.0015778 ]\n",
      "Loss: 2.491996290953294\n",
      "[0.99815574 0.00157785]\n",
      "Loss: 2.4000860535372475\n",
      "[0.9981743  0.00157789]\n",
      "Loss: 2.368065902161876\n",
      "[0.99819041 0.00157792]\n",
      "Loss: 2.340461554537958\n",
      "[0.99819506 0.00157794]\n",
      "Loss: 2.3325343617488454\n",
      "[0.998228   0.00157798]\n",
      "Loss: 2.276713310432326\n",
      "[0.99826704 0.00157802]\n",
      "Loss: 2.211508309806493\n",
      "[0.99827803 0.00157805]\n",
      "Loss: 2.193346648205252\n",
      "[0.99831159 0.00157809]\n",
      "Loss: 2.1383351282811724\n",
      "[0.99832809 0.00157812]\n",
      "Loss: 2.111568826031239\n",
      "[0.99835415 0.00157815]\n",
      "Loss: 2.0696655528717076\n",
      "[0.99837825 0.00157818]\n",
      "Loss: 2.031317981217551\n",
      "[0.99837856 0.00157819]\n",
      "Loss: 2.030826486532758\n",
      "[0.99840904 0.00157823]\n",
      "Loss: 1.9828837189700501\n",
      "[0.99842631 0.00157825]\n",
      "Loss: 1.9559936000484888\n",
      "[0.99843016 0.00157827]\n",
      "Loss: 1.9500255882429265\n",
      "[0.99843035 0.00157828]\n",
      "Loss: 1.9497327394483237\n",
      "[0.99846176 0.00157831]\n",
      "Loss: 1.9014145427282003\n",
      "[0.99848146 0.00157834]\n",
      "Loss: 1.8714532604943825\n",
      "[0.99850267 0.00157838]\n",
      "Loss: 1.8394888885821354\n",
      "[0.9985215 0.0015784]\n",
      "Loss: 1.81134074445849\n",
      "[0.99852301 0.00157842]\n",
      "Loss: 1.8091041167958155\n",
      "[0.99852566 0.00157843]\n",
      "Loss: 1.8051674147441268\n",
      "[0.99852678 0.00157844]\n",
      "Loss: 1.8034974075906283\n",
      "[0.99852797 0.00157845]\n",
      "Loss: 1.8017383504707554\n",
      "[0.99853472 0.00157846]\n",
      "Loss: 1.7917283149935614\n",
      "[0.99853625 0.00157847]\n",
      "Loss: 1.7894701318634056\n",
      "[0.99854639 0.00157849]\n",
      "Loss: 1.774512896856236\n",
      "[0.9985566  0.00157851]\n",
      "Loss: 1.7595287584617576\n",
      "[0.99856521 0.00157853]\n",
      "Loss: 1.7469454439084129\n",
      "[0.99859159 0.00157856]\n",
      "Loss: 1.708697212655227\n",
      "[0.99861244 0.00157859]\n",
      "Loss: 1.6787828815586077\n",
      "[0.99863293 0.00157861]\n",
      "Loss: 1.6496825944488733\n",
      "[0.99863393 0.00157862]\n",
      "Loss: 1.6482604912927683\n",
      "[0.99865352 0.00157864]\n",
      "Loss: 1.6207142108082306\n",
      "[0.99867177 0.00157866]\n",
      "Loss: 1.5952738992651652\n",
      "[0.99867417 0.00157867]\n",
      "Loss: 1.5919397676214062\n",
      "[0.99869759 0.0015787 ]\n",
      "Loss: 1.559661784394219\n",
      "[0.99870908 0.00157872]\n",
      "Loss: 1.5439504979169396\n",
      "[0.99871228 0.00157873]\n",
      "Loss: 1.5396003296335512\n",
      "[0.99872797 0.00157875]\n",
      "Loss: 1.5183210320393121\n",
      "[0.99874486 0.00157877]\n",
      "Loss: 1.495613855334808\n",
      "[0.99875652 0.00157878]\n",
      "Loss: 1.4800323498724182\n",
      "[0.99875872 0.00157879]\n",
      "Loss: 1.4771166405180685\n",
      "[0.99875901 0.0015788 ]\n",
      "Loss: 1.476731944309496\n",
      "[0.99876406 0.00157881]\n",
      "Loss: 1.4700174237738652\n",
      "[0.99877033 0.00157882]\n",
      "Loss: 1.4617092457720213\n",
      "[0.99878127 0.00157884]\n",
      "Loss: 1.4472847697229925\n",
      "[0.99878239 0.00157885]\n",
      "Loss: 1.4458119439521766\n",
      "[0.99879386 0.00157886]\n",
      "Loss: 1.430786021279599\n",
      "[0.99879782 0.00157888]\n",
      "Loss: 1.425610517768912\n",
      "[0.99879844 0.00157889]\n",
      "Loss: 1.4248070552764647\n",
      "[0.99879934 0.00157889]\n",
      "Loss: 1.4236413031009725\n",
      "[0.9987996 0.0015789]\n",
      "Loss: 1.4232912747164284\n",
      "[0.99880891 0.00157892]\n",
      "Loss: 1.411191556679517\n",
      "[0.99881955 0.00157893]\n",
      "Loss: 1.39743463368933\n",
      "[0.99882616 0.00157894]\n",
      "Loss: 1.388930561848574\n",
      "[0.99883206 0.00157895]\n",
      "Loss: 1.3813644188249927\n",
      "[0.99884564 0.00157897]\n",
      "Loss: 1.3640155057012275\n",
      "[0.99884596 0.00157897]\n",
      "Loss: 1.3636180104561633\n",
      "[0.99884643 0.00157897]\n",
      "Loss: 1.3630107089458843\n",
      "[0.99886038 0.00157899]\n",
      "Loss: 1.3453483067155894\n",
      "[0.99886164 0.001579  ]\n",
      "Loss: 1.3437571226250906\n",
      "[0.99886326 0.00157901]\n",
      "Loss: 1.3417108129459603\n",
      "[0.99886507 0.00157901]\n",
      "Loss: 1.339433891514213\n",
      "[0.99886582 0.00157902]\n",
      "Loss: 1.3384789191550222\n",
      "[0.99886849 0.00157903]\n",
      "Loss: 1.3351297886960445\n",
      "[0.99887967 0.00157904]\n",
      "Loss: 1.3211171656587721\n",
      "[0.99887981 0.00157904]\n",
      "Loss: 1.3209375187245065\n",
      "[0.99888472 0.00157905]\n",
      "Loss: 1.3148186546652079\n",
      "[0.9989019  0.00157907]\n",
      "Loss: 1.29350869119798\n",
      "[0.99890375 0.00157908]\n",
      "Loss: 1.291216423114547\n",
      "[0.99890584 0.00157908]\n",
      "Loss: 1.288648785956229\n",
      "[0.99890586 0.00157909]\n",
      "Loss: 1.2886163879042507\n",
      "[0.99891767 0.00157911]\n",
      "Loss: 1.2741168844048798\n",
      "[0.9989275  0.00157912]\n",
      "Loss: 1.2621174804986228\n",
      "[0.99892814 0.00157913]\n",
      "Loss: 1.261333532021778\n",
      "[0.99892946 0.00157913]\n",
      "Loss: 1.2597321898965026\n",
      "[0.99893245 0.00157914]\n",
      "Loss: 1.2560971468967466\n",
      "[0.99893446 0.00157915]\n",
      "Loss: 1.2536510720869185\n",
      "[0.99893904 0.00157916]\n",
      "Loss: 1.2481005664670124\n",
      "[0.99895322 0.00157918]\n",
      "Loss: 1.2310161574982894\n",
      "[0.99896007 0.00157919]\n",
      "Loss: 1.2228013925936128\n",
      "[0.99896922 0.0015792 ]\n",
      "Loss: 1.2118895009481156\n",
      "[0.99897692 0.00157921]\n",
      "Loss: 1.202750227555065\n",
      "[0.99898713 0.00157922]\n",
      "Loss: 1.1906866586591045\n",
      "[0.99899471 0.00157924]\n",
      "Loss: 1.1817819081623082\n",
      "[0.99900393 0.00157925]\n",
      "Loss: 1.170986815962285\n",
      "[0.99900489 0.00157925]\n",
      "Loss: 1.169870805695842\n",
      "[0.99901408 0.00157926]\n",
      "Loss: 1.1591816085410374\n",
      "[0.999016   0.00157927]\n",
      "Loss: 1.156953122373126\n",
      "[0.99902389 0.00157928]\n",
      "Loss: 1.1478339639387243\n",
      "[0.99902579 0.00157929]\n",
      "Loss: 1.1456439380479109\n",
      "[0.99902913 0.0015793 ]\n",
      "Loss: 1.1417999011725513\n",
      "[0.99903035 0.00157931]\n",
      "Loss: 1.140398783723356\n",
      "[0.99903048 0.00157931]\n",
      "Loss: 1.1402466362263497\n",
      "[0.99903061 0.00157931]\n",
      "Loss: 1.140094305023721\n",
      "[0.99903422 0.00157932]\n",
      "Loss: 1.135950870767039\n",
      "[0.99903558 0.00157933]\n",
      "Loss: 1.134396976517328\n",
      "[0.99904292 0.00157934]\n",
      "Loss: 1.1260019187214194\n",
      "[0.99905102 0.00157935]\n",
      "Loss: 1.1167821199968313\n",
      "[0.99906167 0.00157936]\n",
      "Loss: 1.1047320989842486\n",
      "[0.99906435 0.00157937]\n",
      "Loss: 1.1017155078693686\n",
      "[0.99906438 0.00157937]\n",
      "Loss: 1.1016800296441722\n",
      "[0.99906906 0.00157938]\n",
      "Loss: 1.0964102218854854\n",
      "[0.99907341 0.00157939]\n",
      "Loss: 1.0915289220851034\n",
      "[0.99907678 0.0015794 ]\n",
      "Loss: 1.0877548810255926\n",
      "[0.99907994 0.0015794 ]\n",
      "Loss: 1.084228781009651\n",
      "[0.99908201 0.00157941]\n",
      "Loss: 1.0819202268254262\n",
      "[0.99908959 0.00157942]\n",
      "Loss: 1.0734844144853692\n",
      "[0.99909101 0.00157942]\n",
      "Loss: 1.0719149627217828\n",
      "[0.99910029 0.00157943]\n",
      "Loss: 1.0616487234931615\n",
      "[0.99910058 0.00157944]\n",
      "Loss: 1.0613369970491564\n",
      "[0.99910331 0.00157945]\n",
      "Loss: 1.0583277713580777\n",
      "[0.99910731 0.00157945]\n",
      "Loss: 1.0539275269540356\n",
      "[0.99910877 0.00157946]\n",
      "Loss: 1.0523269319365245\n",
      "[0.999116   0.00157947]\n",
      "Loss: 1.0444117468430258\n",
      "[0.99912262 0.00157948]\n",
      "Loss: 1.037193400096521\n",
      "[0.99912571 0.00157948]\n",
      "Loss: 1.0338365088628345\n",
      "[0.99912907 0.00157949]\n",
      "Loss: 1.0301981248305638\n",
      "[0.99913008 0.00157949]\n",
      "Loss: 1.0291024492552874\n",
      "[0.99913153 0.0015795 ]\n",
      "Loss: 1.0275332088296654\n",
      "[0.9991363 0.0015795]\n",
      "Loss: 1.0223767227150342\n",
      "[0.99914003 0.00157951]\n",
      "Loss: 1.018358792828797\n",
      "[0.99914283 0.00157951]\n",
      "Loss: 1.0153461010604228\n",
      "[0.9991437  0.00157952]\n",
      "Loss: 1.0144087288701078\n",
      "[0.99914413 0.00157952]\n",
      "Loss: 1.0139542611243073\n",
      "[0.99914604 0.00157953]\n",
      "Loss: 1.0118976057144318\n",
      "[0.99914655 0.00157953]\n",
      "Loss: 1.011355125134166\n",
      "[0.99914859 0.00157953]\n",
      "Loss: 1.0091741391038223\n",
      "[0.99914943 0.00157954]\n",
      "Loss: 1.0082676546666987\n",
      "[0.9991559  0.00157955]\n",
      "Loss: 1.0013619545500503\n",
      "[0.9991595  0.00157955]\n",
      "Loss: 0.9975302576519705\n",
      "[0.99916477 0.00157956]\n",
      "Loss: 0.9919415814999696\n",
      "[0.99916564 0.00157956]\n",
      "Loss: 0.9910170681470027\n",
      "[0.99916767 0.00157957]\n",
      "Loss: 0.9888682170057508\n",
      "[0.9991686  0.00157957]\n",
      "Loss: 0.9878823737014224\n",
      "[0.9991727  0.00157957]\n",
      "Loss: 0.9835500853611043\n",
      "[0.99917961 0.00157958]\n",
      "Loss: 0.9762796722514191\n",
      "[0.99918329 0.00157959]\n",
      "Loss: 0.9724233774531813\n",
      "[0.99918771 0.00157959]\n",
      "Loss: 0.9678026319967746\n",
      "[0.99919406 0.0015796 ]\n",
      "Loss: 0.9611882359526212\n",
      "[0.99919636 0.0015796 ]\n",
      "Loss: 0.9587981311781844\n",
      "[0.99920084 0.00157961]\n",
      "Loss: 0.954146042445452\n",
      "[0.9992049  0.00157962]\n",
      "Loss: 0.9499465280049012\n",
      "[0.99920523 0.00157962]\n",
      "Loss: 0.9496063446989026\n",
      "[0.9992053  0.00157962]\n",
      "Loss: 0.949536228449964\n",
      "[0.99920614 0.00157962]\n",
      "Loss: 0.9486664846991757\n",
      "[0.99920645 0.00157962]\n",
      "Loss: 0.9483472017181208\n",
      "[0.99920762 0.00157963]\n",
      "Loss: 0.9471386300141243\n",
      "[0.99921155 0.00157963]\n",
      "Loss: 0.9430937035326349\n",
      "[0.99921263 0.00157964]\n",
      "Loss: 0.9419834521547115\n",
      "[0.99921668 0.00157964]\n",
      "Loss: 0.9378249530061062\n",
      "[0.99922076 0.00157965]\n",
      "Loss: 0.9336453329631077\n",
      "[0.99922342 0.00157965]\n",
      "Loss: 0.9309323871555831\n",
      "[0.99922766 0.00157966]\n",
      "Loss: 0.926610260953842\n",
      "[0.99922994 0.00157966]\n",
      "Loss: 0.9242922400242942\n",
      "[0.99923316 0.00157967]\n",
      "Loss: 0.9210159931687851\n",
      "[0.99923551 0.00157967]\n",
      "Loss: 0.9186400002818648\n",
      "[0.99923843 0.00157967]\n",
      "Loss: 0.9156836025376865\n",
      "[0.9992419  0.00157968]\n",
      "Loss: 0.9121791093664785\n",
      "[0.99924402 0.00157968]\n",
      "Loss: 0.910042919591981\n",
      "[0.99924402 0.00157968]\n",
      "Loss: 0.9100417530294853\n",
      "[0.9992478  0.00157969]\n",
      "Loss: 0.9062429739832578\n",
      "[0.99925096 0.00157969]\n",
      "Loss: 0.9030735463498847\n",
      "[0.99925378 0.0015797 ]\n",
      "Loss: 0.9002479924782101\n",
      "[0.99925542 0.0015797 ]\n",
      "Loss: 0.8986134667427225\n",
      "[0.99925557 0.0015797 ]\n",
      "Loss: 0.8984557770001562\n",
      "[0.9992585  0.00157971]\n",
      "Loss: 0.8955361707179491\n",
      "[0.99925978 0.00157971]\n",
      "Loss: 0.8942597349936019\n",
      "[0.99926329 0.00157971]\n",
      "Loss: 0.8907646912467766\n",
      "[0.99926467 0.00157972]\n",
      "Loss: 0.889392529007729\n",
      "[0.99926856 0.00157972]\n",
      "Loss: 0.8855342544235163\n",
      "[0.99927264 0.00157973]\n",
      "Loss: 0.8814955734009938\n",
      "[0.99927788 0.00157973]\n",
      "Loss: 0.8763278398304338\n",
      "[0.99928021 0.00157974]\n",
      "Loss: 0.8740399160373452\n",
      "[0.99928448 0.00157974]\n",
      "Loss: 0.8698452566652265\n",
      "[0.99928494 0.00157974]\n",
      "Loss: 0.8693959732235211\n",
      "[0.99928696 0.00157975]\n",
      "Loss: 0.8674187308580447\n",
      "[0.99928934 0.00157975]\n",
      "Loss: 0.8650908102199745\n",
      "[0.99929    0.00157975]\n",
      "Loss: 0.8644496088218494\n",
      "[0.99929168 0.00157976]\n",
      "Loss: 0.8628084849842285\n",
      "[0.99929469 0.00157976]\n",
      "Loss: 0.8598745130899893\n",
      "[0.99929483 0.00157976]\n",
      "Loss: 0.8597444207709386\n",
      "[0.99929593 0.00157976]\n",
      "Loss: 0.8586750686182546\n",
      "[0.99929598 0.00157976]\n",
      "Loss: 0.85861740606479\n",
      "[0.99929799 0.00157977]\n",
      "Loss: 0.8566700963968265\n",
      "[0.99930155 0.00157977]\n",
      "Loss: 0.8532116087120387\n",
      "[0.99930428 0.00157978]\n",
      "Loss: 0.8505763510822025\n",
      "[0.99930735 0.00157978]\n",
      "Loss: 0.8476066192468862\n",
      "[0.99930784 0.00157978]\n",
      "Loss: 0.8471334564034319\n",
      "[0.99931022 0.00157978]\n",
      "Loss: 0.8448416719300684\n",
      "[0.99931231 0.00157979]\n",
      "Loss: 0.842836117085345\n",
      "[0.99931246 0.00157979]\n",
      "Loss: 0.8426869658128685\n",
      "[0.99931273 0.00157979]\n",
      "Loss: 0.8424292613324348\n",
      "[0.99931291 0.00157979]\n",
      "Loss: 0.8422556895756147\n",
      "[0.99931535 0.0015798 ]\n",
      "Loss: 0.8399099805739688\n",
      "[0.99931884 0.0015798 ]\n",
      "Loss: 0.8365723952029447\n",
      "[0.99931977 0.0015798 ]\n",
      "Loss: 0.8356816806780822\n",
      "[0.99931977 0.0015798 ]\n",
      "Loss: 0.8356806838228882\n",
      "[0.99932149 0.00157981]\n",
      "Loss: 0.8340361994597986\n",
      "[0.9993219  0.00157981]\n",
      "Loss: 0.8336431568553556\n",
      "[0.99932301 0.00157981]\n",
      "Loss: 0.8325821087300177\n",
      "[0.99932329 0.00157981]\n",
      "Loss: 0.8323167900043479\n",
      "[0.99932415 0.00157981]\n",
      "Loss: 0.831497571563611\n",
      "[0.99932557 0.00157982]\n",
      "Loss: 0.8301424237800341\n",
      "[0.99932713 0.00157982]\n",
      "Loss: 0.828659852972083\n",
      "[0.99932746 0.00157982]\n",
      "Loss: 0.8283446184792963\n",
      "[0.99932803 0.00157982]\n",
      "Loss: 0.8277954287196517\n",
      "[0.99932835 0.00157982]\n",
      "Loss: 0.8274910019734377\n",
      "[0.99932868 0.00157983]\n",
      "Loss: 0.8271848755844842\n",
      "[0.99932902 0.00157983]\n",
      "Loss: 0.8268552153377053\n",
      "[0.9993316  0.00157983]\n",
      "Loss: 0.8244075927544677\n",
      "[0.99933171 0.00157983]\n",
      "Loss: 0.8243057680152889\n",
      "[0.99933276 0.00157983]\n",
      "Loss: 0.8233082403331866\n",
      "[0.99933301 0.00157983]\n",
      "Loss: 0.8230693503594789\n",
      "[0.99933369 0.00157984]\n",
      "Loss: 0.8224299138499607\n",
      "[0.99933672 0.00157984]\n",
      "Loss: 0.8195635550975328\n",
      "[0.99933673 0.00157984]\n",
      "Loss: 0.8195490512911376\n",
      "[0.9993394  0.00157984]\n",
      "Loss: 0.8170251271147823\n",
      "[0.99933977 0.00157985]\n",
      "Loss: 0.8166802225090347\n",
      "[0.99934183 0.00157985]\n",
      "Loss: 0.8147392981497645\n",
      "[0.99934268 0.00157985]\n",
      "Loss: 0.8139383364147567\n",
      "[0.99934272 0.00157985]\n",
      "Loss: 0.8139040032932514\n",
      "[0.99934301 0.00157985]\n",
      "Loss: 0.8136232291991286\n",
      "[0.99934394 0.00157985]\n",
      "Loss: 0.812755515874293\n",
      "[0.99934461 0.00157986]\n",
      "Loss: 0.8121259879883329\n",
      "[0.9993456  0.00157986]\n",
      "Loss: 0.8111940704408357\n",
      "[0.99934587 0.00157986]\n",
      "Loss: 0.8109388737207549\n",
      "[0.99934633 0.00157986]\n",
      "Loss: 0.8105037023082448\n",
      "[0.99934718 0.00157986]\n",
      "Loss: 0.809706655651325\n",
      "[0.99935025 0.00157987]\n",
      "Loss: 0.8068281031037068\n",
      "[0.99935063 0.00157987]\n",
      "Loss: 0.8064801367380691\n",
      "[0.99935066 0.00157987]\n",
      "Loss: 0.8064441296313689\n",
      "[0.99935203 0.00157987]\n",
      "Loss: 0.8051690000571878\n",
      "[0.99935347 0.00157987]\n",
      "Loss: 0.8038218190996522\n",
      "[0.9993535  0.00157987]\n",
      "Loss: 0.8037889582045166\n",
      "[0.99935431 0.00157987]\n",
      "Loss: 0.8030372471797032\n",
      "[0.9993549  0.00157988]\n",
      "Loss: 0.8024810744276508\n",
      "[0.99935597 0.00157988]\n",
      "Loss: 0.8014845757489972\n",
      "[0.99935635 0.00157988]\n",
      "Loss: 0.8011368591984844\n",
      "[0.99935885 0.00157988]\n",
      "Loss: 0.7988041389140147\n",
      "[0.99936025 0.00157988]\n",
      "Loss: 0.7975016145300383\n",
      "[0.99936046 0.00157988]\n",
      "Loss: 0.7973066371183792\n",
      "[0.99936085 0.00157988]\n",
      "Loss: 0.7969455134064851\n",
      "[0.99936236 0.00157989]\n",
      "Loss: 0.7955393261621541\n",
      "[0.99936517 0.00157989]\n",
      "Loss: 0.792937046420672\n",
      "[0.99936652 0.00157989]\n",
      "Loss: 0.7916859932307743\n",
      "[0.99936671 0.00157989]\n",
      "Loss: 0.7915156167332623\n",
      "[0.99936707 0.00157989]\n",
      "Loss: 0.7911790010569613\n",
      "[0.99936789 0.00157989]\n",
      "Loss: 0.7904229421255797\n",
      "[0.99936885 0.0015799 ]\n",
      "Loss: 0.7895290788634378\n",
      "[0.99936932 0.0015799 ]\n",
      "Loss: 0.7890988068948763\n",
      "[0.99936952 0.0015799 ]\n",
      "Loss: 0.7889160878565119\n",
      "[0.99936981 0.0015799 ]\n",
      "Loss: 0.7886485717964545\n",
      "[0.99936995 0.0015799 ]\n",
      "Loss: 0.7885192810582909\n",
      "[0.99937083 0.0015799 ]\n",
      "Loss: 0.787703202042253\n",
      "[0.99937103 0.0015799 ]\n",
      "Loss: 0.7875204530960307\n",
      "[0.99937231 0.0015799 ]\n",
      "Loss: 0.7863399511786265\n",
      "[0.99937341 0.0015799 ]\n",
      "Loss: 0.7853261663908914\n",
      "[0.99937359 0.0015799 ]\n",
      "Loss: 0.7851627924905569\n",
      "[0.99937375 0.00157991]\n",
      "Loss: 0.7850151738726238\n",
      "[0.99937423 0.00157991]\n",
      "Loss: 0.7845683763111904\n",
      "[0.99937493 0.00157991]\n",
      "Loss: 0.7839239692965582\n",
      "[0.99937573 0.00157991]\n",
      "Loss: 0.783192479303583\n",
      "[0.99937672 0.00157991]\n",
      "Loss: 0.7822826675287986\n",
      "[0.99937688 0.00157991]\n",
      "Loss: 0.7821353218491521\n",
      "[0.99937724 0.00157991]\n",
      "Loss: 0.7818036187120765\n",
      "[0.99937904 0.00157991]\n",
      "Loss: 0.7801547105978421\n",
      "[0.99937927 0.00157991]\n",
      "Loss: 0.7799448190108073\n",
      "[0.99937989 0.00157992]\n",
      "Loss: 0.7793735548060585\n",
      "[0.99938006 0.00157992]\n",
      "Loss: 0.7792141738885319\n",
      "[0.99938185 0.00157992]\n",
      "Loss: 0.7775804947673112\n",
      "[0.99938223 0.00157992]\n",
      "Loss: 0.7772334785017341\n",
      "[0.99938266 0.00157992]\n",
      "Loss: 0.7768377781781383\n",
      "[0.99938266 0.00157992]\n",
      "Loss: 0.7768333601521625\n",
      "[0.99938391 0.00157992]\n",
      "Loss: 0.7756919328130437\n",
      "[0.99938399 0.00157992]\n",
      "Loss: 0.7756235915975911\n",
      "[0.99938408 0.00157992]\n",
      "Loss: 0.7755357372988764\n",
      "[0.99938425 0.00157992]\n",
      "Loss: 0.7753871530618651\n",
      "[0.99938441 0.00157992]\n",
      "Loss: 0.7752403975432856\n",
      "[0.99938447 0.00157993]\n",
      "Loss: 0.7751808885183681\n",
      "[0.99938456 0.00157993]\n",
      "Loss: 0.775102452728045\n",
      "[0.99938492 0.00157993]\n",
      "Loss: 0.7747716849777477\n",
      "[0.99938539 0.00157993]\n",
      "Loss: 0.7743428610576791\n",
      "[0.99938668 0.00157993]\n",
      "Loss: 0.773165839468307\n",
      "[0.99938758 0.00157993]\n",
      "Loss: 0.7723484759231098\n",
      "[0.99938768 0.00157993]\n",
      "Loss: 0.7722514431406587\n",
      "[0.99938797 0.00157993]\n",
      "Loss: 0.7719879958609597\n",
      "[0.99938813 0.00157993]\n",
      "Loss: 0.7718404196330331\n",
      "[0.99938834 0.00157993]\n",
      "Loss: 0.7716527963790686\n",
      "[0.99938915 0.00157993]\n",
      "Loss: 0.7709184802110836\n",
      "[0.99939051 0.00157994]\n",
      "Loss: 0.7696797669397588\n",
      "[0.99939107 0.00157994]\n",
      "Loss: 0.7691650759254126\n",
      "[0.99939114 0.00157994]\n",
      "Loss: 0.7691064582429074\n",
      "[0.99939165 0.00157994]\n",
      "Loss: 0.7686385272868245\n",
      "[0.99939173 0.00157994]\n",
      "Loss: 0.7685730653729453\n",
      "[0.99939237 0.00157994]\n",
      "Loss: 0.7679899141141061\n",
      "[0.99939312 0.00157994]\n",
      "Loss: 0.7673090657017243\n",
      "[0.9993934  0.00157994]\n",
      "Loss: 0.7670518189654206\n",
      "[0.99939373 0.00157994]\n",
      "Loss: 0.7667573510976565\n",
      "[0.99939377 0.00157994]\n",
      "Loss: 0.766715267832506\n",
      "[0.9993938  0.00157994]\n",
      "Loss: 0.7666859575976235\n",
      "[0.99939397 0.00157994]\n",
      "Loss: 0.7665380147508598\n",
      "[0.99939414 0.00157994]\n",
      "Loss: 0.7663776238135477\n",
      "[0.99939528 0.00157995]\n",
      "Loss: 0.7653487655399309\n",
      "[0.9993963  0.00157995]\n",
      "Loss: 0.7644274610736538\n",
      "[0.99939659 0.00157995]\n",
      "Loss: 0.7641593603684623\n",
      "[0.99939757 0.00157995]\n",
      "Loss: 0.763275540790429\n",
      "[0.99939801 0.00157995]\n",
      "Loss: 0.7628740344565125\n",
      "[0.99939826 0.00157995]\n",
      "Loss: 0.7626494319017859\n",
      "[0.99939886 0.00157995]\n",
      "Loss: 0.7621115021980666\n",
      "[0.99939889 0.00157995]\n",
      "Loss: 0.7620758005250814\n",
      "[0.99939895 0.00157995]\n",
      "Loss: 0.7620242813122459\n",
      "[0.99939963 0.00157995]\n",
      "Loss: 0.7614141064207104\n",
      "[0.99939992 0.00157995]\n",
      "Loss: 0.7611451879943251\n",
      "[0.99940074 0.00157995]\n",
      "Loss: 0.7604122436912892\n",
      "[0.99940103 0.00157995]\n",
      "Loss: 0.7601433927694144\n",
      "[0.9994013  0.00157996]\n",
      "Loss: 0.7599008100541698\n",
      "[0.99940141 0.00157996]\n",
      "Loss: 0.75980384102844\n",
      "[0.99940204 0.00157996]\n",
      "Loss: 0.759240439993836\n",
      "[0.99940237 0.00157996]\n",
      "Loss: 0.758937209817637\n",
      "[0.9994024  0.00157996]\n",
      "Loss: 0.7589116202627344\n",
      "[0.99940275 0.00157996]\n",
      "Loss: 0.7585933457446707\n",
      "[0.99940282 0.00157996]\n",
      "Loss: 0.758536576956236\n",
      "[0.99940302 0.00157996]\n",
      "Loss: 0.7583514180092902\n",
      "[0.99940306 0.00157996]\n",
      "Loss: 0.7583199049511132\n",
      "[0.99940349 0.00157996]\n",
      "Loss: 0.7579324508620233\n",
      "[0.99940373 0.00157996]\n",
      "Loss: 0.7577130175373576\n",
      "[0.99940378 0.00157996]\n",
      "Loss: 0.7576650600309004\n",
      "[0.99940386 0.00157996]\n",
      "Loss: 0.7575986734230941\n",
      "[0.99940387 0.00157996]\n",
      "Loss: 0.7575878061145019\n",
      "[0.99940405 0.00157996]\n",
      "Loss: 0.7574209380023444\n",
      "[0.99940431 0.00157996]\n",
      "Loss: 0.7571873102294419\n",
      "[0.99940484 0.00157996]\n",
      "Loss: 0.7567181573537127\n",
      "[0.99940548 0.00157996]\n",
      "Loss: 0.7561407541847786\n",
      "[0.99940562 0.00157996]\n",
      "Loss: 0.7560134457829845\n",
      "[0.99940584 0.00157997]\n",
      "Loss: 0.7558105278465375\n",
      "[0.99940593 0.00157997]\n",
      "Loss: 0.7557318799149623\n",
      "[0.99940652 0.00157997]\n",
      "Loss: 0.7552025937927834\n",
      "[0.99940734 0.00157997]\n",
      "Loss: 0.7544652715326535\n",
      "[0.99940762 0.00157997]\n",
      "Loss: 0.7542170124179379\n",
      "[0.99940826 0.00157997]\n",
      "Loss: 0.7536383397040423\n",
      "[0.99940833 0.00157997]\n",
      "Loss: 0.7535766916601758\n",
      "[0.99940887 0.00157997]\n",
      "Loss: 0.7530901646994291\n",
      "[0.99940927 0.00157997]\n",
      "Loss: 0.7527303057549989\n",
      "[0.99940931 0.00157997]\n",
      "Loss: 0.7526981839921041\n",
      "[0.99940972 0.00157997]\n",
      "Loss: 0.7523329856860373\n",
      "[0.99941049 0.00157997]\n",
      "Loss: 0.7516426687726814\n",
      "[0.99941062 0.00157997]\n",
      "Loss: 0.7515273235483878\n",
      "[0.99941073 0.00157997]\n",
      "Loss: 0.7514274288732409\n",
      "[0.99941106 0.00157997]\n",
      "Loss: 0.7511274030956417\n",
      "[0.9994112  0.00157997]\n",
      "Loss: 0.7510018016796812\n",
      "[0.99941122 0.00157997]\n",
      "Loss: 0.7509845201798308\n",
      "[0.99941141 0.00157997]\n",
      "Loss: 0.7508149299648474\n",
      "[0.9994116  0.00157998]\n",
      "Loss: 0.7506473097645726\n",
      "[0.99941188 0.00157998]\n",
      "Loss: 0.7503927438659002\n",
      "[0.99941211 0.00157998]\n",
      "Loss: 0.750193034743583\n",
      "[0.99941267 0.00157998]\n",
      "Loss: 0.7496890043036963\n",
      "[0.99941277 0.00157998]\n",
      "Loss: 0.7495995511714655\n",
      "[0.99941309 0.00157998]\n",
      "Loss: 0.7493122539007829\n",
      "[0.9994131  0.00157998]\n",
      "Loss: 0.7493028667670626\n",
      "[0.99941311 0.00157998]\n",
      "Loss: 0.7492944709815121\n",
      "[0.99941317 0.00157998]\n",
      "Loss: 0.7492404701355198\n",
      "[0.99941323 0.00157998]\n",
      "Loss: 0.749188658261964\n",
      "[0.99941344 0.00157998]\n",
      "Loss: 0.7490015046967226\n",
      "[0.99941353 0.00157998]\n",
      "Loss: 0.7489182149355803\n",
      "[0.99941395 0.00157998]\n",
      "Loss: 0.7485471946160369\n",
      "[0.99941403 0.00157998]\n",
      "Loss: 0.7484754508368913\n",
      "[0.99941419 0.00157998]\n",
      "Loss: 0.7483329806330764\n",
      "[0.99941434 0.00157998]\n",
      "Loss: 0.7481907533444306\n",
      "[0.99941448 0.00157998]\n",
      "Loss: 0.7480700069536377\n",
      "[0.99941485 0.00157998]\n",
      "Loss: 0.7477379801590288\n",
      "[0.99941493 0.00157998]\n",
      "Loss: 0.7476698259148704\n",
      "[0.99941507 0.00157998]\n",
      "Loss: 0.7475462411662427\n",
      "[0.99941507 0.00157998]\n",
      "Loss: 0.7475441009942452\n",
      "[0.99941545 0.00157998]\n",
      "Loss: 0.7472064744849204\n",
      "[0.99941554 0.00157998]\n",
      "Loss: 0.7471218549840897\n",
      "[0.99941555 0.00157998]\n",
      "Loss: 0.7471130118302306\n",
      "[0.99941567 0.00157998]\n",
      "Loss: 0.7470068040000869\n",
      "[0.99941576 0.00157998]\n",
      "Loss: 0.746925886522051\n",
      "[0.99941592 0.00157998]\n",
      "Loss: 0.746783022926706\n",
      "[0.99941606 0.00157998]\n",
      "Loss: 0.7466620195740641\n",
      "[0.99941609 0.00157999]\n",
      "Loss: 0.746630937580104\n",
      "[0.9994163  0.00157999]\n",
      "Loss: 0.7464488627348721\n",
      "[0.99941649 0.00157999]\n",
      "Loss: 0.7462720255350334\n",
      "[0.99941682 0.00157999]\n",
      "Loss: 0.7459785502407497\n",
      "[0.99941727 0.00157999]\n",
      "Loss: 0.7455776099274042\n",
      "[0.99941753 0.00157999]\n",
      "Loss: 0.7453518039756161\n",
      "[0.99941771 0.00157999]\n",
      "Loss: 0.7451869377984779\n",
      "[0.99941772 0.00157999]\n",
      "Loss: 0.7451787951398022\n",
      "[0.99941781 0.00157999]\n",
      "Loss: 0.7450951898609508\n",
      "[0.99941791 0.00157999]\n",
      "Loss: 0.745007513233958\n",
      "[0.99941795 0.00157999]\n",
      "Loss: 0.7449745223495385\n",
      "[0.99941835 0.00157999]\n",
      "Loss: 0.744619825416736\n",
      "[0.99941835 0.00157999]\n",
      "Loss: 0.7446189160105154\n",
      "[0.99941869 0.00157999]\n",
      "Loss: 0.7443155496316712\n",
      "[0.99941883 0.00157999]\n",
      "Loss: 0.7441875387770968\n",
      "[0.99941919 0.00157999]\n",
      "Loss: 0.7438703355038635\n",
      "[0.99941926 0.00157999]\n",
      "Loss: 0.7438083655902676\n",
      "[0.99941935 0.00157999]\n",
      "Loss: 0.743728392130799\n",
      "[0.99941967 0.00157999]\n",
      "Loss: 0.7434425746892518\n",
      "[0.99941988 0.00157999]\n",
      "Loss: 0.7432589942435687\n",
      "[0.99942012 0.00157999]\n",
      "Loss: 0.7430431141456515\n",
      "[0.99942017 0.00157999]\n",
      "Loss: 0.7429935165395293\n",
      "[0.99942056 0.00157999]\n",
      "Loss: 0.7426506694556951\n",
      "[0.99942056 0.00157999]\n",
      "Loss: 0.7426470209573052\n",
      "[0.99942063 0.00157999]\n",
      "Loss: 0.7425896301041405\n",
      "[0.99942068 0.00157999]\n",
      "Loss: 0.7425460206981412\n",
      "[0.99942071 0.00157999]\n",
      "Loss: 0.742513308246124\n",
      "[0.99942113 0.00157999]\n",
      "Loss: 0.7421423336642036\n",
      "[0.99942135 0.00157999]\n",
      "Loss: 0.7419442240233032\n",
      "[0.99942153 0.00157999]\n",
      "Loss: 0.7417886010972663\n",
      "[0.99942194 0.00157999]\n",
      "Loss: 0.7414270281634722\n",
      "[0.99942198 0.00157999]\n",
      "Loss: 0.7413907915509453\n",
      "[0.99942208 0.00158   ]\n",
      "Loss: 0.7413004431253669\n",
      "[0.99942244 0.00158   ]\n",
      "Loss: 0.7409816478614882\n",
      "[0.99942245 0.00158   ]\n",
      "Loss: 0.7409710770265487\n",
      "[0.9994226 0.00158  ]\n",
      "Loss: 0.7408347179166147\n",
      "[0.99942286 0.00158   ]\n",
      "Loss: 0.7406023515313366\n",
      "[0.99942299 0.00158   ]\n",
      "Loss: 0.7404913704947045\n",
      "[0.99942327 0.00158   ]\n",
      "Loss: 0.7402403105915503\n",
      "[0.99942346 0.00158   ]\n",
      "Loss: 0.7400771997904282\n",
      "[0.99942364 0.00158   ]\n",
      "Loss: 0.7399163976316783\n",
      "[0.99942365 0.00158   ]\n",
      "Loss: 0.739906561070913\n",
      "[0.99942367 0.00158   ]\n",
      "Loss: 0.7398863938393568\n",
      "[0.99942399 0.00158   ]\n",
      "Loss: 0.7396036671961435\n",
      "[0.99942401 0.00158   ]\n",
      "Loss: 0.7395877358299546\n",
      "[0.99942401 0.00158   ]\n",
      "Loss: 0.7395876671059941\n",
      "[0.99942421 0.00158   ]\n",
      "Loss: 0.7394073344999065\n",
      "[0.99942449 0.00158   ]\n",
      "Loss: 0.7391610631425002\n",
      "[0.99942464 0.00158   ]\n",
      "Loss: 0.7390294748101549\n",
      "[0.99942472 0.00158   ]\n",
      "Loss: 0.7389561417764068\n",
      "[0.99942473 0.00158   ]\n",
      "Loss: 0.73894530666122\n",
      "[0.99942493 0.00158   ]\n",
      "Loss: 0.7387682739805828\n",
      "[0.99942508 0.00158   ]\n",
      "Loss: 0.7386377301560265\n",
      "[0.99942527 0.00158   ]\n",
      "Loss: 0.7384643391996635\n",
      "[0.99942538 0.00158   ]\n",
      "Loss: 0.7383735340910098\n",
      "[0.99942543 0.00158   ]\n",
      "Loss: 0.7383304711021715\n",
      "[0.99942552 0.00158   ]\n",
      "Loss: 0.7382428885156546\n",
      "[0.99942568 0.00158   ]\n",
      "Loss: 0.7381008875225107\n",
      "[0.99942582 0.00158   ]\n",
      "Loss: 0.737977291618188\n",
      "[0.99942615 0.00158   ]\n",
      "Loss: 0.7376870671253535\n",
      "[0.99942637 0.00158   ]\n",
      "Loss: 0.7374968997800074\n",
      "[0.99942642 0.00158   ]\n",
      "Loss: 0.7374449740921322\n",
      "[0.99942661 0.00158   ]\n",
      "Loss: 0.7372850055409849\n",
      "[0.99942667 0.00158   ]\n",
      "Loss: 0.7372245573724793\n",
      "[0.99942672 0.00158   ]\n",
      "Loss: 0.7371865178922906\n",
      "[0.99942674 0.00158   ]\n",
      "Loss: 0.7371645276620258\n",
      "[0.99942684 0.00158   ]\n",
      "Loss: 0.7370815029239893\n",
      "[0.99942685 0.00158   ]\n",
      "Loss: 0.7370713697839211\n",
      "[0.99942691 0.00158   ]\n",
      "Loss: 0.7370189799103055\n",
      "[0.99942697 0.00158   ]\n",
      "Loss: 0.736966126278343\n",
      "[0.99942711 0.00158   ]\n",
      "Loss: 0.7368422808634882\n",
      "[0.99942732 0.00158   ]\n",
      "Loss: 0.7366552387670244\n",
      "[0.9994276 0.00158  ]\n",
      "Loss: 0.7364083517535944\n",
      "[0.99942761 0.00158   ]\n",
      "Loss: 0.7363992166346531\n",
      "[0.99942761 0.00158   ]\n",
      "Loss: 0.7363945049955192\n",
      "[0.99942763 0.00158   ]\n",
      "Loss: 0.7363804902993141\n",
      "[0.99942774 0.00158   ]\n",
      "Loss: 0.7362787315336239\n",
      "[0.99942779 0.00158   ]\n",
      "Loss: 0.7362380709827033\n",
      "[0.99942782 0.00158   ]\n",
      "Loss: 0.736213379990779\n",
      "[0.99942791 0.00158   ]\n",
      "Loss: 0.7361315005469478\n",
      "[0.99942795 0.00158   ]\n",
      "Loss: 0.7360988821875478\n",
      "[0.99942817 0.00158   ]\n",
      "Loss: 0.7358975294783184\n",
      "[0.99942838 0.00158   ]\n",
      "Loss: 0.735717913398089\n",
      "[0.99942838 0.00158   ]\n",
      "Loss: 0.7357119471211984\n",
      "[0.99942865 0.00158001]\n",
      "Loss: 0.7354755898655297\n",
      "[0.99942874 0.00158001]\n",
      "Loss: 0.7353998159365333\n",
      "[0.99942876 0.00158001]\n",
      "Loss: 0.735377998864725\n",
      "[0.99942877 0.00158001]\n",
      "Loss: 0.735372385692191\n",
      "[0.99942877 0.00158001]\n",
      "Loss: 0.7353705826398255\n",
      "[0.99942896 0.00158001]\n",
      "Loss: 0.7352033848617756\n",
      "[0.99942912 0.00158001]\n",
      "Loss: 0.7350644557569177\n",
      "[0.99942923 0.00158001]\n",
      "Loss: 0.734967256149879\n",
      "[0.99942933 0.00158001]\n",
      "Loss: 0.7348771301059015\n",
      "[0.99942937 0.00158001]\n",
      "Loss: 0.7348376332007757\n",
      "[0.99942961 0.00158001]\n",
      "Loss: 0.7346259072707566\n",
      "[0.99942975 0.00158001]\n",
      "Loss: 0.7345032450571229\n",
      "[0.99942976 0.00158001]\n",
      "Loss: 0.7344976339074758\n",
      "[0.99942981 0.00158001]\n",
      "Loss: 0.7344509555775456\n",
      "[0.99942993 0.00158001]\n",
      "Loss: 0.7343469909726108\n",
      "[0.99943007 0.00158001]\n",
      "Loss: 0.7342184504245978\n",
      "[0.9994302  0.00158001]\n",
      "Loss: 0.7341032514522153\n",
      "[0.9994302  0.00158001]\n",
      "Loss: 0.7341024432398834\n",
      "[0.99943026 0.00158001]\n",
      "Loss: 0.7340538001224917\n",
      "[0.99943028 0.00158001]\n",
      "Loss: 0.7340365120101902\n",
      "[0.99943041 0.00158001]\n",
      "Loss: 0.7339250214920726\n",
      "[0.99943042 0.00158001]\n",
      "Loss: 0.733916022621267\n",
      "[0.99943048 0.00158001]\n",
      "Loss: 0.7338634865532867\n",
      "[0.99943054 0.00158001]\n",
      "Loss: 0.7338042368108163\n",
      "[0.99943054 0.00158001]\n",
      "Loss: 0.7338037510068293\n",
      "[0.99943054 0.00158001]\n",
      "Loss: 0.7338036157849629\n",
      "[0.99943061 0.00158001]\n",
      "Loss: 0.7337461053893128\n",
      "[0.99943067 0.00158001]\n",
      "Loss: 0.7336892054990849\n",
      "[0.99943071 0.00158001]\n",
      "Loss: 0.7336605264564712\n",
      "[0.99943077 0.00158001]\n",
      "Loss: 0.7336017870388803\n",
      "[0.99943077 0.00158001]\n",
      "Loss: 0.7335994229393674\n",
      "[0.99943078 0.00158001]\n",
      "Loss: 0.7335917185937503\n",
      "[0.99943082 0.00158001]\n",
      "Loss: 0.7335574673334063\n",
      "[0.99943087 0.00158001]\n",
      "Loss: 0.7335107122788613\n",
      "[0.99943096 0.00158001]\n",
      "Loss: 0.7334345178621592\n",
      "[0.99943098 0.00158001]\n",
      "Loss: 0.7334219405699338\n",
      "[0.99943099 0.00158001]\n",
      "Loss: 0.7334133997454134\n",
      "[0.99943099 0.00158001]\n",
      "Loss: 0.7334084315374696\n",
      "[0.99943109 0.00158001]\n",
      "Loss: 0.7333233299467988\n",
      "[0.9994312  0.00158001]\n",
      "Loss: 0.7332254806556234\n",
      "[0.99943123 0.00158001]\n",
      "Loss: 0.7331937193280905\n",
      "[0.99943127 0.00158001]\n",
      "Loss: 0.7331609957999126\n",
      "[0.99943129 0.00158001]\n",
      "Loss: 0.7331421120816333\n",
      "[0.99943142 0.00158001]\n",
      "Loss: 0.7330259597932379\n",
      "[0.99943149 0.00158001]\n",
      "Loss: 0.7329636932522923\n",
      "[0.99943153 0.00158001]\n",
      "Loss: 0.7329366181935368\n",
      "[0.99943154 0.00158001]\n",
      "Loss: 0.7329216150368929\n",
      "[0.9994316  0.00158001]\n",
      "Loss: 0.7328696286919507\n",
      "[0.99943162 0.00158001]\n",
      "Loss: 0.7328566679152636\n",
      "[0.99943165 0.00158001]\n",
      "Loss: 0.73283096836107\n",
      "[0.99943169 0.00158001]\n",
      "Loss: 0.7327924067484014\n",
      "[0.99943176 0.00158001]\n",
      "Loss: 0.7327320299300488\n",
      "[0.99943176 0.00158001]\n",
      "Loss: 0.7327278229602839\n",
      "[0.99943192 0.00158001]\n",
      "Loss: 0.7325882434479584\n",
      "[0.99943193 0.00158001]\n",
      "Loss: 0.7325817703110805\n",
      "[0.99943201 0.00158001]\n",
      "Loss: 0.7325131237522892\n",
      "[0.99943211 0.00158001]\n",
      "Loss: 0.7324168764428634\n",
      "[0.99943218 0.00158001]\n",
      "Loss: 0.7323566558951033\n",
      "[0.9994322  0.00158001]\n",
      "Loss: 0.7323383206164239\n",
      "[0.99943223 0.00158001]\n",
      "Loss: 0.7323130184780646\n",
      "[0.99943228 0.00158001]\n",
      "Loss: 0.7322703748892538\n",
      "[0.99943228 0.00158001]\n",
      "Loss: 0.7322698290703318\n",
      "[0.99943236 0.00158001]\n",
      "Loss: 0.7321992838767407\n",
      "[0.99943242 0.00158001]\n",
      "Loss: 0.7321436524567518\n",
      "[0.99943244 0.00158001]\n",
      "Loss: 0.7321293995722445\n",
      "[0.99943253 0.00158001]\n",
      "Loss: 0.7320530838457171\n",
      "[0.99943259 0.00158001]\n",
      "Loss: 0.7319991167328938\n",
      "[0.99943264 0.00158001]\n",
      "Loss: 0.7319536403309179\n",
      "[0.99943267 0.00158001]\n",
      "Loss: 0.731926565576638\n",
      "[0.9994327  0.00158001]\n",
      "Loss: 0.731901412049152\n",
      "[0.9994327  0.00158001]\n",
      "Loss: 0.7318972211214644\n",
      "[0.99943276 0.00158001]\n",
      "Loss: 0.7318479187711749\n",
      "[0.99943277 0.00158001]\n",
      "Loss: 0.7318417469384585\n",
      "[0.99943282 0.00158001]\n",
      "Loss: 0.7317940323774934\n",
      "[0.99943286 0.00158001]\n",
      "Loss: 0.7317643401521612\n",
      "[0.9994329  0.00158001]\n",
      "Loss: 0.7317256171504544\n",
      "[0.99943295 0.00158001]\n",
      "Loss: 0.7316789245489221\n",
      "[0.99943305 0.00158001]\n",
      "Loss: 0.7315922262963325\n",
      "[0.99943307 0.00158001]\n",
      "Loss: 0.731572838117823\n",
      "[0.99943314 0.00158001]\n",
      "Loss: 0.73151449827706\n",
      "[0.99943322 0.00158001]\n",
      "Loss: 0.7314465341106032\n",
      "[0.99943334 0.00158001]\n",
      "Loss: 0.7313328273126781\n",
      "[0.99943335 0.00158001]\n",
      "Loss: 0.7313318007424582\n",
      "[0.99943335 0.00158001]\n",
      "Loss: 0.7313261121290423\n",
      "[0.99943339 0.00158001]\n",
      "Loss: 0.73128939300505\n",
      "[0.9994334  0.00158001]\n",
      "Loss: 0.731287027579376\n",
      "[0.99943341 0.00158001]\n",
      "Loss: 0.731277683258214\n",
      "[0.99943349 0.00158001]\n",
      "Loss: 0.731207326630536\n",
      "[0.99943355 0.00158001]\n",
      "Loss: 0.7311562067430416\n",
      "[0.99943355 0.00158001]\n",
      "Loss: 0.7311497372910907\n",
      "[0.99943364 0.00158001]\n",
      "Loss: 0.7310686121941297\n",
      "[0.99943369 0.00158001]\n",
      "Loss: 0.7310305048637198\n",
      "[0.99943373 0.00158001]\n",
      "Loss: 0.7309898731377031\n",
      "[0.99943377 0.00158001]\n",
      "Loss: 0.7309600264810094\n",
      "[0.9994338  0.00158001]\n",
      "Loss: 0.7309346936114287\n",
      "[0.99943389 0.00158001]\n",
      "Loss: 0.730852180979628\n",
      "[0.9994339  0.00158001]\n",
      "Loss: 0.7308452005632957\n",
      "[0.99943394 0.00158001]\n",
      "Loss: 0.730807977242504\n",
      "[0.999434   0.00158001]\n",
      "Loss: 0.7307574630318452\n",
      "[0.999434   0.00158001]\n",
      "Loss: 0.7307560741133456\n",
      "[0.99943401 0.00158001]\n",
      "Loss: 0.7307458998788348\n",
      "[0.99943405 0.00158001]\n",
      "Loss: 0.7307090754786336\n",
      "[0.99943411 0.00158001]\n",
      "Loss: 0.7306606131752649\n",
      "[0.99943411 0.00158001]\n",
      "Loss: 0.7306571258016659\n",
      "[0.99943412 0.00158001]\n",
      "Loss: 0.7306541289658712\n",
      "[0.99943416 0.00158001]\n",
      "Loss: 0.7306171790088559\n",
      "[0.99943416 0.00158001]\n",
      "Loss: 0.7306150866180352\n",
      "[0.99943417 0.00158001]\n",
      "Loss: 0.7306051600290072\n",
      "[0.99943417 0.00158001]\n",
      "Loss: 0.7306021055383479\n",
      "[0.99943423 0.00158001]\n",
      "Loss: 0.7305527731433874\n",
      "[0.99943426 0.00158001]\n",
      "Loss: 0.7305235412424697\n",
      "[0.99943432 0.00158002]\n",
      "Loss: 0.7304771851400459\n",
      "[0.99943434 0.00158002]\n",
      "Loss: 0.730455693929991\n",
      "[0.99943437 0.00158002]\n",
      "Loss: 0.7304296825066336\n",
      "[0.99943442 0.00158002]\n",
      "Loss: 0.7303839058035954\n",
      "[0.99943448 0.00158002]\n",
      "Loss: 0.7303313407853367\n",
      "[0.99943451 0.00158002]\n",
      "Loss: 0.7303035066542126\n",
      "[0.99943456 0.00158002]\n",
      "Loss: 0.7302656744915861\n",
      "[0.99943461 0.00158002]\n",
      "Loss: 0.7302146845387085\n",
      "[0.99943465 0.00158002]\n",
      "Loss: 0.7301853524835864\n",
      "[0.99943469 0.00158002]\n",
      "Loss: 0.7301486958168455\n",
      "[0.99943469 0.00158002]\n",
      "Loss: 0.7301464769122977\n",
      "[0.9994347  0.00158002]\n",
      "Loss: 0.73013541589324\n",
      "[0.99943471 0.00158002]\n",
      "Loss: 0.7301293423549693\n",
      "[0.99943472 0.00158002]\n",
      "Loss: 0.7301246600829615\n",
      "[0.99943475 0.00158002]\n",
      "Loss: 0.7300995397671536\n",
      "[0.99943475 0.00158002]\n",
      "Loss: 0.7300987627140675\n",
      "[0.99943482 0.00158002]\n",
      "Loss: 0.7300345248330277\n",
      "[0.99943483 0.00158002]\n",
      "Loss: 0.7300285789180385\n",
      "[0.99943483 0.00158002]\n",
      "Loss: 0.7300281357085241\n",
      "[0.99943485 0.00158002]\n",
      "Loss: 0.7300055411872769\n",
      "[0.99943488 0.00158002]\n",
      "Loss: 0.7299809552147899\n",
      "[0.99943489 0.00158002]\n",
      "Loss: 0.7299686651328445\n",
      "[0.99943491 0.00158002]\n",
      "Loss: 0.7299567399279078\n",
      "[0.99943492 0.00158002]\n",
      "Loss: 0.7299428297870597\n",
      "[0.99943497 0.00158002]\n",
      "Loss: 0.7299018563071548\n",
      "[0.99943499 0.00158002]\n",
      "Loss: 0.72988363823905\n",
      "[0.99943501 0.00158002]\n",
      "Loss: 0.7298657376118965\n",
      "[0.99943505 0.00158002]\n",
      "Loss: 0.729830501339635\n",
      "[0.99943507 0.00158002]\n",
      "Loss: 0.729817506710498\n",
      "[0.99943508 0.00158002]\n",
      "Loss: 0.7298020206161894\n",
      "[0.99943511 0.00158002]\n",
      "Loss: 0.729783143829074\n",
      "[0.99943516 0.00158002]\n",
      "Loss: 0.7297354092927925\n",
      "[0.99943518 0.00158002]\n",
      "Loss: 0.7297156623293174\n",
      "[0.99943518 0.00158002]\n",
      "Loss: 0.7297140982343627\n",
      "[0.99943521 0.00158002]\n",
      "Loss: 0.7296928988041855\n",
      "[0.99943524 0.00158002]\n",
      "Loss: 0.7296657155334078\n",
      "[0.99943526 0.00158002]\n",
      "Loss: 0.7296446532424838\n",
      "[0.99943527 0.00158002]\n",
      "Loss: 0.7296412322029208\n",
      "[0.9994353  0.00158002]\n",
      "Loss: 0.7296117228615845\n",
      "[0.99943532 0.00158002]\n",
      "Loss: 0.7295948971116906\n",
      "[0.99943535 0.00158002]\n",
      "Loss: 0.7295674470758409\n",
      "[0.99943538 0.00158002]\n",
      "Loss: 0.7295429533560538\n",
      "[0.99943538 0.00158002]\n",
      "Loss: 0.7295395258758107\n",
      "[0.99943541 0.00158002]\n",
      "Loss: 0.7295186217113999\n",
      "[0.99943545 0.00158002]\n",
      "Loss: 0.729481147174165\n",
      "[0.99943547 0.00158002]\n",
      "Loss: 0.7294636600218671\n",
      "[0.99943548 0.00158002]\n",
      "Loss: 0.7294519999963242\n",
      "[0.99943548 0.00158002]\n",
      "Loss: 0.7294499266545413\n",
      "[0.99943548 0.00158002]\n",
      "Loss: 0.7294492125301294\n",
      "[0.99943551 0.00158002]\n",
      "Loss: 0.7294248434173402\n",
      "[0.99943553 0.00158002]\n",
      "Loss: 0.7294133048621524\n",
      "[0.99943556 0.00158002]\n",
      "Loss: 0.7293817341641687\n",
      "[0.99943558 0.00158002]\n",
      "Loss: 0.7293686285440176\n",
      "[0.99943559 0.00158002]\n",
      "Loss: 0.7293548044314203\n",
      "[0.9994356  0.00158002]\n",
      "Loss: 0.7293488106539392\n",
      "[0.9994356  0.00158002]\n",
      "Loss: 0.7293459091309119\n",
      "[0.99943561 0.00158002]\n",
      "Loss: 0.7293369799792179\n",
      "[0.99943563 0.00158002]\n",
      "Loss: 0.7293248542391462\n",
      "[0.99943566 0.00158002]\n",
      "Loss: 0.7292940531522659\n",
      "[0.99943566 0.00158002]\n",
      "Loss: 0.729291744550152\n",
      "[0.99943567 0.00158002]\n",
      "Loss: 0.7292832753829175\n",
      "[0.9994357  0.00158002]\n",
      "Loss: 0.7292631231377136\n",
      "[0.99943571 0.00158002]\n",
      "Loss: 0.7292495268014278\n",
      "[0.99943571 0.00158002]\n",
      "Loss: 0.7292481186839674\n",
      "[0.99943572 0.00158002]\n",
      "Loss: 0.7292453879325282\n",
      "[0.99943575 0.00158002]\n",
      "Loss: 0.7292154391481646\n",
      "[0.99943575 0.00158002]\n",
      "Loss: 0.7292131131480513\n",
      "[0.99943576 0.00158002]\n",
      "Loss: 0.7292027280453692\n",
      "[0.99943578 0.00158002]\n",
      "Loss: 0.7291887223253835\n",
      "[0.99943579 0.00158002]\n",
      "Loss: 0.7291847526653246\n",
      "[0.9994358  0.00158002]\n",
      "Loss: 0.7291713919192553\n",
      "[0.99943581 0.00158002]\n",
      "Loss: 0.7291634989248216\n",
      "[0.99943583 0.00158002]\n",
      "Loss: 0.7291492386906759\n",
      "[0.99943585 0.00158002]\n",
      "Loss: 0.7291309835250149\n",
      "[0.99943587 0.00158002]\n",
      "Loss: 0.7291126315313975\n",
      "[0.99943589 0.00158002]\n",
      "Loss: 0.7290970187647394\n",
      "[0.99943589 0.00158002]\n",
      "Loss: 0.7290906315614593\n",
      "[0.99943591 0.00158002]\n",
      "Loss: 0.729075182837043\n",
      "[0.99943591 0.00158002]\n",
      "Loss: 0.7290740092090989\n",
      "[0.99943592 0.00158002]\n",
      "Loss: 0.7290691869886886\n",
      "[0.99943595 0.00158002]\n",
      "Loss: 0.7290414189981262\n",
      "[0.99943596 0.00158002]\n",
      "Loss: 0.729033832790441\n",
      "[0.99943597 0.00158002]\n",
      "Loss: 0.7290215229695481\n",
      "[0.99943598 0.00158002]\n",
      "Loss: 0.7290125181882994\n",
      "[0.99943599 0.00158002]\n",
      "Loss: 0.7290014982143735\n",
      "[0.999436   0.00158002]\n",
      "Loss: 0.7289947995025009\n",
      "[0.99943602 0.00158002]\n",
      "Loss: 0.7289785641625653\n",
      "[0.99943602 0.00158002]\n",
      "Loss: 0.72897656485575\n",
      "[0.99943602 0.00158002]\n",
      "Loss: 0.7289764498622039\n",
      "[0.99943603 0.00158002]\n",
      "Loss: 0.7289668300353404\n",
      "[0.99943604 0.00158002]\n",
      "Loss: 0.7289572976063214\n",
      "[0.99943607 0.00158002]\n",
      "Loss: 0.7289382137025056\n",
      "[0.99943608 0.00158002]\n",
      "Loss: 0.7289222372382764\n",
      "[0.9994361  0.00158002]\n",
      "Loss: 0.7289088670804148\n",
      "[0.9994361  0.00158002]\n",
      "Loss: 0.728908304171369\n",
      "[0.99943612 0.00158002]\n",
      "Loss: 0.728891187444621\n",
      "[0.99943613 0.00158002]\n",
      "Loss: 0.7288811505621773\n",
      "[0.99943613 0.00158002]\n",
      "Loss: 0.7288786153173411\n",
      "[0.99943614 0.00158002]\n",
      "Loss: 0.7288754535506675\n",
      "[0.99943615 0.00158002]\n",
      "Loss: 0.7288644342246074\n",
      "[0.99943615 0.00158002]\n",
      "Loss: 0.7288621566311888\n",
      "[0.99943617 0.00158002]\n",
      "Loss: 0.7288507275574766\n",
      "[0.99943618 0.00158002]\n",
      "Loss: 0.7288341623604756\n",
      "[0.9994362  0.00158002]\n",
      "Loss: 0.7288203779509951\n",
      "[0.99943623 0.00158002]\n",
      "Loss: 0.7287956570556142\n",
      "[0.99943623 0.00158002]\n",
      "Loss: 0.7287940867327872\n",
      "[0.99943625 0.00158002]\n",
      "Loss: 0.7287795293401966\n",
      "[0.99943626 0.00158002]\n",
      "Loss: 0.7287692912529412\n",
      "[0.99943626 0.00158002]\n",
      "Loss: 0.7287656898687946\n",
      "[0.99943626 0.00158002]\n",
      "Loss: 0.7287636296048882\n",
      "[0.99943628 0.00158002]\n",
      "Loss: 0.728750168061142\n",
      "[0.99943629 0.00158002]\n",
      "Loss: 0.7287385220269119\n",
      "[0.99943629 0.00158002]\n",
      "Loss: 0.7287377920300944\n",
      "[0.99943632 0.00158002]\n",
      "Loss: 0.7287174507085833\n",
      "[0.99943632 0.00158002]\n",
      "Loss: 0.7287140398871935\n",
      "[0.99943635 0.00158002]\n",
      "Loss: 0.7286917152721064\n",
      "[0.99943635 0.00158002]\n",
      "Loss: 0.728687090441277\n",
      "[0.99943636 0.00158002]\n",
      "Loss: 0.7286771714140321\n",
      "[0.99943638 0.00158002]\n",
      "Loss: 0.7286644546923814\n",
      "[0.99943638 0.00158002]\n",
      "Loss: 0.7286641395418256\n",
      "[0.99943639 0.00158002]\n",
      "Loss: 0.7286536848350414\n",
      "[0.9994364  0.00158002]\n",
      "Loss: 0.7286469559575003\n",
      "[0.99943641 0.00158002]\n",
      "Loss: 0.7286376646683778\n",
      "[0.99943642 0.00158002]\n",
      "Loss: 0.7286277863196601\n",
      "[0.99943643 0.00158002]\n",
      "Loss: 0.7286198155213326\n",
      "[0.99943644 0.00158002]\n",
      "Loss: 0.7286135150010243\n",
      "[0.99943644 0.00158002]\n",
      "Loss: 0.7286062944093735\n",
      "[0.99943645 0.00158002]\n",
      "Loss: 0.7285965560995662\n",
      "[0.99943648 0.00158002]\n",
      "Loss: 0.7285779331639629\n",
      "[0.99943649 0.00158002]\n",
      "Loss: 0.7285657851855636\n",
      "[0.99943649 0.00158002]\n",
      "Loss: 0.7285624558662629\n",
      "[0.9994365  0.00158002]\n",
      "Loss: 0.7285573870478624\n",
      "[0.99943651 0.00158002]\n",
      "Loss: 0.7285469423707868\n",
      "[0.99943653 0.00158002]\n",
      "Loss: 0.7285314793676447\n",
      "[0.99943653 0.00158002]\n",
      "Loss: 0.7285276554012357\n",
      "[0.99943653 0.00158002]\n",
      "Loss: 0.7285268003364144\n",
      "[0.99943654 0.00158002]\n",
      "Loss: 0.7285247272454375\n",
      "[0.99943654 0.00158002]\n",
      "Loss: 0.7285246912680934\n",
      "[0.99943656 0.00158002]\n",
      "Loss: 0.7285071342755591\n",
      "[0.99943656 0.00158002]\n",
      "Loss: 0.728505042376657\n",
      "[0.99943657 0.00158002]\n",
      "Loss: 0.7284955974951618\n",
      "[0.99943657 0.00158002]\n",
      "Loss: 0.7284918977196626\n",
      "[0.99943657 0.00158002]\n",
      "Loss: 0.7284911745937536\n",
      "[0.99943658 0.00158002]\n",
      "Loss: 0.7284866711330674\n",
      "[0.99943658 0.00158002]\n",
      "Loss: 0.7284834718723053\n",
      "[0.99943659 0.00158002]\n",
      "Loss: 0.7284790689964339\n",
      "[0.99943659 0.00158002]\n",
      "Loss: 0.7284766785911716\n",
      "[0.9994366  0.00158002]\n",
      "Loss: 0.728468203927843\n",
      "[0.99943661 0.00158002]\n",
      "Loss: 0.7284608100615481\n",
      "[0.99943661 0.00158002]\n",
      "Loss: 0.7284597018269467\n",
      "[0.99943662 0.00158002]\n",
      "Loss: 0.7284522927008318\n",
      "[0.99943663 0.00158002]\n",
      "Loss: 0.728445585273412\n",
      "[0.99943664 0.00158002]\n",
      "Loss: 0.7284365533138807\n",
      "[0.99943664 0.00158002]\n",
      "Loss: 0.7284303416530274\n",
      "[0.99943665 0.00158002]\n",
      "Loss: 0.7284209266999075\n",
      "[0.99943666 0.00158002]\n",
      "Loss: 0.7284155333904614\n",
      "[0.99943667 0.00158002]\n",
      "Loss: 0.7284114700801142\n",
      "[0.99943667 0.00158002]\n",
      "Loss: 0.7284042938726963\n",
      "[0.99943668 0.00158002]\n",
      "Loss: 0.7283964415238862\n",
      "[0.99943668 0.00158002]\n",
      "Loss: 0.7283954696524766\n",
      "[0.99943669 0.00158002]\n",
      "Loss: 0.7283878720013901\n",
      "[0.99943669 0.00158002]\n",
      "Loss: 0.7283865841605875\n",
      "[0.9994367  0.00158002]\n",
      "Loss: 0.7283824793954932\n",
      "[0.99943671 0.00158002]\n",
      "Loss: 0.7283742757596787\n",
      "[0.99943671 0.00158002]\n",
      "Loss: 0.7283736088845302\n",
      "[0.99943672 0.00158002]\n",
      "Loss: 0.7283671353157561\n",
      "[0.99943672 0.00158002]\n",
      "Loss: 0.7283656154689302\n",
      "[0.99943672 0.00158002]\n",
      "Loss: 0.728362848468777\n",
      "[0.99943672 0.00158002]\n",
      "Loss: 0.7283617851489269\n",
      "[0.99943672 0.00158002]\n",
      "Loss: 0.728360095330882\n",
      "[0.99943672 0.00158002]\n",
      "Loss: 0.7283590408699622\n",
      "[0.99943673 0.00158002]\n",
      "Loss: 0.7283525733651937\n",
      "[0.99943674 0.00158002]\n",
      "Loss: 0.7283486174900761\n",
      "[0.99943674 0.00158002]\n",
      "Loss: 0.7283417031950552\n",
      "[0.99943674 0.00158002]\n",
      "Loss: 0.728341668108734\n",
      "[0.99943675 0.00158002]\n",
      "Loss: 0.7283398326310146\n",
      "[0.99943675 0.00158002]\n",
      "Loss: 0.7283381280736492\n",
      "[0.99943675 0.00158002]\n",
      "Loss: 0.7283342126993421\n",
      "[0.99943676 0.00158002]\n",
      "Loss: 0.7283296884895091\n",
      "[0.99943676 0.00158002]\n",
      "Loss: 0.7283257973464247\n",
      "[0.99943676 0.00158002]\n",
      "Loss: 0.7283250412046425\n",
      "[0.99943677 0.00158002]\n",
      "Loss: 0.7283200619105537\n",
      "[0.99943678 0.00158002]\n",
      "Loss: 0.7283141846073918\n",
      "[0.99943678 0.00158002]\n",
      "Loss: 0.7283119914660875\n",
      "[0.99943678 0.00158002]\n",
      "Loss: 0.7283079060822211\n",
      "[0.99943678 0.00158002]\n",
      "Loss: 0.7283062612330204\n",
      "[0.99943679 0.00158002]\n",
      "Loss: 0.728301938159412\n",
      "[0.99943679 0.00158002]\n",
      "Loss: 0.7282973792828862\n",
      "[0.9994368  0.00158002]\n",
      "Loss: 0.7282954066442189\n",
      "[0.9994368  0.00158002]\n",
      "Loss: 0.7282915381035999\n",
      "[0.99943681 0.00158002]\n",
      "Loss: 0.7282857133992009\n",
      "[0.99943682 0.00158002]\n",
      "Loss: 0.7282789776121548\n",
      "[0.99943682 0.00158002]\n",
      "Loss: 0.7282733658822105\n",
      "[0.99943683 0.00158002]\n",
      "Loss: 0.7282709120888423\n",
      "[0.99943683 0.00158002]\n",
      "Loss: 0.7282670298736001\n",
      "[0.99943683 0.00158002]\n",
      "Loss: 0.7282658390919844\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "data = get_data(1000)\n",
    "\n",
    "model.fit(data[:,0].reshape((-1, 1)), data[:,1], lr_fn=lambda i: 0.0000001 * 0.99**i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f29dca54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e45d307f10>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(data[:, 0], data[:, 1])\n",
    "\n",
    "x_vals = np.sort(data[:, 0])\n",
    "y_vals = model.predict(x_vals.reshape(-1, 1))\n",
    "\n",
    "plt.plot(x_vals, y_vals, color='red')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
